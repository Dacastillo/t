\documentclass{book}
\usepackage[margin=1.5in]{geometry} 
\usepackage[spanish]{babel}
\usepackage{amsmath,amsthm,amssymb,hyperref,braket,fontspec,color, sectsty, graphicx, appendix, empheq, subfig}
\usepackage[dvipsnames]{xcolor}
\setromanfont[
BoldFont=QuattrocentoSans-Bold.ttf, 
ItalicFont=QuattrocentoSans-BoldItalic.ttf,
BoldItalicFont=QuattrocentoSans-Italic.ttf
]{QuattrocentoSans-Regular.ttf}
\setsansfont[
BoldFont=QuattrocentoSans-Bold.ttf,
ItalicFont=QuattrocentoSans-BoldItalic.ttf,
BoldItalicFont=QuattrocentoSans-Italic.ttf
]{QuattrocentoSans-Regular.ttf}
\begin{document}
\author{\textcolor{Red}{Daniel Castillo}}
\title{\textcolor{Red}{Óptica e Información Cuántica}}
\vspace{0.05in}
\chapterfont{\color{Red}}
\sectionfont{\color{Red}}
\subsectionfont{\color{Red}}
\everymath{\color{blue}}
\everydisplay{\color{blue}}
\maketitle
\tableofcontents
\chapter{Generación de Ecuaciones Maestras}

\section{Derivación formal: semigrupos}

\section{Introducción}
Comenzando con la evolución temporal de $\rho_A(t)$, parte A de una matriz compuesta $\rho_{AB}$
\begin{equation}\label{eq1.1}\rho_A(t)=tr_B{U(t,t_0)\rho(t_0)U^\dag(t,t_0)}\end{equation}
y la ecuación de Von Neumann 
\begin{equation}\label{eq1.2}\dot{\rho_A}(t)=-iTr_B[H(t),\rho(t)]\end{equation}
Se puede definir un \textcolor{Red}{mapa dinámico} como una función que va del espacio de Hilbert de A hasta sí mismo y consiste en \textcolor{Red}{superoperadores} que representan la evolución temporal para la matriz densidad para $\rho_B$ y tiempo fijos.
\begin{equation}\label{eq1.3}V(t):\mathbb{H}_A\textcolor{red}{\Rightarrow}\mathbb{H}_A\textcolor{red}{\Rightarrow} \rho_A(t)=V(t)\rho_A(0)=U(t,0)[\rho_A(0)\otimes\rho_B(0)]U^\dag(t,0) \end{equation}
Si se escribe $\rho_A$ en su base espectral, y se escribe las componentes matriciales del operador unitario $U(t,0)$ presente en \textcolor{blue}{\ref{eq1.3}} en la misma base:
\begin{equation}\label{eq1.4}\rho_A=\sum_\alpha\lambda_\alpha\ket{\varphi_\alpha}\bra{\varphi_\alpha}, W_{\alpha\beta}=\sqrt{\lambda_\alpha\lambda_\beta}\bra{\varphi_\alpha}U(t,0)\ket{\varphi_\beta}\end{equation}
Se puede reescribir la evolución temporal $V(t)$ como una medición generalizada (POVM)
\begin{equation}\label{eq1.5}V(t)\rho_A=\sum_{\alpha\beta}W_{\alpha\beta}(t)\rho_AW^\dag_{\alpha\beta}(t)\end{equation}
Dicha evolución temporal cumple como propiedades
\begin{equation}\label{eq1.6} \sum_{\alpha\beta}W_{\alpha\beta}^\dag(t)W_{\alpha\beta}(t)=\mathbb{I}_A\textcolor{red}{\Rightarrow} tr_A(V(t)\rho_A)=tr_A(\rho_A)=1\end{equation}
De acá se deduce que la operación evolución temporal cuántica es convexa, positiva y preservadora de traza, por lo que mantiene el sentido físico de las matrices densidad, que son las probabilidades. Estos operadores de evolución son los que forman un semigrupo que cumple con la siguiente propiedad fundamental.
\begin{equation}\label{eq1.7}V(t_1)V(t_2)=V(t_1+t_2) \end{equation}
La propiedad en \textcolor{blue}{\ref{eq1.7}} está vinculada con la Markovialidad.
Se define entonces un \textcolor{Red}{generador de semigrupo}, que consiste en un mapa lineal.
\begin{equation}\label{eq1.8}\mathcal{L}:V(t)=e^{\mathcal{L}t}\textcolor{red}{\Rightarrow} \dot{\rho_A(t)}=\dot{V}(t)\rho_A=\mathcal{L}\rho_A(t)\end{equation}
Se pretenderá entonces, construir el $\mathcal{L}$ más general en el espacio $\mathbb{H}_A\otimes\mathbb{H}_A$ definiendo operadores $F_i$ desde $0$ hasta $N^2$ definiendo un producto escalar para operadores
\begin{equation}\label{eq1.9}  X\cdot Y=Tr_A(X^\dag Y)\end{equation}
Se puede escribir el operador de medida generalizada, y por lo tanto el operador de evolución temporal como combinación lineal de los elementos de la base (considerando el producto escalar anterior:
\begin{equation}\label{eq1.10}W_{\alpha\beta}=\sum_{i=1}^{N^2} F_i(F_i\cdot W_{\alpha\beta}(t))\textcolor{red}{\Rightarrow} V(t)\rho_A=\sum_{\alpha\beta}\sum_{i,j=1}^{N^2} F_i(F_i\cdot W_{\alpha\beta}(t))\rho_AF_j^\dag(F_j\cdot W_{\alpha\beta}(t))^*\end{equation}
Llamando $c_{ij}$ a los resultados de los productos escalares la expansión de \textcolor{blue}{\ref{eq1.10}} se simplifica
\begin{equation}\label{eq1.11}c_{ij}=(F_i\cdot W_{\alpha\beta}(t))(F_j W_{\alpha\beta}(t))^*\textcolor{red}{\Rightarrow} V(t)\rho_A=\sum_{i,j}^{N^2}c_{ij}(t)F_i\rho_AF_j^\dag\end{equation}
Por la forma dada en la definición en \textcolor{blue}{\ref{eq1.11}}, los $c_{ij}$, forman una matriz hermítica y positiva.
Sin perder generalidad se define que el último operador de la lista sera proporcional a la identidad, así como que los operadores traza tendrán traza $0$
\begin{equation}\label{eq1.12}\forall i:[0,n^2-1] Tr_A F_i=0, F_{N^2}=\frac{\mathbb{I}_A}{\sqrt{N}}\end{equation}
Dicho esto se procede a evaluar la derivada de la evolución temporal, considerando \textcolor{blue}{\ref{eq1.8}}, la definición formal de derivada (con un tiempo $\epsilon$) y las definiciones dadas en \textcolor{blue}{\ref{eq1.12}}
\begin{equation}\label{eq1.13}\mathcal{L}\rho_A= \lim\limits_{\epsilon\textcolor{red}{\Rightarrow} 0}\frac{V(\epsilon)\rho_A-\rho_A}{\epsilon}\end{equation}
Se descompone $V(\epsilon)$ en los operadores base usando las condiciones señaladas
\begin{equation}\label{eq1.14}\begin{aligned}=\lim\limits_{\epsilon\textcolor{red}{\Rightarrow} 0}\frac{1}{\epsilon}(\sum_{i,j}^{N^2-1} C_{ij}(\epsilon)F_i\rho_AF_j^\dag+\frac{1}{\sqrt{n}}\sum_{i=1}^{N^2-1}(C_{iN^2}(\epsilon)F_i\rho_A+\\ C_{N^2i}(\epsilon)\rho_AF_i^\dag)+\frac{C_{N^2N^2}(\epsilon)-N}{N}\rho_A)\end{aligned}\end{equation}
Como se puede notar de descompuso en los productos que no incluyen a $F_N^2$, los que lo incluyen una vez y el que lo incluye 2 veces. Definiendo las siguientes constantes
\begin{equation}\label{eq1.15} a_{N^2N^2}=\lim\limits_{\epsilon\textcolor{red}{\Rightarrow} 0}\frac{C_{N^2N^2}(\epsilon)-N}{\epsilon}, a_{iN^2}=\lim\limits_{\epsilon\textcolor{red}{\Rightarrow} 0}\frac{C_{iN^2}(\epsilon)}{\epsilon} \end{equation}
\begin{equation}\label{eq1.16} a_{ij}=\lim\limits_{\epsilon\textcolor{red}{\Rightarrow} 0}\frac{C_{ij}(\epsilon)}{\epsilon}, a_{ N^2 i}=\lim\limits_{\epsilon\textcolor{red}{\Rightarrow} 0}\frac{C_{N^2 i}(\epsilon)}{\epsilon} \end{equation}
La ecuación de \textcolor{blue}{\ref{eq1.14}} se simplifica
\begin{equation}\label{eq1.17}\mathcal{L}\rho_A=\sum_{i,j}^{N^2-1}a_{ij}F_i\rho_AF_j^\dag+\frac{1}{\sqrt{N}}\sum_{i=1}^{N^2-1}(a_{iN^2}F_i\rho_A+a_{N^2i}\rho_AF_i^\dag)+\frac{a_{N^2N^2}}{N}\rho_A\end{equation}
Definiendo ahora los siguientes operadores
\begin{equation}\label{eq1.18} F=\frac{1}{\sqrt{N}}\sum_{i=1}^{N^2-1}a_{iN^2}F_i, G=\frac{a_{N^2N^2}}{2N}\mathbb{I}_A+\frac{1}{2}(F^\dag+F)\end{equation}
Se puede también escribir el Hamiltoniano en función de estos operadores
\begin{equation}\label{eq1.19}  H=\frac{1}{2i}(F^\dag-F)\end{equation}
Y la expresión de \textcolor{blue}{\ref{eq1.17}} se simplifica más
\begin{equation}\label{eq1.20} =\sum_{i,j}^{N^2-1}a_{ij}F_i\rho_AF_j^\dag+(F\rho_A+\rho_AF^\dag)+(G-\frac{F^\dag+F}{2})\rho_A+\rho_A(G-\frac{F^\dag+F}{2})\end{equation}
Usando la notación de anticonmutador $\left\{A,B\right\}=AB+BA$ y separando los elementos de F y G en los útimos sumandos se obtiene:
\begin{equation}\label{eq1.21}=\sum_{i,j}^{N^2-1}a_{ij}F_i\rho_A F_j^\dag+\left\{G,\rho_A\right\}+(F-\frac{F}{2}-\frac{F^\dag}{2})\rho_A+\rho_A(F^\dag-\frac{F^\dag}{2}-\frac{F}{2})\end{equation}
De los últimos 2 sumandos de \textcolor{blue}{\ref{eq1.21}} se reemplaza:
\begin{equation}\label{eq1.22} (\frac{F-F^\dag}{2})\rho_A+\rho_A(\frac{F^\dag-F}{2})=-iH\rho_A+i\rho_AH\end{equation}
Además considerando que el semigrupo preserva la traza
\begin{equation} \label{eq1.23}0=Tr_A\dot{\rho_A}=Tr_A\mathcal{L}\rho_A=Tr_A((2G+\sum_{i,j=1}^{N^2-1}a_{ij}F_j^\dag F_i)\rho_A)\end{equation}
Lo último debido a que la traza es cíclica. De \textcolor{blue}{\ref{eq1.23}} se obtiene que
\begin{equation}\label{eq1.24}-2G=\sum_{i,j=1}^{N^2-1}a_{ij}F_j^\dag F_i \textcolor{red}{\Rightarrow} G=\frac{-1}{2}\sum_{i,j=1}^{N^2-1}a_{ij}F_j^\dag F_i\end{equation}
E insertando \textcolor{blue}{\ref{eq1.24}} y \textcolor{blue}{\ref{eq1.22}} en \textcolor{blue}{\ref{eq1.20}} se obtiene
\begin{equation}\label{eq1.25}\mathcal{L}\rho_A=-i[H,\rho_A]+\sum_{i,j}^{N^2-1}a_{ij}(F_i\rho_AF_j^\dag+\left\{\frac{-1}{2}F_j^\dag F_i,\rho_A\right\})\end{equation}
Al ser los elementos $a_{ij}$ positivos, la matriz $A$ que los contiene puede diagonalizarse. Si se escribe la propiedad anterior como
\begin{equation}\label{eq1.26} UAU^\dag=diag(\gamma_1...\gamma_{N^2-1})\end{equation}
Se pueden escribir los operadores del espacio $\mathbb{H}_A\otimes\mathbb{H}_A$ definidos para \textcolor{blue}{\ref{eq1.10}} en términos de esta diagonalización
\begin{equation}\label{eq1.27} F_i=\sum_{k=1}^{N^2-1}u_{ki}A_k\end{equation} Despreciando el término hamiltoniano de la ecuación \textcolor{blue}{\ref{eq1.25}} e insertándolo en \textcolor{blue}{\ref{eq1.27}} se obtiene:
\begin{equation}\label{eq1.28}\mathcal{L}\rho_A=\sum_{i,j,k}^{N^2-1}( a_{ij}((u_{ki}A_k)\rho_A(u_{kj}^*A_k^\dag))+\left\{\frac{-1}{2}(u_{kj}^*A_k^\dag) (u_{ki}A_k),\rho_A\right\})\end{equation}
Y finalmente, usando que $u_{ki}a_{ij}u_{jk}^*=\gamma_k$ se puede reescribir \textcolor{blue}{\ref{eq1.28}} como 
\begin{equation}\label{eq1.29}\mathcal{L}\rho_A=\sum_k^{N^2-1}\gamma_k(A_k\rho_A A_k^\dag)+\left\{\frac{-1}{2}(A_k^\dag A_k),\rho_A\right\})\end{equation}
\section{Derivación para sistema de 2 niveles acoplado a baño de osciladores}
Definido el Hamiltoniano de Jaynes-Cumming que \textcolor{Red}{no depende del tiempo}: 
\begin{equation}\label{eq1.30}H=\hslash\omega a^\dag a+\sum_j \hslash \omega_jb_j^\dag b_j+\sum_j \hslash g_j(a^\dag b_j+b_j^\dag a\end{equation}
Donde vale la aproximación de \textcolor{Red}{onda rotante}. Se puede pasar rápidamente al marco de interacción.
\begin{equation}\label{eq1.31}H_I=\sum_j \hslash g_j(a^\dag b_j+b_j^\dag a) \end{equation} Y partiendo de la ecuación diferencial de Liouville (Von Neumann) válida cuando el hamiltoniano no depende del tiempo, considerando operadores en el marco de interacción:
\begin{equation}\label{eq1.32}\dot{\bar{\rho_{AB}}}=\frac{-i}{\hslash}[\bar{H_I}(t),\bar{\rho_{AB}(t)]}\end{equation}
La ecuación se itera 2 veces para hallar la solución (lo que por lo general \textcolor{Red}{basta como aproximación})
\begin{equation}\label{eq1.33} \begin{aligned}\bar{\rho}_{AB}(t)\simeq\bar\rho_{AB}(0)-\frac{i}{\hslash}\int_0^t dt_1[\bar{H_I}(t_1),\bar{\rho}_{AB}(t_1)]- \\ \frac{1}{\hslash^2}\int_0^t\int_0^{t_1}dt_1dt_2[\bar{H_I}(t_1),[\bar{H_I}(t_2),\bar{\rho}_{AB}(t_2)]]\end{aligned}\end{equation}
\textcolor{Red}{Asumiendo que} $\bar{\rho}_{AB}(0)=0$ y que 
\begin{equation}\label{eq1.34}Tr_B[\bar{H}_i,\rho_B(0)]=0\end{equation}
(Lo que equivale a decir físicamente de acuerdo a \textcolor{red}{\cite{Orszag}}, que \textcolor{Red}{el reservorio es un estado termal}), los 2 primeros sumandos de \textcolor{blue}{\ref{eq1.33}} se anulan, se deriva 1 vez y la ecuación maestra finalmente queda
\begin{equation}\label{eq1.35}\dot{\bar{\rho}_A(t)=\frac{-1}{\hslash^2}\int_0^tdt_1 tr_B[\bar{H}_i(t),[\bar{H}_i(t_1),\bar{\rho}_{AB}(t_1)]]}\end{equation}
\textcolor{Red}{Asumiendo que la matriz densidad en $t_1$ es un estado producto}, se procede a descomponer el integrando 
\begin{equation}\label{eq1.36}\begin{aligned}\frac{1}{\hslash^2}[\bar{H}_I(t),[\bar{H}_1(t_1),\bar{\rho}_A(t_1)\otimes\bar{\rho}_B(t_1)]]=\frac{1}{\hslash^2}(\bar{H_I}(t)\bar{H_1}(t_1)(\bar{\rho}_A(t_1)\otimes\bar{\rho}_B(t_1))-\\  \bar{H}_I(t)(\bar{\rho}_A(t_1)\otimes\bar{\rho}_B(t_1))\bar{H}_I(t_1) -\bar{H}_I(t_1)(\bar{\rho}_A(t_1)\otimes\bar{\rho}_B(t_1))\bar{H}_I(t)+ \\ (\bar{\rho}_A(t_1)\otimes\bar{\rho}_B(t_1))\bar{H}_I(t_1)\bar{H}_I(t))\end{aligned}\end{equation}
 y usando el hamiltoniano de interacción en \textcolor{blue}{\ref{eq1.32}}, \textcolor{blue}{\ref{eq1.36}} se vuelve:
 \begin{equation}\begin{aligned} \label{eq1.37} { =(G(t)a^\dag+G^\dag(t)a)(G(t_1)a^\dag+G^\dag(t_1)a)(\bar{\rho}_A(t_1)\otimes\bar{\rho}_B(t_1))-} \\ {(G(t)a^\dag+G^\dag(t)a)(\bar{\rho}_A(t_1)\otimes\bar{\rho}_B(t_1))(G(t_1)a^\dag+G^\dag(t_1)a)-} \\ {(G(t_1)a^\dag+G^\dag(t_1)a)(\bar{\rho}_A(t_1)\otimes\bar{\rho}_B(t_1))(G(t)a^\dag+G^\dag(t)a)+} \\ {(\bar{\rho}_A(t_1)\otimes\bar{\rho}_B(t_1))(G(t_1)a^\dag+G^\dag(t_1)a)(G(t)a^\dag+G^\dag(t)a)} \end{aligned}\end{equation}
 Otra aproximación que se realiza es el \textcolor{Red}{acoplamiento débil}, qué básicamente se vale de la desigualdad temporal de Heisenberg
 \begin{equation}\label{eq1.38} {\Delta t \Delta E \geq \frac{\hslash}{2}}\end{equation}
 Para decir que el tiempo de correlación del baño es menos cuando los anchos de energía son mayores y que se puede asumir que el tiempo propio del baño será mucho menor al tiempo del sistema original \textcolor{Red}{en el cuadro de interacción}
 \begin{equation}\label{eq1.39} {\tau_B \leq \frac{\hslash}{E_B}, T_A\propto \frac{1}{H^A_I}\textcolor{red}{\Rightarrow} \tau_B << T_A}\end{equation}
 El primer sumando en \textcolor{blue}{\ref{eq1.37}} equivale a
 \begin{equation}\begin{aligned}\label{eq1.40} {G(t)a^\dag G(t_1)a^\dag(\bar{\rho}_A(t_1)\otimes\bar{\rho}_B(t_1))+ G(t)a^\dag G^\dag(t_1)a(\bar{\rho}_A(t_1)\otimes\bar{\rho}_B(t_1))+} \\ {G^\dag(t)a G(t_1)a^\dag(\bar{\rho}_A(t_1)\otimes\bar{\rho}_B(t_1))+ G^\dag(t)a G^\dag(t_1) a(\bar{\rho}_A(t_1)\otimes\bar{\rho}_B(t_1))}\end{aligned}\end{equation}
 El segundo sumando en \textcolor{blue}{\ref{eq1.37}} (sin contar el signo menos) equivale a
 \begin{equation}\label{eq1.41}\begin{aligned} {G(t)a^\dag(\bar{\rho}_A(t_1)\otimes\bar{\rho}_B(t_1))G(t_1)a^\dag+ G(t)a^\dag(\bar{\rho}_A(t_1)\otimes\bar{\rho}_B(t_1))G^\dag(t_1)a+} \\ {G^\dag(t)a(\bar{\rho}_A(t_1)\otimes\bar{\rho}_B(t_1))G(t_1)a^\dag+ G^\dag(t)a(\bar{\rho}_A(t_1)\otimes\bar{\rho}_B(t_1))G^\dag(t_1)a }\end{aligned}\end{equation}
 El tercer sumando en \textcolor{blue}{\ref{eq1.37}} (sin contar el signo menos) equivale a
 \begin{equation}\label{eq1.42}\begin{aligned} { G(t_1)a^\dag(\bar{\rho}_A(t_1)\otimes\bar{\rho}_B(t_1))G(t)a^\dag+ G(t_1)a^\dag(\bar{\rho}_A(t_1)\otimes\bar{\rho}_B(t_1))G^\dag(t)a+} \\ {G^\dag(t_1)a(\bar{\rho}_A(t_1)\otimes\bar{\rho}_B(t_1))G(t)a^\dag+ G^\dag(t_1)a(\bar{\rho}_A(t_1)\otimes\bar{\rho}_B(t_1))G^\dag(t)a}\end{aligned}\end{equation}
 El cuarto sumando en  \textcolor{blue}{\ref{eq1.37}} equivale a
 \begin{equation}\label{eq1.43}\begin{aligned} {(\bar{\rho}_A(t_1)\otimes\bar{\rho}_B(t_1))G(t_1)a^\dag G(t)a^\dag+(\bar{\rho}_A(t_1)\otimes\bar{\rho}_B(t_1))G(t_1)a^\dag G^\dag(t)a+}\\  { (\bar{\rho}_A(t_1)\otimes\bar{\rho}_B(t_1))G^\dag(t_1)a G(t)a^\dag +(\bar{\rho}_A(t_1)\otimes\bar{\rho}_B(t_1))G^\dag(t_1)a G^\dag(t)a }\end{aligned}\end{equation}
 Para poder quitar los operadores del campo de un modo de la integral se hace una aproximación bastante fuerte: \textcolor{Red}{que $\rho_A$ en $t_1$ en realidad depende del tiempo $t$, lo que significa que el sistema es local en el tiempo.}
 Con esto, la traza en B de cualquiere de estos elementos solo depende de $\rho_B$ y de los operadores G, e incluso $\rho_A(t)$ se puede pasar para afuera de la integral, quedando \textcolor{blue}{\ref{eq1.35}} como:
  \begin{equation}\label{eq1.44}\begin{aligned}\dot{\bar{\rho}_A=-(a^\dag a^\dag \bar{\rho_A}(t)I_1+a^\dag a \rho_A(t) I_2+ aa^\dag \rho_A(t) I_3+aa\rho_A(t) I_4)} \\ {+(a^\dag\bar{\rho_A}(t)a^\dag (I_5+I_9)+a^\dag\bar{\rho_A}(t)a (I_6+I_{10})+ a\bar{\rho_A}(t)a^\dag (I_7+I_{11})+a\bar{\rho_A}(t)a (I_8+I_{12})} \\ {-(\bar{\rho_A}(t)a^\dag a^\dag I_{13}+\bar{\rho_A}(t)a^\dag a I_{14}+ \bar{\rho_A}(t)aa^\dag I_{15}+\bar{\rho_A}(t)aa I_{15})} \end{aligned}\end{equation}
 Entiéndanse los $I_i$ como las integrales de los elementos dependientes de B, estando $I_1$ a $I_4$ generados a partir de \textcolor{blue}{\ref{eq1.40}}, $I_5$ a $I_8$ a partir de \textcolor{blue}{\ref{eq1.41}}, $I_9$ a $I_{12}$ a partir de \textcolor{blue}{\ref{eq1.42}} y $I_{13}$a $I_{16}$ de \textcolor{blue}{\ref{eq1.43}}.
 Usando la propiedad de que la traza de una matriz es cíclica, 
 \begin{equation}\label{eq1.45} {Tr_B(G(t)G(t_1)\bar{\rho}_B(t_1))=Tr_B(G(t_1)\bar{\rho}_B(t_1)G(t))\textcolor{red}{\Rightarrow} I_{1}=I_{9} }\end{equation}
 \begin{equation}\label{eq1.46} {Tr_B(G(t)G^\dag(t_1)\bar{\rho}_B(t_1))=Tr_B(G^\dag(t_1)\bar{\rho}_B(t_1)G(t))\textcolor{red}{\Rightarrow} I_{2}=I_{11}}\end{equation}
 \begin{equation}\label{eq1.47}{ Tr_B(G^\dag(t)G(t_1)\bar{\rho}_B(t_1))=Tr_B(G(t_1)\bar{\rho}_B(t_1)G^\dag(t))\textcolor{red}{\Rightarrow} I_{3}=I_{10} }\end{equation}
 \begin{equation}\label{eq1.48}{Tr_B(G^\dag(t)G^\dag(t_1)\bar{\rho}_B(t_1))=Tr_B(G^\dag(t_1)\bar{\rho}_B(t_1)G^\dag(t))\textcolor{red}{\Rightarrow} I_{4}=I_{12}}\end{equation}
 \begin{equation} \label{eq1.49}{ Tr_B(G(t)\hat{\rho}_B(t_1)G(t_1))=Tr_B(\hat{\rho}_B(t_1)G(t_1)G(t)) \textcolor{red}{\Rightarrow} I_{5}=I_{13}
}\end{equation}
\begin{equation}\label{eq1.50} {Tr_B(G(t)\hat{\rho}_B(t_1)G^\dag(t_1))=Tr_B(\hat{\rho}_B(t_1)G^\dag(t_1)G(t)) \textcolor{red}{\Rightarrow} I_{6}=I_{15} }\end{equation}
\begin{equation}\label{eq1.51}{Tr_B(G^\dag(t)\hat{\rho}_B(t_1)G(t_1))=Tr_B(\hat{\rho}_B(t_1)G(t_1)G^\dag(t)) \textcolor{red}{\Rightarrow} I_{7}=I_{14}}\end{equation}
\begin{equation}\label{eq1.52} {Tr_B(G^\dag(t)\hat{\rho}_B(t_1)G^\dag(t_1))=Tr_B(\hat{\rho}_B(t_1)G^\dag(t_1)G^\dag(t)) \textcolor{red}{\Rightarrow} I_{8}=I_{16} }\end{equation}
 Lo anterior Reduce las integrales a resolver a solo 8. Pero ahora, si se considera que las funciones de correlación, más que de $t_1$ o $t$, dependen de la diferencia entre ellos, se puede aseverar que:
 \begin{equation}\label{eq1.53}{Tr_B(G(t)G(t_1)\hat{\rho}_B(t_1))=Tr_B(G(t_1)G(t)\hat{\rho}_B(t_1)) \textcolor{red}{\Rightarrow} I_1=I_9=I_5=I_{13}}\end{equation}
 \begin{equation}\label{eq1.54}{Tr_B(G(t)G^\dag(t_1)\hat{\rho}_B(t_1))=Tr_B(G(t_1)G^\dag(t)\hat{\rho}_B(t_1)) \textcolor{red}{\Rightarrow} I_2=I_{11}=I_7=I_{14}}\end{equation}
 \begin{equation}\label{eq1.55}{Tr_B(G^\dag(t)G(t_1)\hat{\rho}_B(t_1))=Tr_B(G^\dag(t_1)G(t)\hat{\rho}_B(t_1)) \textcolor{red}{\Rightarrow} I_3=I_{10}=I_6=I_{15}}\end{equation}
 \begin{equation}\label{eq1.56}{Tr_B(G^\dag(t)G^\dag(t_1)\hat{\rho}_B(t_1))=Tr_B(G^\dag(t_1)G^\dag(t)\hat{\rho}_B(t_1)) \textcolor{red}{\Rightarrow} I_4=I_{12}=I_8=I_{16} }\end{equation}
 Lo que Reduce la obtención de la ecuación maestra a obtener 4 integrales, que se llamarán $A$,$B$, $C$ y $D$ para las resultantes de \textcolor{blue}{\ref{eq1.53}},\textcolor{blue}{\ref{eq1.54}}, \textcolor{blue}{\ref{eq1.55}} y \textcolor{blue}{\ref{eq1.56}} respectivamente.
 Con esto, \textcolor{blue}{\ref{eq1.44}} se Reduce a
 \begin{equation}\label{eq1.57}\begin{aligned}\dot{\bar{\rho}}_A=A(2a^\dag \bar{\rho_A}(t)a^\dag-a^\dag a^\dag\bar{\rho_A}(t)-\bar{\rho_A}(t)a^\dag a^\dag)+B(2a\bar{\rho_A}(t)a^\dag-a^\dag a\bar{\rho_A}(t)-\bar{\rho_A}(t)a^\dag a) \\ +C(2a^\dag \bar{\rho}_A(t)a-aa^\dag\bar{\rho_A}(t)-\bar{\rho_A}(t)a a^\dag)+D(2a \bar{\rho}_A(t)a-a a\bar{\rho_A}(t)-\bar{\rho_A}(t)a a)\end{aligned}\end{equation}
 Lo conveniente es que las 4 integrales se resuelven de manera similar.
\begin{equation}\label{eq1.58}{ A= }\end{equation}
\begin{equation}\label{eq1.59}{ B= }\end{equation}
\begin{equation}\label{eq1.60}{ C= }\end{equation}
\begin{equation}\label{eq1.61}{ D= }\end{equation}


\section{Derivación para sistema comprimido de muchos modos}
\begin{thebibliography}{9}
\bibitem{Orszag} Orszag, M. Quantum Optics, Third Edition (2013)
\end{thebibliography}



\chapter{Clonado Cuántico}
 \section{Motivaciones}
Se pueden considerar las siguientes 2 motivaciones para estudiar el copiado cuántico:
\begin{itemize} 
\item Se puede considerar un sistema cuántico con un estado determinado (por ejemplo, polarizaciones del modo de un campo o spines de un electrón) que se quiere amplificar. Para eso, se busca una forma eficiente de copiar varias veces el estado. El entrelazamiento será el principal impedimento que se encontrará para hacer un clonado en principio (al interactuar varios sistemas, habrán correlaciones y los estados podrían entrelazarse). Curiosamente, es ese mismo entrelazamiento posible que parece imposibilitar el copiado el que después será fundamental para implementar máquinas de copiado cuántico, como se verá en los desarrollos de este documento. 
\item En criptografía, se tiene comunicaciones entre Alice y Bob por un canal cuántico sin ruido (piensen, por ejemplo, en el envío de señales usando polarizaciones de fotones en un canal de fibra óptica). Esta señal puede ser intervenida por un agente externo (por lo general llamado Eva) que puede copiar la información que capta en el canal. Una ventaja de la criptografía cuántica (y que varios de los protocolos terminan usando) es que los procedimientos de copiado cuántico provocan que al copiar un estado si o si se termina alterando el estado original, por lo que si Bob recibe un estado con determinadas características, puede darse cuenta que la comunicación fue intervenida y desechar el estado por seguridad. Las máquinas de copiado cuántico pueden modelar la acción de Eva e indicar qué probabilidades son las que indicarían a Bob la intervención. \end{itemize}
\section{¿Se puede clonar un estado?}
\subsection{¿Qué es clonar?}
Para efectos de Teoría de Información, clonar un estado es básicamente tomar un conjunto de números en un sistema y escribir los mismos números en otro conjunto con números arbitrarios (por simplicidad por lo general se elige para el segundo sistema solo ceros
\begin{equation}\label{eq2.1} {(101010111011)_A\otimes(000000000000)_B\textcolor{red}{\Rightarrow}  (101010111011)_A\otimes(101010111011)_B }\end{equation}
A nivel clásico esto siempre se puede hacer porque se cumplen 2 condiciones
\begin{itemize}
    \item Es posible copiar de forma "perfecta", o sea llevar la secuencia a otro estado con probabilidad 1. Esto se debe porque a nivel clásico la física es local, o sea, hacer cambios en un sistema cerrado nunca realiza cambios en otro sistema cerrado. 
    \item El copiado no depende del tipo de secuencia que queramos copiar, no hay una secuencia favorita a priori para copiar. Para \textcolor{Red}{todas} las secuencias se puede copiar con probabilidad 1.
\end{itemize}

Dicho esto, aparece la pregunta: ¿Se puede hacer un proceso de clonado a nivel cuántico? Es decir, pasar estas secuencias a determinados estados cuánticos:
\begin{equation}\label{eq2.2}{ \ket{101010111011}_A\otimes\ket{000000000000}_B\textcolor{red}{\Rightarrow}  \ket{101010111011}_A\otimes\ket{101010111011}_B }\end{equation}
\subsection{Teorema de no-clonado}
La respuesta que da la física cuántica es \textcolor{Red}{NO}. Detrás de esta respuesta están fenómenos puramente cuánticos como el entrelazamiento de estados o el teorema de no signal. Esto será probado siguiendo a \textcolor{red}{\cite{bib1}}.
Se considera una máquina de clonado perfecto:
\begin{equation}\label{eq2.3}\begin{aligned}{\ket{0}_A\ket{Q}_M\textcolor{red}{\Rightarrow} \ket{0}_A\ket{0}_B\ket{Q_0}_M} {\ket{1}_A\ket{Q}_M\textcolor{red}{\Rightarrow} \ket{1}_A\ket{1}_B\ket{Q_1}_M}\end{aligned}\end{equation}
Los estados $\ket{Q}$ en \textcolor{blue}{\ref{eq2.3}} son estados de la máquina de copiado, sobre la que se trazará por lo que no son del todo necesarios para los próximos análisis.
Definiendo un estado cualquiera que se quiere copiar (por simplicidad $\alpha, \beta \in \mathbb{R}$)
\begin{equation}\label{eq2.4}{\ket{s}_A=\alpha\ket{0}_A+\beta\ket{1}_B: \alpha^2+\beta^2=1}\end{equation}
Se calcula el resultado de la máquina para el estado en \textcolor{blue}{\ref{eq2.4}} usando \textcolor{blue}{\ref{eq2.3}}
\begin{equation}\label{eq2.5} {\ket{s}_A\ket{Q}_M\textcolor{red}{\Rightarrow}\alpha(\ket{0}_A\ket{0}_B\ket{Q_0}_M)+\beta(\ket{1}_A\ket{1}_B\ket{Q_1}_M)\equiv\ket{\phi}^{out}_{ABX}}\end{equation}
Se asume que los estados de la máquina son ortogonales entre sí
\begin{equation}\label{eq2.6}{\bra{Q_i}\ket{Q_j}=\delta_{ij} \forall i, j \in[0,1]}\end{equation}
Y con ello se puede calcular la matriz densidad producto de la traza para $A$
y $B$
\begin{equation}\label{eq2.7}{\rho_{AB}=Tr_M(\ket{\phi}_{ABX}^{out}\bra{\phi}_{ABX}^{out})=\begin{pmatrix}\alpha^2 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & \beta^2\end{pmatrix}}\end{equation}
Si se toma el estado producto para $\ket{s}$ en $A$ y $B$
\begin{equation}\label{eq2.8}{\Pi_s\otimes\Pi_s=(\ket{s}_A\bra{s})\otimes(\ket{s}_B\bra{s})=\begin{pmatrix} 
\alpha^4 & \alpha^3\beta & \alpha^3\beta& \alpha^2\beta^2 \\ \alpha^3\beta & \alpha^2\beta^2& \alpha^2\beta^2& \beta^3\alpha \\ \alpha^3\beta & \alpha^2\beta^2 & \alpha^2\beta^2 & \beta^3\alpha \\ \alpha^2\beta^2 & \beta^3\alpha& \beta^3\alpha & \beta^4 \end{pmatrix}}\end{equation}
Claramente son distintas. Si funcionase como el proceso clásico, estas matrices serían iguales como lo señala \textcolor{blue}{\ref{eq2.2}}. Existen formas de cuantificar la \textit{distancia} entre ambos estados, pero lo relevante es entender por qué son distintos.
\begin{itemize}
    \item La física cuántica (hasta por donde sabemos) es \textcolor{Red}{no local}. Existen estados entralazados que pueden alterar resultados de medidas a distancia. Esto contradice lo necesario para tener probabilidad 1 de copiado.
    \item Se puede demostrar (y se hará en la siguiente parte) que un procedimiento de copíado como el que se está implementando es mejor para algunos estados que para otros, lo que contradice la necesidad de independencia del estado que se quiere copiar.
\end{itemize}
Para evaluar la distancia entre 2 estados es necesario definir una distancia. Se define la \textcolor{red}{distancia Hilbert-Schmidt}:
\begin{equation}\label{eq2.9}{D(\rho_1,\rho_2)=\lvert\lvert\rho_1-\rho_2\rvert\rvert^2=(Tr((\rho_1-\rho_2)^\dag(\rho_1-\rho_2))^\frac{1}{2})^2}\end{equation} que nos bastará como distancia para matrices $2X2$ (la doble barra representa el determinante de esas 2 matrices).
También será necesario entonces calcular las matrices densidad Reducida para las 2 matrices que se quieren comparar:
\begin{equation}\label{eq2.10}{Tr_A\rho_{AB}=Tr_B\rho_{AB}=\begin{pmatrix}\alpha^2&0\\ 0&\beta^2\end{pmatrix}}\end{equation}
\begin{equation}\label{eq2.11}{Tr_A(\Pi_s\otimes\Pi_s)=Tr_B(\Pi_s\otimes\Pi_s)=\begin{pmatrix}\alpha^2 & \alpha\beta \\ \alpha\beta & \beta^2\end{pmatrix}}\end{equation}
Entonces la distancia entre ambas matrices Reducidas será
\begin{equation}\label{eq2.12}{Tr(\begin{pmatrix}0&\alpha\beta \\ \alpha\beta &0\end{pmatrix}^2)=2\alpha^2 \beta^2=2\alpha^2(1-\alpha^2)}\end{equation}
Lo que demuestra inmediatamente que el segundo punto (necesidad de independencia de los estados) no se cumple. Para los estados en los que $\alpha$ vale $0$ o $1$ (es decir, para $\ket{0}$ o $\ket{1}$ la máquina copiará a la manera clásica, y para $\frac{\ket{0}\pm\ket{1}}{\sqrt{2}}$ no solo copiará mal, sino que incluso destrozará el estado.
\section{Máquinas de clonado cuántico}
\subsection{Máquina Universal de Clonado Cuántico}
Siguiendo las derivaciones de \textcolor{red}{\cite{bib1}}, se puede construir una máquina de copiado \textcolor{Red}{probabilista}. Esto relajando una de las 2 condiciones.
\begin{itemize}
    \item Ya que la física cuántica es no local, se requiere relajar la condición de perfección, las copias no tienen por que llevar al mismo estado con probabilidad 1. Tendrán una probabilidad menor que 1 que se puede optimizar:
    \item Se seguirá exigiendo que la máquina sea independiente del estado a elegir, es decir, que se pueda usar para copiar cualquier estado con la probabilidad mencionada anteriormente. 
\end{itemize}
La relajación de la condición de perfección se escribe matemáticamente así.
\begin{equation}\label{eq2.13}\begin{aligned}{\ket{0}_A\ket{Q}_M\textcolor{red}{\Rightarrow} \ket{0}_A\ket{0}_B\ket{Q_0}_M+(\ket{0}_A\ket{1}_B+\ket{1}_A\ket{0}_B)\ket{Y_0}_M} \\ {\ket{1}_A\ket{Q}_M\textcolor{red}{\Rightarrow} \ket{1}_A\ket{1}_B\ket{Q_1}_M+(\ket{0}_A\ket{1}_B+\ket{1}_A\ket{0}_B)\ket{Y_1}_M}\end{aligned}\end{equation}
Para exigir unitariedad a la transformación debe cumplirse
\begin{equation}\label{eq2.14}{\bra{Q_i}_M\ket{Q_i}_M+2\bra{Y_i}_M\ket{Y_i}_M=1, \forall i \in[0,1]}\end{equation}
\begin{equation}\label{eq2.15}{\bra{Y_0}_M\ket{Y_1}_M=\bra{Y_1}_M\ket{Y_0}_M=0}\end{equation}
Si se exige que los estados $\ket{Q_i}_M$ y $\ket{Y_i}_M$ sean ortogonales entre sí, usando \textcolor{blue}{\ref{eq2.13}} se obtiene la misma relación de ortogonalidad para los $\ket{Q_i}_M$
\begin{equation}\label{eq2.16}{\bra{Q_i}_M\ket{Y_i}_M=0, \forall i \in[0,1] \textcolor{red}{\Rightarrow} \bra{Q_0}_M\ket{Q_1}_M=\bra{Q_1}_M\ket{Q_0}=0}\end{equation}
Ojo: los estados entre sí son \textcolor{Red}{ortogonales}, no \textcolor{Red}{ortonormales}. Los productos punto valdrán elementos menores que 1 que son los que se optimizarán. Definiendo 
\begin{equation}\label{eq2.17}{ \xi= \bra{Y_0}_M\ket{Y_0}_M=  \bra{Y_1}_M\ket{Y_1}_M }\end{equation}
\begin{equation}\label{eq2.18}{ \eta= 2\bra{Y_0}_M\ket{Q_1}_M= 2\bra{Y_1}_M\ket{Q_0}_M= 2\bra{Q_0}_M\ket{Y_1}_M= 2\bra{Q_1}_M\ket{Y_0}_M}\end{equation}
Se representa el estado para la transformación \textcolor{blue}{\ref{eq2.11}} en el estado mostrado en \textcolor{blue}{\ref{eq2.4}}
\begin{equation}\label{eq2.19}\begin{aligned}{\ket{s}_A\ket{Q}_M\textcolor{red}{\Rightarrow} \alpha(\ket{0}_A\ket{0}_B\ket{Q_0}_M+(\ket{0}_A\ket{1}_B+\ket{1}_A\ket{0}_B)\ket{Y_0}_M)}\\
{+\beta(\ket{1}_A\ket{1}_B\ket{Q_1}_M+(\ket{0}_A\ket{1}_B+\ket{1}_A\ket{0}_B)\ket{Y_1}_M)}\end{aligned}\end{equation}
En la referencia \textcolor{red}{\cite{bib1}} se dice que por desigualdad de Schwarz:
\begin{equation}{\label{eq2.20} 0\leq\xi\leq\frac{1}{2},\eta\leq\frac{1}{\sqrt{2}}}\end{equation}
Aunque físicamente es más claro ver por qué pasa esto. $\xi^2$ es la probabilidad de que la máquina falle para todo el estado, mientras $\eta$ es la de acertar para solo 1 de las componentes del estado en la base lógica. 
Usando de \textcolor{red}{\ref{eq2.12}} a \textcolor{red}{\ref{eq2.16}}, se escribe la matriz densidad Reducida para $A$ y $B$.
\begin{equation}{\label{eq2.21}\rho_{AB}=\begin{pmatrix} (1-2\xi)\alpha^2 & \frac{\eta}{2}\alpha\beta & \frac{\eta}{2}\alpha\beta & 0 \\ \frac{\eta}{2}\alpha\beta & \xi(\alpha^2+\beta^2) & \xi(\alpha^2+\beta^2) & \frac{\eta}{2}\alpha\beta \\ \frac{\eta}{2}\alpha\beta & \xi(\alpha^2+\beta^2) & \xi(\alpha^2+\beta^2) & \frac{\eta}{2} \alpha\beta \\ 0 & \frac{\eta}{2}\alpha\beta & \frac{\eta}{2}\alpha\beta & (1-2\xi)\beta^2 \end{pmatrix} }\end{equation}
Y la matriz Reducida tanto para $A$ como para $B$ será
\begin{equation}{\label{eq2.22}\rho_A=\rho_B=\begin{pmatrix} (1-\xi)\alpha^2+\xi\beta^2 & \eta\alpha\beta \\ \eta\alpha\beta & \xi\alpha^2 + (1-\xi)\beta^2 \end{pmatrix} }\end{equation}
Calculando la distancia para las matrices recién calculadas considerando la definición de \textcolor{blue}{\ref{eq2.9}} se obtiene
\begin{equation}{\label{eq2.23}D(\rho_{A},\rho_{ideal})=Tr\begin{pmatrix} \xi(\beta^2-\alpha^2)& (\eta-1)\alpha\beta  \\ (\eta-1)\alpha\beta &\xi(\alpha^2-\beta^2) \end{pmatrix}^2=2(\xi^2(2\alpha^2-1)^2+(\eta-1)^2\alpha^2(1-\alpha^2))}\end{equation}
La función (para que cumpla la segunda condición de independencia de los estados no debe depender de $\alpha$
\begin{equation}\label{eq2.24}{\frac{\partial D(\rho_{AB},\rho_{ideal})}{\partial\alpha}=0 \textcolor{red}{\Rightarrow} \eta=1-2\xi}\end{equation}
Esto simplifica considerablemente \textcolor{blue}{\ref{eq2.23}}
\begin{equation}\label{eq2.25}{D(\rho_{A},\rho_{ideal})=2(\xi^2(2\alpha^2-1)^2+(2\xi)^2\alpha^2(1-\alpha^2))=2\xi^2}\end{equation}
Así la distancia solo depende de $\xi$, parámetro que también se puede optimizar para obtener el valor númerico de distancia mínima entre el estado resultante y el ideal. Esto se puede hacer buscando que la distancia entre la matrices resultante conjunta $\rho_{AB}$ y la ideal para ese espacio (usando lo obtenido en \textcolor{blue}{\ref{eq2.24}} tampoco dependa de $\alpha$ ni a segundo órden
\begin{equation}\label{eq2.26}{ \rho_{AB}-\rho_{ideal}=\begin{pmatrix} (1-2\xi)\alpha^2-\alpha^4 & (\frac{1}{2}-\xi)\alpha\beta-\alpha^3\beta& (\frac{1}{2}-\xi)\alpha\beta-\alpha^3\beta& -\alpha^2\beta^2\\ (\frac{1}{2}-\xi)\alpha\beta-\alpha^3\beta& \xi-\alpha^2\beta^2& \xi-\alpha^2\beta^2 &(\frac{1}{2}-\xi)\alpha\beta-\beta^3\alpha\\  (\frac{1}{2}-\xi)\alpha\beta-\alpha^3\beta& \xi-\alpha^2\beta^2& \xi-\alpha^2\beta^2 &(\frac{1}{2}-\xi)\alpha^2 \\ \beta^2-\beta^3\alpha -\alpha^2\beta^2 & (\frac{1}{2}-\xi)\alpha\beta-\beta^3 \alpha& (\frac{1}{2}-\xi)\alpha\beta-\beta^3\alpha & (1-2\xi)\beta^2-\beta^4 \end{pmatrix}}\end{equation}
Optimizando la distancia calculada en la matriz del \textcolor{blue}{\ref{eq2.26}}
\begin{equation}\label{eq2.27}{\frac{\partial^2 D(\rho_{AB},\rho_{ideal}\otimes\rho_{ideal})}{d\alpha^2}=0 \textcolor{red}{\Rightarrow} \xi=\frac{1}{6} \textcolor{red}{\Rightarrow} D(\rho_A, \rho_{ideal})=\frac{1}{18}}\end{equation}
Una forma más de calcular la eficiencia de la medición es la fidelidad, sugerida en \textcolor{red}{\cite{bib2}}
\begin{equation}\label{eq2.28}{F_A=\bra{s}_A\rho_A\ket{s}_A, F_B=\bra{s}_B\rho_B\ket{s}_B}\end{equation}
Donde $\ket{S}_A$ (lo mismo para B) es el definido en \textcolor{blue}{\ref{eq2.4}} y por \textcolor{blue}{\ref{eq2.22}}, ambas fidelidades serán iguales. Calculando para el caso general:
\begin{equation}\label{eq2.29}{F=\alpha^2((1-\xi)\alpha^2+\xi\beta^2)+\beta^2(\xi\alpha^2+(1-\xi)\beta^2)+2\eta\alpha^2\beta^2}\end{equation}
Y reemplazando para lo obtenido en \textcolor{blue}{\ref{eq2.24}} y \textcolor{blue}{\ref{eq2.27}}
\begin{equation}\label{eq2.30}{F=\alpha^2(\frac{5}{6}\alpha^2+\frac{1}{6}\beta^2)+\beta^2(\frac{1}{6}\alpha^2+\frac{5}{6}\beta^2)+\frac{4}{3}\alpha^2\beta^2=\frac{5}{6}}\end{equation}
Que es lo que se esboza en \textcolor{red}{\cite{bib2}}. La matriz densidad resultante luego de hacer los reemplazos tanto para $\rho_A$ como para $\rho_B$ es 
\begin{equation}\label{eq2.31}{\rho_A=\rho_B=\begin{pmatrix}\frac{5}{6} \alpha^2+ \frac{1}{6} \beta^2 & \frac{2}{3} \alpha\beta \frac{2}{3}\alpha\beta & \frac{5}{6} \beta^2 + \frac{1}{6}\alpha^2\end{pmatrix}= \frac{2}{3}\ket{s}\bra{s}+\frac{1}{6}\mathbb{I}}\end{equation}
\subsection{Copiado covariante (dependientes del estado)}
Las distancias entre estado real e ideal y fidelidad para la máquina de copiado universal servirán como una cota para la eficiencia de una máquina de copiado a nivel cuántico. Ninguna máquina de copiado cuántico que funcione para un qubit arbitrario podrá copiar estados con una distancia del caso ideal menos a $\frac{1}{18}$ ni con una fidelidad mayor a $\frac{5}{6}$ (ambos formalismos son equivalentes. 
Esto no significa que no se puedan copiar estados mejores cifras de eficiencia. Lo que implica es que si se quiere construir una máquina de copiado de mejor calidad, será necesario relajar la otra condición señalada al inicio: La de que sea independiente del estado. Acá dos ejemplos de ello siguiendo los desarrollos tanto de \textcolor{red}{\cite{bib1}} como \textcolor{red}{\cite{bib2}}
Si se considera estados de la forma 
\begin{equation}\label{eq2.32}{\ket{\phi}_{cov}=\frac{1}{\sqrt{2}}(\ket{0}+e^{i\varphi}\ket{1}) \textcolor{red}{\Rightarrow} \rho_{ideal}=\frac{1}{2} \begin{pmatrix} 1& e^{-i\varphi} \\ e^{i\varphi} & 1\end{pmatrix}}\end{equation}
Y la siguiente transformación de copiado para estos estados:
\begin{equation}\label{eq2.33}\begin{aligned}{\ket{0}_A\ket{0}_B\textcolor{red}{\Rightarrow} \ket{0}_A\ket{0}_B} \\  {\ket{1}_A\ket{0}_B\textcolor{red}{\Rightarrow} cos\eta\ket{1}_A\ket{0}_B+sin\eta\ket{0}_A\ket{1}_B}\end{aligned}\end{equation}
Se puede escribir $\rho_{AB}$ para estos estados (Nótese que no aparece necesario el estado máquina).
\begin{equation}\label{eq2.34}{\rho_{AB}=\frac{1}{2} \begin{pmatrix} 1 &e^{-i\varphi}sin\eta &e^{-i\varphi}cos\eta &0\\  e^{i\varphi}sin\eta& sin^2\eta & cos\eta sin\eta&0 \\ e^{i\varphi}cos\eta& cos\eta sin\eta & cos^2 \eta &0  \\ 0&0 & 0&0 \end{pmatrix}}\end{equation}
Trazando para A y B \ref{eq2.34}
\begin{equation}\label{eq2.35}{\rho_A=\frac{1}{2}\begin{pmatrix}1+sin^2\eta&e^{-i\varphi}cos\eta  \\ e^{i\varphi}cos\eta & cos^2\eta \end{pmatrix}, \rho_B=\frac{1}{2}\begin{pmatrix} 1+cos^2\eta& e^{-i\varphi}sin\eta\\  e^{i\varphi} sin\eta & sin^2 \eta \end{pmatrix}}\end{equation}
Es fundamental para poder hablar de \textit{copiado} que $\rho_A=\rho_B$. Esto se logra si $\eta=\frac{\pi}{4}$, con lo que la matriz en los 2 subespacios queda
\begin{equation}\label{eq2.36}{\rho_A=\rho_B=\frac{1}{2}\begin{pmatrix} \frac{3}{2} & \frac{e^{-i\varphi}}{\sqrt{2}} \\  \frac{e^{i\varphi}}{\sqrt{2}} & \frac{1}{2} \end{pmatrix}}\end{equation}
Y entonces se calcula la fidelidad de la máquina al estado orginal:
\begin{equation}\label{eq2.37}{F=\bra{s}_A\rho_A\ket{s}_A=\frac{1}{2}(1+\frac{1}{\sqrt{2}})\simeq 08535 > 0.8333...=\frac{5}{6}}\end{equation}
\subsection{Máquina Probabilista de copiado}
Otro enfoque para observar copiado cuántico es el enfoque de Dan y Guo, propuesto en 2000 y reseñado en \cite{bib3} y \cite{bib4}. Los textos (se consultó los 2 mencionados anteriormente además de \cite{bib2} y todos lo decían de manera similar) hablan por lo general que estos son copiados "con fidelidad perfecta pero con probabilidad de éxito y falla". Esto parece contradecir el teorema de no cloning, pero haciendo el desarrollo quedará más claro.

Se tiene 2 estados $\ket{\phi_0}_A$ y $\ket{\phi_1}_A$ no necesariamente ortogonales
\begin{equation} \label{eq2.38}{ \bra{\phi_0}\ket{\phi_1} =\alpha }\end{equation}

Y se hace una máquina de copiado cuántico que copia ambos estados con cierta probabilidad y que el resto de posibilidades las condensa a un solo estado que reúne todas las posibilidades de error.
\begin{equation} \label{eq2.39}{ \ket{\phi_0}_A\ket{\Sigma}_B\ket{m_i}_M\textcolor{red}{\Rightarrow} \sqrt{\eta_0}\ket{\phi_0}_A\ket{\phi_0}_B\ket{m_0}_M+\sqrt{1-\eta_0}\ket{\Phi_0}_{ABM}}\end{equation} 
\begin{equation} \label{eq2.40} {\ket{\phi_1}_A\ket{\Sigma}_B\ket{m_i}_M\textcolor{red}{\Rightarrow} \sqrt{\eta_1}\ket{\phi_1}_A\ket{\phi_1}_B\ket{m_1}_M+\sqrt{1-\eta_1}\ket{\Phi_1}_{ABM}}\end{equation} 
Los estados máquina son elegidos de manera que sean ortogonales entre sí. De esta forma, midiendo en $M$ se sabrá si la máquina funciona o falla. Considerando $\eta_0=\eta_1$ (lo que también iguala los estados máquina y los Reduce a 1 estado de éxito y 1 de falla) multiplicar escalarmente \ref{eq2.39} y \ref{eq2.40} genera
\begin{equation}\label{eq2.41}{\alpha=\eta\alpha^2+(1-\eta)}\end{equation}
Al ser el copiado propuesto una transformación unitaria, la norma en el lado izquierdo (antes de la transformación) y derecho (después de la transformación) deben ser iguales. Esto se optimiza cuando, al resolver la ecuación cuadrática para $\alpha$ que aparece en \ref{eq2.41}
\begin{equation}\label{eq2.42} \begin{aligned}{ \eta\alpha^2-\alpha+(1-\eta) \textcolor{red}{\Rightarrow} \alpha=\frac{1\pm\sqrt{1-4(1-\eta)\eta}}{2\eta} } \\ {\textcolor{red}{\Rightarrow} \alpha= \frac{1}{2\eta}\pm \frac{2\eta-1}{2\eta}= \{1, \frac{1}{\eta}-1\}} \end{aligned}\end{equation}
Considerando que $\lvert\alpha\rvert$ siempre es positivo
\begin{equation}\label{eq2.43}{\frac{1}{\eta}-1\geq \lvert\alpha\rvert \textcolor{red}{\Rightarrow} \eta \leq \frac{1}{1+\lvert\alpha\rvert} }\end{equation}
con lo que se recupera el resultado de no-cloning: $\eta$ (o sea, la probabilidad de acertar en el clonado) es 1 solo cuando $\alpha=0$, es decir, cuando los estados son ortogonales entre sí.
\section{Implementación en sistemas físicos de máquinas de clonado}
\subsection{Implementación de máquinas de copiado universal en circuitos}
Para poder ver cómo funciona una máquina de copiado cuántico más allá de lo meramente teórico, es interesante observar lo realizado en \cite{bib3}. 
\begin{figure}[ht][ht]
\label{cir1}
\includegraphics[width=0.6\textwidth]{circuito.png}
\caption{Representación gráfica de lo que sería una máquina de copiado en formalismo de circuitos. Está compuesta de 4 compuertas C-NOT que entrelazan los espacios A, B y S. A es el espacio donde se encuentra el estado a copiar, B es la "hoja en blanco" y M el estado máquina. tomada de \cite{bib3} }
\end{figure}
Formalmente, lo que realiza el circuito mostrado en figura 1 es:
\begin{equation}\label{eq2.44}{ \ket{\alpha}_A\ket{\beta}_B\ket{\xi}_M\textcolor{red}{\Rightarrow}\ket{\alpha\oplus\beta\oplus\xi}_A\ket{\alpha\oplus\beta}_B\ket{\alpha\oplus\xi}_M}\end{equation}
Se aplica \ref{eq2.44} en los siguientes 2 casos particulares: Uno donde se coloca el estado de \ref{eq2.4} en A (el estado que se quiere copiar) y un estado maximalmente entrelazado para B y M
\begin{equation}\label{eq2.45} \begin{aligned} {\ket{s}_A\ket{\Phi^+}_{BM}=\frac{\alpha}{\sqrt{2}}(\ket{000}+\ket{011})+\frac{\beta}{\sqrt{2}}(\ket{100}+\ket{111})} \\ {\textcolor{red}{\Rightarrow} \frac{\alpha}{\sqrt{2}}(\ket{000}+\ket{011})+\frac{\beta}{\sqrt{2}}(\ket{111}+\ket{100})}\end{aligned} \end{equation}
y otro estado en el que el sistema es con el estado de \ref{eq2.4} en A, $\ket{0}$ en B y el estado $\ket{+X}$ en M 
\begin{equation}\label{eq2.46} \begin{aligned}{\ket{s}_A\ket{0}_B\ket{+}_M=\frac{\alpha}{\sqrt{2}}(\ket{000}+\ket{001})+\frac{\beta}{\sqrt{2}}(\ket{100}+\ket{101}}\\ {\textcolor{red}{\Rightarrow} \frac{\alpha}{\sqrt{2}}(\ket{000}+\ket{101})+\frac{\beta}{\sqrt{2}}(\ket{111}+\ket{010})=\ket{s}_B\ket{\Phi^+}_{AM}}\end{aligned}\end{equation}
Se observa que mientras para el estado de \ref{eq2.45} no produce cambios, el de \ref{eq2.46} copia la información del estado de A a B. Se superponer ambos estados posibles para preparar el estado:
\begin{equation}\label{eq2.47}\begin{aligned}{\ket{s}_A\ket{\phi^{i}}_{BM}=a(\ket{s}_A\ket{\Phi^+}_{BM})+b(\ket{s}_A\ket{0}_B\ket{+}_M)} \\ {=a(\frac{\alpha}{\sqrt{2}}(\ket{000}+\ket{011})+\frac{\beta}{\sqrt{2}}(\ket{100}+\ket{111}))+}\\ {b(\frac{\alpha}{\sqrt{2}}(\ket{000}+\ket{001})+\frac{\beta}{\sqrt{2}}(\ket{100}+\ket{101}))}\\   {=\frac{\alpha(a+b)}{\sqrt{2}}\ket{000}+\frac{b\alpha}{\sqrt{2}}\ket{001}+\frac{a\alpha}{\sqrt{2}}\ket{011}+} \\  {\frac{\beta(a+b)}{\sqrt{2}}\ket{100}+\frac{\beta b}{\sqrt{2}}\ket{101}+\frac{\beta a}{\sqrt{2}}\ket{111}}\end{aligned}\end{equation}
A los coeficientes $a$ y $b$ se les impone, por normalización:
\begin{equation}\label{eq2.48}{\frac{1}{2}\alpha^2+\beta^2((a+b)^2+a^2+b^2)=\frac{2(a^2+b^2+ab)}{2}=a^2+b^2+ab=1}\end{equation}
El estado de \ref{eq2.47} se pasa por el circuito, quedando:
\begin{equation}\label{eq2.49}\begin{aligned}{\ket{s}_A\ket{\phi^{f}}_{BM}
=\frac{\alpha(a+b)}{\sqrt{2}}\ket{000}+\frac{b\alpha}{\sqrt{2}}\ket{101}+\frac{a\alpha}{\sqrt{2}}\ket{011}+} \\ {\frac{\beta(a+b)}{\sqrt{2}}\ket{111}+\frac{\beta b}{\sqrt{2}}\ket{010}+\frac{\beta a}{\sqrt{2}}\ket{100}}\end{aligned}\end{equation}
Y las matrices densidad Reducida, tanto para A como para B tendrán la forma:
\begin{equation}\label{eq2.50}{\rho_A=\frac{1}{2}\begin{pmatrix}\alpha^2((a+b)^2+a^2)+\beta^2 b^2 & \alpha\beta(1-b^2)  \alpha\beta(1-b^2) & \beta^2((a+b)^2+a^2)+\alpha^2 b^2 \end{pmatrix}}\end{equation}
\begin{equation}\label{eq2.51}{\rho_B=\frac{1}{2}\begin{pmatrix}\alpha^2((a+b)^2+b^2)+\beta^2 a^2 & \alpha\beta(1-a^2)  \alpha\beta(1-a^2) & \beta^2((a+b)^2+b^2)+\alpha^2 a^2 \end{pmatrix}}\end{equation}
Considerando \ref{eq2.4} se tiene que 
\begin{equation}\label{eq2.52}{\ket{s}\bra{s}=\begin{pmatrix} a^2 & ab  ab & b^2 \end{pmatrix} \textcolor{red}{\Rightarrow}  \ket{s^\perp}=\begin{pmatrix}b  -a \end{pmatrix} \ket{s^\perp}\bra{s^\perp}=\begin{pmatrix} b^2 & -ab -ab & a^2\end{pmatrix} }\end{equation}
Con esto las matrices de \ref{eq2.50} y \ref{eq2.51} se simplifican
\begin{equation}\label{eq2.53}{\rho_A=(1-\frac{b^2}{2})\ket{s}\bra{s}+ \frac{b^2}{2}\ket{s^\perp}\bra{s^\perp}=(1-b^2)\ket{s}{s}+\frac{b^2}{2} \mathbb{I}}\end{equation}
\begin{equation} \label{eq2.54}{ \rho_B= (1-\frac{a^2}{2})\ket{s}\bra{s}+\frac{b^2}{2}\ket{s^\perp} \bra{s^\perp} =(1-a^2)\ket{s}\bra{s}+\frac{a^2}{2}\mathbb{I}}\end{equation} 
De \ref{eq2.47} y \ref{eq2.51} se declara que, como en ambos espacios se obtiene el estado $\ket{s}$ de manera probabilista, se puede considerar el resultado como una  máquina de copiado con las fidelidades
\begin{equation} \label{eq2.55}{ F_A=1-\frac{b^2}{2}, F_B=1-\frac{a^2}{2}}\end{equation}
Si se exige que $a$ y $b$ sean iguales, combinando con la condición de normalización
\begin{equation} \label{eq2.56}{ a=b=\frac{1}{\sqrt{3}} \textcolor{red}{\Rightarrow} F=F_A=F_B=1-\frac{1}{3}\frac{1}{2}=\frac{5}{6} }\end{equation} 
Se recupera la máquina de clonado universal. Lo que también se observa al reemplazar en \ref{eq2.50} y \ref{eq2.51}
\subsection{Copiado universal para N dimensiones}
Para hallar el clonado universal para estados de mayor dimensión a un qubit, Para $d>2$ el procedimiento es análogo. Solo que los siguientes estados cambian:
\begin{equation}  \label{eq2.57}{ \ket{\Phi^+}_{BM}=\frac{1}{\sqrt{2}}\sum_{k=0}^{d-1}  \ket{k}_B \ket{k}_M }\end{equation}
\begin{equation} \label{eq2.58}{\ket{+}_{M}=\frac{1}{\sqrt{2}\sum_{k=0}^{d-1} \ket{k}_M }}\end{equation}
Además de considerar el $\ket{0}_B$ como el vector 0 de la dimensión de B. El circuito se aplica a estos estados de similar forma
\begin{equation} \label{eq2.59}      
{\ket{k_1}_A\ket{k_2}_B\ket{k_3}_M\textcolor{red}{\Rightarrow} \ket{k_1\oplus k_2 \oplus k_3}_A\ket{k_2 \oplus k_1}_B\ket{k_3 \oplus k_1}_M
}\end{equation}
Donde las sumas ahora son en binario para todo el espacio de tamaño N. Dicho esto, los casos particulares a superponer (de manera análoga al cálculo anterior) son:
\begin{equation} \label{eq2.60}{     \ket{s}_A\ket{\Phi^+} \textcolor{red}{\Rightarrow} \ket{s}_A
\ket{\Phi^+}_{BM} 
}\end{equation}
\begin{equation} \label{eq2.61} {     
\ket{s}_A\ket{0}_B\ket{+}_M\textcolor{red}{\Rightarrow} \ket{s}_B\ket{\Phi^+}_{AM}
}\end{equation}
Con lo que el estado completo superpuesto (a la manera del cálculo anterior) quedará:
\begin{equation} \label{eq2.62}      
\begin{aligned}{ a(\ket{s}_A\ket{\Phi^+}_{BM}+b(\ket{s}_A\ket{0}_B\ket{+}_M)\textcolor{red}{\Rightarrow}}  {a(\ket{s}_A\ket{\Phi^+}_{BM})+b( \ket{s}_B\ket{\Phi^+}_{AM})}
\end{aligned}\end{equation}
El estado completo sigue la siguiente normalización
\begin{equation} \label{eq2.63} {\frac{d(a+b)^2+(d-1)(a^2+b^2)} {d} = a^2+b^2+\frac{2ab}{d}=1 }\end{equation} 
La matriz densidad Reducida tanto para A como para B es
\begin{equation} \label{eq2.64} {
\rho_A=(1-b^2)\ket{s}\bra{s}_A + \frac{b^2}{d},
\rho_B=(1-a^2)\ket{s}\bra{s}_A + \frac{a^2}{d}
 \mathbb{I} }\end{equation}
Por lo tanto, de manera análoga a lo anterior, se puede considerar esto una clonación con las fidelidades
\begin{equation} \label{eq2.65}{ 
F_A=1-b^2+\frac{b^2}{d}, F_B=1-a^2+\frac{a^2}{d}
}\end{equation}
Y si se exige que $a=b$, combinado con la ecuación de normalización de \ref{eq2.60}, resulta la obtención de la fidelidad para las copias de un estado de dimensión N:
\begin{equation}\label{eq2.66}{ a=b \textcolor{red}{\Rightarrow} a^2=b^2=\frac{d}{2(d+1)}\textcolor{red}{\Rightarrow} F_A=F_B=1-\frac{d-1}{d}\frac{d}{2(d+1)} =\frac{d+3}{2(d+1)}}\end{equation}
Para $d=2$ se obtiene inmediatamente el resultado anterior de $\frac{5}{6}$.
\subsection{Esquema para realizar una máquina de copiado}
\begin{figure}[ht][ht]
\centering
\label{cir2}
\includegraphics[width=0.6\textwidth]{circuito2.png}
\caption{Representación gráfica de lo que sería la máquina de copiado para un sistema. Incluye, además del circuito ya visto, un espacio de preparación del estado en B y M. Consta de 3 operadores de rotación que pueden variar su ángulo, preparando el estado para una máquina de copiado universal o de copiado covariante. Tomado de \cite{bib4}.}
\end{figure}
\begin{thebibliography}{9}
\bibitem{bib1} Buzek, Hillery. PRA 54,1844(1996)
\bibitem{bib2} Jimenez. Apuntes Información Cuántica 1. (2019)
\bibitem{bib3} Scarani, Iblisdir et al Rev.Mod.Phys 77,1225(2005)
 \bibitem{bib4} Fang, Wang, et. al. Physics Reports 544, 3, 20 (2014) 
\end{thebibliography}

\chapter{Termodinámica Cuántica}
\section{Motivaciones}
La termodinámica, tal y como es conocida a nivel clásico, fue concebida para analizar las propiedades de aparatos de muchas piezas cuyo comportamiento individual no se considera realmente relevante. Fue desarrollada durante la Primera Revolución Industrial, pensando en máquinas mecánicas a vapor. Con todo, constituye una especie de \textit{pegamento teórico} entre las distintas Mecánicas que evaluán evoluciones temporales de sistemas a distintas escalas (Relativista, Clásica y Cuántica). Funciona razonablemente en los 3 formalismos y define variables relativamente sencillas de entender intuitivamente y aplicar experimentalmente. Yendo hacia la Mecánica Cuántica, con todo lo aprendido sobre sistemas cuánticos cerrados y abiertos. ¿Qué se puede observar? ¿Tendrá alguna influencia en los resultados las características propias de la Mecánica Cuántica (incerteza, no conmutatividad, teoría de medida, entrelazamiento de estados)?
\section{La Termodinámica como resultado del entrelazamiento}
Si. Este título sorprende. Pero es cierto. Y muy poderoso. La siguiente sección intentará mostrar que esto es cierto. Como es sabido, la Termodinámica desde el punto de vista meramente clásico, es una teoría \textcolor{red}{fenomenológica}, es decir, se deriva de su ajuste a experimentos, más a una razón fundamental (en el sentido de Física Fundamental). A continuación, siguiendo el formalismo de \cite{bib3.1} se enuncian los 5 axiomas desde los que se deriva la Termodinámica como teoría: 
\subsection{Axiomas de la Termodinámica}
\begin{enumerate}
    \item Conocida como \textit{Ley Cero de la Termodinámica}. 2 sistemas están en equilibrio térmico entre sí se encuentran en equilibrio con un un tercero. Esto matemáticamente se puede expresar como la existencia de las ecuaciones de estado para sistemas en equilibrio, tales que: 
    \begin{equation}\label{eq3.1}{f(V,P,T)=0}\end{equation}
    \item Conocida como \textit{Primera Ley de la Termodinámica}. Considerando un sistema \textcolor{red}{cerrado}, ante los procesos térmicos, mecánicos y químicos dentro de dicho sistema, la energía total interna se conserva. La \textcolor{red}{Energía Interna} entonces se puede considerar la energía promedio de las partículas dentro del sistema:
    \begin{equation}\label{eq3.2}{dE=\delta Q+\delta W= \delta Q+ \delta W_M + \delta W_C}\end{equation} Aclaración: El diferencial de energía interna si es un diferencial completo, pero los sumandos en \ref{eq3.2} no tiene por qué serlo. 
    \item Conocida como \textit{Segunda Ley la Termodinámica}. El intercambio de energía entre sistemas cercanos al equilibrio (e incluso no cercanos a él) solo aumenta la \textcolor{red}{entropía}, que se puede definir para tales efectos como un indicador de la dependencia y \textit{fuerza} del movimiento interno de las partículas dentro de la energía interna total:
    \begin{equation}\label{eq3.3}{\int\frac{\delta Q}{t}=\Delta S \geq 0  }\end{equation}
    \item Conocida como \textit{Tercera Ley la Termodinámica} o \textit{Principio de Nernst}. Al bajar la temperatura hasta el cero absoluto, el cambio de entropía definido en la ley anterior tiende a ser nulo. Lo que equivale a considerar que las partículas se moverán cada vez menos y, por lo tanto, su movimiento influirá cada vez menos en la energía total hasta estabilizarse en un valor estático.
    \begin{equation}\label{eq3.4}{\lim_{T\textcolor{red}{\Rightarrow} 0} \Delta S=0}\end{equation}
    \item Conocida como \textit{Cuarta Ley de la Termodinámica}. Esta ley define de manera más rigurosa qué es un sistema en equilibrio. Un sistema está \textcolor{red}{En equilibrio termodinámico} si los flujos entre sus variables internas son \textit{balanceados}. Esto se establece definiendo lo siguiente para los valores de entropía la derivada temporada considerando las variables dinámicas y definiendo \textit{fuerzas} dependientes de esas variables: 
    \begin{equation}\label{eq3.5}{\dot{s}=\sum_k\frac{\partial S}{\partial x_k}\dot{x_k}=\sum_k\frac{\partial S}{\partial x_k}J_k}\end{equation}
    Se puede construír entonces una matriz de coeficientes cinéticos:
    \begin{equation}\label{eq3.6}{L_{j,k}=\frac{\partial J_k}{\partial F_j}\rvert_{F_j=0}}\end{equation}
    Los cuales cumplen con la condición de simetría ${L_{j,k}=L_{k,j}}$
\end{enumerate}
    Todas las leyes mencionadas anteriormente funcionan para sistemas \textcolor{red}{cuasiestáticos}, es decir, que interactúan con un ambiente a un valor suficientemente pequeño como para ser reversibles y mantener el equilibrio termodinámico. Cuando esto no se cumple, se pasa al área de Sistemas termodinámicos fuera del equilibrio. De todas formas, se puede trabajar con dichos sistemas asumiendo que tienen un comportamiento cuasiestático para cambios pequeños (similar al paso del discreto al contínuo en Cálculo).
\subsection{Sistemas Cuánticos Envariantes}
    Como se dijo antes, el gran problema de todas estas teorías es que son de orden fenomenológico. Felizmente, la mecánica cuántica tiene particularidades que permiten modelar las relaciones de tipo cuasiestáticas de manera más fundamental, sin insertar conceptos ficticios aproximados (que es lo que por lo general se hace en los cursos de Termodinámica). Acá se hace clave el concepto de \textcolor{red}{Envariancia}, que se basa en el entrelazamiento de estados cuánticos y que se explicará en los siguientes párrafos:
    
    Si se considera un sistema interactuando con un ambiente (considerando que dicho ambiente no es necesariamente mucho más grande que el sistema), es deseable para modelar un proceso cuasiestático de manera no aproximada tener una transformación en el sistema que pueda invertirse aplicando una transformación en el ambiente. 
    A nivel clásico esto no se puede hacer, porque los sistemas son locales, si se hacen cambios en un sistema solo se pueden revertir en el mismo sistema. 
    Como es sabido (al menos para quien ha estudiado paradoja EPR y Teoremas de Bell), la Mecánica Cuántica es \textcolor{red}{no-local}, teniendo ese efecto de no localidad más acentuado cuando el sistema está lo más entrelazado posible con el ambiente. 
    Dicho esto, se puede definir el concepto de Envariancia (Dicho en inglés como \textit{envariance}, contracción de \textit{enviroment invariance}): Sea un estado en un sistema maximalmente entrelazado con un ambiente $\ket{\phi}_{SE} \in \mathbb{S}\otimes\mathbb{E}$ tal que existe una transformación en $\mathbb{S}$ (el espacio del sistema) que puede invertirse aplicando otra transformación en $\mathbb{E}$ (el espacio del ambiente):
    \begin{equation}\label{eq3.7}(U_{S}\otimes\mathbb{I}_E)\ket{\phi}_{SE}=\ket{\eta}_{SE}\end{equation}
    \begin{equation}\label{eq3.8}(\mathbb{I}_S\otimes U_{E})\ket{\eta}_{SE}=(\mathbb{I}_S\otimes U_{E})(U_{S}\otimes\mathbb{I}_E)\ket{\phi}_{SE}=\ket{\phi}_{SE}\end{equation}
    La envariancia se puede ver con un ejemplo simple: Considerando un sistema y ambiente de un qubit (por lo que el estado maximalmente entrelazado a evaluar coincide con alguno de los estados de Bell), Tomando uno de los estados al azar se puede mostrar que será envariante bajo la transformación:
    \begin{equation}\label{eq3.9}(\sigma_x\otimes\mathbb{I})\ket{\Phi^+}=\ket{\Psi^+}\end{equation}
    \begin{equation}\label{eq3.10}(\mathbb{I}\otimes\sigma_x)\ket{\Psi^+}=(\mathbb{I}\otimes\sigma_x)(\sigma_x\otimes\mathbb{I})\ket{\Phi^+}=\ket{\Phi^+}\end{equation}
    Como se puede ver, la propiedad de envariancia corresponde a una simetría asistida, y es muy útil para definir un proceso cuasiestático. No es una propiedad meramente matemática, se ha encontrado experimental y numéricamente.
\subsection{Ensembles Microcanónico y Canónico a partir de la envariancia}
Se puede escribir la descomposición de Schmidt del estado mixto maximalmente entrelazado
\begin{equation}\label{eq3.11}\ket{\phi}_{SE}=\sum_k a_k\ket{s_k}\otimes\ket{e_k}\end{equation}
Y también se pueden escribir las transformaciones unitarias para cada subespacio ($\mathbb{S}$ y $\mathbb{E}$) de manera que cumplan la propiedad deseada.
\begin{equation}\label{eq3.12} U_S=\sum_{k=1}^N e^{i\phi_k}\ket{s_k}\bra{s_k}\end{equation}
\begin{equation}\label{eq3.13}U_E=\sum_{k=1}^N e^{-i(\phi_k+2\pi l_k)}\ket{e_k}\bra{e_k}\end{equation}
\begin{equation}\label{eq3.14} \begin{aligned} \textcolor{red}{\Rightarrow} (U_S\otimes \mathbb{I}_E)(\mathbb{I}_S\otimes U_E)\\ = (\sum_{k,j=1}^N e^{i\phi_k}\ket{s_k}\bra{s_k}\otimes \ket{e_j}\bra{e_j}) (\sum_{l,m=1}^N\ket{s_l}\bra{s_l}\otimes e^{-i(\phi_m+2\pi l_m)}\ket{e_m}\bra{e_m})\\ = \sum_{k,j=1}^N e^{i\phi_k}\ket{s_k}\bra{s_k}\otimes e^{-i(\phi_j+2\pi l_j)}\ket{e_j}\bra{e_j}\end{aligned}\end{equation}
Acá se han aprovechado las bases ortonormales y completas de los subespacios basadas en la descomposición de Schmidt de \ref{eq3.11}, considerando que ambos tienen la misma dimensión $N$. Aplicar la matriz resultante al estado maximalmente entrelazado genera: 
\begin{equation}\label{eq3.15}\begin{aligned}(U_S\otimes \mathbb{I}_E)(\mathbb{I}_S\otimes U_E)\ket{\phi}_{SE}=\\ (\sum_{k,j=1}^N e^{i\phi_k}\ket{s_k}\bra{s_k}\otimes e^{-i(\phi_j+2\pi l_j)}\ket{e_j}\bra{e_j})(\sum_{l=1}^N a_l\ket{s_l}\otimes\ket{e_l})\\ = \sum_{l=1}^N e^{i\phi_l }a_l \ket{s_l}\otimes e^{-i(\phi_l+2\pi l_l)}\ket{e_l}=\sum_l a_l \ket{s_l}\otimes\ket{e_l}=\ket{\phi}_{SE} \end{aligned}\end{equation}Si se escribe el estado de \ref{eq3.11} como matriz densidad queda como:
\begin{equation}\label{eq3.16}\rho_{SE}=\sum_k\lvert a_k\rvert^2\ket{s_k,e_k}\bra{s_k,e_k}\end{equation}
Se puede pensar este estado como un estado puro resultado o de una medición o de decoherencias. Todos los resultados anteriores pueden llevar a un resultado interesante: una transformación unitaria envariante tiene los mismos autovalores que los resultantes de la descomposición de Schmidt del estado maximalmente entrelazado. Se prueba esto por contradicción: 

Una operación unitaria $U_S$ no codiagonal con la base de Schmidt generada con el estado definido en \ref{eq3.11} opera en la forma:
\begin{equation}\label{eq3.17} (U_S\otimes\mathbb{I}_E)\ket{\phi}_{SE}=\sum_k a_k(U_S\ket{s_k})\otimes\ket{e_k}\end{equation}
Y si cumple con la envariancia se cumple:
\begin{equation}\label{eq3.18} \begin{aligned} (\mathbb{I}_S\otimes U_E)\sum_k a_k(U_S\ket{s_k})\otimes\ket{e_k}= \\ \sum_k a_k (U_S\ket{s_k})\otimes (U_E\ket{e_k})=\ket{\phi}_{SE} \end{aligned} \end{equation}
Las transformaciones unitarias en un espacio solo pueden actuar en ese espacio, por lo que a partir de \ref{eq3.17} se puede igualar considerando las descomposiciones de \ref{eq3.11}, \ref{eq3.12} y \ref{eq3.13}:
\begin{equation}\label{eq3.19} U_S\ket{s_k}=e^{i\phi_k}\ket{s_k}, U_E\ket{e_k}=e^{-i(\phi_k+2\pi l_k)}\ket{e_k} \end{equation}
Y lo obtenido en \ref{eq3.18} se hace cierto. Lo que implica que las transformaciones unitarias deben ser, por contradicción, codiagonales con la base de Schmidt. 
 

\section{Construcción de la termodinámica a partir de principios cuánticos}
El concepto de envariancia es muy útil dado que permite construír los ensembles microcanónico y canónico a partir de primeros principios. En esta sección, se demostrará lo mencionado.
\subsection{Ensembles y Entropía de Von Neumann}
Tomando el concepto de envariancia, un estado será \textcolor{red}{maximalmente envariante} si todos los operadores unitarios en $\mathbb{S}$ cumplen con la propiedad de envariancia. Usando la propiedad de \ref{eq3.19} (la codiagonalidad de una transformación unitaria con la base de Schmidt construida con el estado envariante), se puede escribir
\begin{equation}\label{eq3.20} \ket{\phi}_{SE}=\sum_k a_k\ket{s_k}\otimes\ket{e_k}\textcolor{red}{\Rightarrow} U_S=\sum_{k=1}^N e^{i\phi_k}\ket{s_k}\bra{s_k} \end{equation}
\subsection{Estados Pasivos y Pasivos Completos}
\subsection{Redefinición de los axiomas termodinámicos}
Con todo lo anterior, se puede reescribir los axiomas presentados al principio de la exposición de la siguiente forma:
\begin{enumerate}
    \item Conocida como \textit{Ley Cero de la Termodinámica}. 2 sistemas están en equilibrio térmico entre sí se encuentran en equilibrio con un un tercero. Esto matemáticamente se puede expresar como la existencia de las ecuaciones de estado para sistemas en equilibrio, tales que: 
    \begin{equation}\label{eq3.1c}{f(V,P,T)=0}\end{equation}
    \item Conocida como \textit{Primera Ley de la Termodinámica}. Considerando un sistema \textcolor{red}{cerrado}, ante los procesos térmicos, mecánicos y químicos dentro de dicho sistema, la energía total interna se conserva. La \textcolor{red}{Energía Interna} entonces se puede considerar la energía promedio de las partículas dentro del sistema:
    \begin{equation}\label{eq3.2c}{dE=\delta Q+\delta W= \delta Q+ \delta W_M + \delta W_C}\end{equation} Aclaración: El diferencial de energía interna si es un diferencial completo, pero los sumandos en \ref{eq3.2} no tiene por qué serlo. 
    \item Conocida como \textit{Segunda Ley la Termodinámica}. El intercambio de energía entre sistemas cercanos al equilibrio (e incluso no cercanos a él) solo aumenta la \textcolor{red}{entropía}, que se puede definir para tales efectos como un indicador de la dependencia y \textit{fuerza} del movimiento interno de las partículas dentro de la energía interna total:
    \begin{equation}\label{eq3.3c}{\int\frac{\delta Q}{t}=\Delta S \geq 0  }\end{equation}
    \item Conocida como \textit{Tercera Ley la Termodinámica} o \textit{Principio de Nernst}. Al bajar la temperatura hasta el cero absoluto, el cambio de entropía definido en la ley anterior tiende a ser nulo. Lo que equivale a considerar que las partículas se moverán cada vez menos y, por lo tanto, su movimiento influirá cada vez menos en la energía total hasta estabilizarse en un valor estático.
    \begin{equation}\label{eq3.4c}{\lim_{T\textcolor{red}{\Rightarrow} 0} \Delta S=0}\end{equation}
    \item Conocida como \textit{Cuarta Ley de la Termodinámica}. Esta ley define de manera más rigurosa qué es un sistema en equilibrio. Un sistema está \textcolor{red}{En equilibrio termodinámico} si los flujos entre sus variables internas son \textit{balanceados}. Esto se establece definiendo lo siguiente para los valores de entropía la derivada temporada considerando las variables dinámicas y definiendo \textit{fuerzas} dependientes de esas variables: 
    \begin{equation}\label{eq3.5c}{\dot{s}=\sum_k\frac{\partial S}{\partial x_k}\dot{x_k}=\sum_k\frac{\partial S}{\partial x_k}J_k}\end{equation}
    Se puede construír entonces una matriz de coeficientes cinéticos:
    \begin{equation}\label{eq3.6c}{L_{j,k}=\frac{\partial J_k}{\partial F_j}\rvert_{F_j=0}}\end{equation}
    Los cuales cumplen con la condición de simetría ${L_{j,k}=L_{k,j}}$
\end{enumerate}

\section{Aplicaciones a Sistemas Cuánticos}
\subsection{Dinámica de Sistemas Cuánticos abiertos}
\subsection{Heralded Photons}
\begin{thebibliography}{9}
\bibitem{bib3.1} Deffner S., Campbell S. Quantum Thermodynamics, an introduction to the Thermodynamics of Quantum Information. arXiv:1907.01596v1, 2 Jul 2019.
\bibitem{bib3.2} Potts P.P., Introduction to Quantum Thermodynamics. arXiv:1906.07439v1, 18 Jun 2019.
\bibitem{bib3.3} Alicki R., Kossloff R., Introduction to Quantum Thermodynamics: History and prospects, arXiv:1801.08314v2 31 May 2018
\bibitem{bib3.4} Zurek, W.H. Phys. Rev. A 71, 052105 s2005d
\bibitem{bib3.5} Horoshko D.B., De Brieve S, et. al. arXiv:1906.09664v2 ,5 Aug 2019
\bibitem{bib3.6} Thingna J., Barra F., Phys. Rev. E 96, 052132 (2017)
\end{thebibliography}
\chapter{Principios de Incerteza Entrópica}
\section{Introducción}
\begin{itemize}
    \item Es conocida la \textcolor{red}{Incerteza de Heisenberg}
    \begin{equation}\label{eq4.1}\Delta P \Delta Q \geq \frac{ \hslash}{2}\end{equation}
    \item También en su forma más general
    \begin{equation}\label{eq4.2}\Delta X\Delta Y \geq  \frac{\lvert\bra{\phi}[X,Y]\ket{\phi}\rvert}{2}\end{equation}
    \item Incerteza: Cotas que pueden depender o no del estado.
    \item Pero tienen una desventaja: \textcolor{red}{No incluyen la medida}
\end{itemize}
\begin{itemize}
    \item Medida $\textcolor{red}{\Rightarrow}$ Probabilidad Condicional $\textcolor{red}{\Rightarrow}$ \textcolor{red}{¡Entropía!}
    \item Con $\Gamma(Q)$ una distribución de probabilidad
    \begin{equation}\label{eq4.3}h(Q)=\int_{-\infty}^\infty \Gamma(Q) Log\Gamma(Q) \end{equation}
    \item En general se cumple que (la igualdad si la distribución es Gaussiana
    \begin{equation}\label{eq4.4}h(Q)\leq log \sqrt{2\pi e \Delta Q},   h(P)\leq log \sqrt{2\pi e \Delta P}\end{equation}
    \item  Resultado: \textcolor{red}{Principio de Incerteza Entrópica} (Beckner(1975))
    \begin{equation}\label{eq4.5}log(2\pi e \Delta(Q)\Delta(P))\geq h(Q)+h(P) \geq log(e\pi) \end{equation}
\end{itemize}
\section{Tipos de Incerteza}
\begin{itemize}
    \item Objetivo de estudio: \textcolor{red}{Encontrar cotas y formas de medición}
    \item Von Neumann: Peso de acuerdo a la cantidad de información.
    \item Es de interés tener \textcolor{red}{otras} mediciones de Entropía que le den más o menos peso a eventos con mucha o poca información.
    \item La \textcolor{red}{Entropía de Renyi} está dada por:
    \begin{equation}\label{eq4.6}H_\alpha(X)\frac{1}{1-\alpha} log\sum_x P(X)x^\alpha \end{equation} Esto para cualquier $\alpha$ entre 0 e infinito.
\end{itemize}
\begin{itemize}
    \item $\alpha=1 \textcolor{red}{\Rightarrow}$ Entropía de Shannon (como límite). 
    \item $\alpha=\frac{1}{2} \textcolor{red}{\Rightarrow} $ \textcolor{red}{Max-entropía}, Da más peso a eventos con menos información.
    \item $\alpha=2\textcolor{red}{\Rightarrow}$ \textcolor{red}{Entropía de Colisión}, pondera las probabilidades de que 2 eventos X distintos sean iguales. 
    \item $\alpha=\infty\textcolor{red}{\Rightarrow}$ \textcolor{red}{Min-Entropía}, equivale a la probabilidad óptima de adivinar X
    \begin{equation}\label{eq4.7}H_{min}=-log( max_x P(X))\end{equation}
    \item $\alpha=0 \textcolor{red}{\Rightarrow} $ El logaritmo de la cardinalidad del Espacio. 
\end{itemize}
\subsection{Incertezas sin memoria}
\begin{itemize}
    \item Las matrices de Pauli tienen como autovectores
    \begin{equation}\label{eq4.8}\ket{x^+}=\frac{1}{\sqrt{2}}(\ket{1}+\ket{0}), \ket{x^-}=\frac{1}{\sqrt{2}}(\ket{1}-\ket{0})\end{equation}
    \begin{equation}\label{eq4.9}\ket{y^+}=\frac{1}{\sqrt{2}}(\ket{0}+i\ket{1}), \ket{y^-}=\frac{1}{\sqrt{2}}(\ket{1}+i\ket{0})\end{equation}
    \begin{equation}\label{eq4.10}\ket{z^+}=\ket{0}, \ket{z^-}=\ket{1}\end{equation}
    Los pares son bases ortonormales de $\mathbb{C}^2$
    \item Para estas 3 bases se cumple:
    \begin{equation}\label{eq4.11}(\forall a\neq b)(j,k=[+,-]) \lvert\bra{a^j}\ket{b^k}\rvert^2=\frac{1}{d}\end{equation}
    por lo que son llamadas \textcolor{red}{mútuamente incompatibles (MUB)}
    \item De acuerdo a Deustch, para 2 mediciones complementarias y coeficientes $\frac{1}{\alpha}+\frac{1}{\beta}=2$:
    \begin{equation}\label{eq4.12}H_\alpha(X)+H_\beta(Y)\geq log \frac{1}{c}=q_{MU}\end{equation}
    \item La \textcolor{red}{cota de Maasen y Uffink} (1988) es definida como \begin{equation} \label{eq4.13}q_{MU}=\frac{1}{c}, c=max_{i,j}\lvert\bra{X^i}\ket{Y^j}\rvert^2\end{equation}
    \item ¡Independiente del estado inicial!.
    \item $\alpha=\infty$y $\beta=\frac{1}{2}\textcolor{red}{\Rightarrow}$ Útil en criptografía.
\end{itemize}
\subsection{Incertezas con memoria}
\begin{itemize}
\item ¿Cómo medir incertezas \textcolor{red}{después} de una medición?
\item La min-entropía puede entenderse como una entropía condicional para un estado CQ
\begin{equation} \label{eq4.14} H_{min} (X|B) = - log p_{guess} (X|B) \end{equation}
\item Para un estado completamente cuántico, la min-entropía está vinculada al entrelazamiento de los estados en A y B. (Uhlman, 1985)
\begin{equation} \label{eq15} H_{min} (A:B) = - log(d_A F(A:B)) \end{equation}
\item Partiendo de la incerteza para cualquier medición de estado:
\begin{equation} \label{eq16} (P_{X_n}=\bra{X_n}\rho_A^y\ket{X_n} ) \sum_n H(X_n) \geq q \end{equation}
Si se considera un estado QC, para las entropías condicionales
\begin{equation} \label{eq17} (\rho_{AY}=\sum_y P_y \rho_A^y\otimes\ket{y} \bra{y}) \sum_n H(X_n:Y) \geq q \end{equation}
\item Para un estado cuántico, si se mide en 2 bases (Berta, 2010)
\begin{equation} \label{eq18} H(X|B) + H(Z|B) \geq q_{MU} +H(A|B) \end{equation}
Lo que \textcolor{red}{recomienda entrelazar los estados}
\end{itemize}
\subsection{Incertezas posición-Momentum}
\begin{itemize}
\item Cota alternativa a la de Beckner con $S(\rho)$(Hall, 1999) 
\begin{equation} \label{eq19} h(Q) + h(P) \geq log(2\pi)+S(\rho_A) \end{equation}
\item Igualdad para estado termal a temperatura infinita
\item Considerando finitud del espacio (Partovi, 1983)
\begin{equation} \label{eq20} h(Q_\delta) + h(P_\delta) \geq log(2\pi)+log(\delta_q\delta_pS_0^1(1,\frac{\delta_q\delta_p}{4} )^2) \end{equation}
Donde aparece la función de onda esférica a orden 0.
\item para un estado bipartito
 \begin{equation} \label{eq21} \rho_{AB}=\sum_k p_k \ket{\psi_k}_A \bra{\psi_k} \otimes \ket{\phi_k}_B\bra{\phi_k} \end{equation}
Se relacionan entropías condicionales y relativas
\begin{equation} \label{eq22} H(Q_\delta|B) =\sum_k D(\rho_B^{k, \delta} |\rho_B) \end{equation}
\item En (Furrel, 2014) se obtiene finalmente como cota
\begin{equation} \label{eq23} H(Q^\delta|B)+H(P^\delta|B) \geq log(2\pi)+log(\delta_q\delta_pS_0^1(1,\frac{\delta_q\delta_p}{4} )^2) \end{equation}
\end{itemize}
\section{Aplicaciones}
\subsection{Criptografía Cuántica}
\begin{itemize}
    \item Protocolo E91: A y B:$\ket{\Phi^i}$, E puede interactuar coherentemente.
    \item A y B: Eligen clásicamente base $\mathbb{X}$ o $\mathbb{Y}$ para medir.
    \item Incerteza para probabilidades luego de medir:
    \begin{equation}\label{eq24} H(Y|E\Omega)+H(Y|B\Omega) \geq q_{MU}=1 \end{equation}
    \item A partir de la desigualdad de procesamiento de datos, se obtiene una incerteza para los datos de E $\textcolor{red}{\Rightarrow}$ \textcolor{red}{Seguridad}
    \begin{equation} \label{eq25} H(Y|E\Omega) \geq H(Y|Y^\prime) \end{equation} 
\end{itemize}
\subsection{Dualidad Onda-Partícula}
\begin{itemize}
    \item Usando $\mathbb{Z}$ como operador de camino se definen \textcolor{red}{Visbilidad} y {Predictabilidad}
    \begin{equation} \label{eq26} (V=\frac{p_0^{max}-p_0^{min}}{p_0^{min}+p_0^{max}}) (P=2p_G(Z)-1) : V^2+P^2\leq 1\end{equation} 
    \item Considerando ambiente, se define \textcolor{red}{Visibilidad}:
    \begin{equation} \label{eq27} (D=2p_G(Z|E)-1): D^2+V^2\leq 1 \end{equation} 
    \item Ambas Desigualdades son equivalentes a las siguientes Desigualdades Entrópicas (Coles, 2014) ($W\in X, Y$) 
    \begin{equation} \label{eq28}H_{min}(Z)+min_W H_{max}(W)\geq 1 \end{equation} 
    \begin{equation} \label{eq29}H_{min}(Z|E)+min_W H_{max}(W)\geq 1\end{equation} 
\end{itemize}
\begin{thebibliography}{9}
\bibitem{bib4.1} Coles P., Berta M., et al, Rev. Mod. Phys., Volume 89, 2017
\bibitem{bib4.2} Jimenez. Apuntes Información Cuántica 1. (2019)
\bibitem{bib4.3} Jimenez. Apuntes Información Cuántica 2. (2019)
\end{thebibliography}
\chapter{Introducción a Confinamiento en Fotónica}
\section{Introducción}
Un área de creciente interés en Ciencia de Materiales es la \textcolor{red}{fotónica}, que consiste en el análisis y manipulación de fotones a nanoescala usando materia. Dentro de está área, se encuentran los \textcolor{red}{Cristales Fotónicos}, que de acuerdo a lo mostrado en el libro del equipo del MIT encabezado por Joannopoulos \textcolor{red}{\cite{abinitio}} permite análisis análogo al que se hace en Cristalografía. Con la consideración de la constante dieléctrica jugando el rol de los potenciales dependientes del material.

En este Seminario de Investigación se realizará una introducción a los conceptos más simples de fotónica, ilustrando con ejemplos simples de simulación usando programas que el mismo equipo del MIT ha desarrollado, para ver cómo un arreglo de dieléctricos puede confinar campo electromagnético, lo que será útil, por ejemplo, para entender mejor el efecto Purcell y la teoría de Cavity QED.

\section{Electromagnetismo en medios con dieléctrico variable}
 
En un medio dieléctrico el campo electromagnético es afectado introduciendo cambios que se modelan con las constantes dieléctricas distintas a la del vacío. Esto se comprobará numéricamente observando el efecto en un arreglo de materiales en 1 dimensión. Para hacer un modelo más realista se extenderá a 2 dimensiones.

 \begin{figure}[ht]
 \centering
   \includegraphics[width=0.7\textwidth]{epsilon.png}
   \caption{Muestra de los arreglos de material dieléctrico a evaluar en las próximas secciones. Consiste en 2 bloques de dieléctrico con $\epsilon=12$ (rojo) y uno de aire ($\epsilon=1$ azul). El arreglo 2D se repetirá como supercelda. En adelante, cuando se hable del \textit{epsilon}, se está hablando de la función constante dieléctrica en todo el arreglo.}
 \end{figure}

Si se hace un arreglo que contiene 2 placas con dieléctrico (elegido sin perder generalidad un factor $\epsilon$ de 12) en los extremos y aire (que por lo general se modela con un $\epsilon$ igual a 1), se pueden observar las bandas de dicho arreglo usando el programa \textcolor{ForestGreen}{MPB} (sigla para \textit{MIT Photonic Bands} \textcolor{red} {\cite{MPB}} ), que calcula las bandas fotónicas.

A través de dicho programa se pueden obtener las bandas y archivos de salida (de formato HDF5) que contienen los valores de densidad de energía y campos eléctrico y magnético presentes en los materiales. Estos archivos se pueden visualizar usando el programa \textcolor{ForestGreen}{h5topng} y leer con \textcolor{ForestGreen}{h5dump}, ambos parte del paquete \textcolor{ForestGreen} {h5tools}.

Todo esto genera como resultado lo visto en las imágenes siguientes. En los sistemas 1D y 2D, los campos magnético y eléctrico se desacoplan, apareciendo 2 direcciones: $TE$ (Transversal al campo eléctrico) y $TM$ (Transversal al campo magnético). Se medirán bandas tanto para la celda 1D como 2D, asi como los valores de densidad de energía, campos magnético y eléctrico (en sus respectivas partes real e imaginaria) para los puntos $(0,0)$ y $(0.5,0)$. 

Se concluye que tomar una dirección u otra no constituye mayor diferencia al evaluar campos y bandas. Aunque se eligirá en adelante la dirección $TE$ sin perder generalidad. Además que en el al usar superceldas que repiten lo pedido como patrón, también se produce doblado de bandas. Sin embargo, si se desea observar dónde hay gaps de bandas será necesario analizar para más bandas.

\pagebreak
\begin{figure}[ht]
 \centering
   \includegraphics[width=0.40\textwidth]{bandas_te.png}
   \caption{Bandas 1D (rojo) y 2D (azul) para el sistema dieléctrico mostrado en la Figura 1. en dirección $TE$. Se observa (obviando bandas falsas que se ven a nivel bidimensional) una correspondencia entre ellas, aún con el doblado de bandas que se empieza a ver desde la cuarta en adelante.}
\end{figure}
\begin{figure}[ht]
 \centering
   \includegraphics[width=0.80\textwidth]{DirTE.png}
   \caption{Comparativa entre epsilon, las densidades de energía y los campos eléctrico y magnético (en su parte real arriba e imaginaria abajo) del arreglo dieléctrico de la Figura 1 en dirección $TE$ para las banda 1 (2 primeras columnas), 2 (2 siguientes columnas) y 3 (2 últimas columnas). Evaluadas en los puntos $(0.5,0)$ (columnas impares) y $(0,0)$ (columnas pares).}
\end{figure}
\pagebreak

\begin{figure}[ht]
 \centering
   \includegraphics[width=0.40\textwidth]{bandas_tm.png}
   \caption{Bandas 1D (rojo) y 2D (azul) para el sistema dieléctrico mostrado en la Figura 1. en dirección $TM$. Se observa (obviando bandas falsas que se ven a nivel bidimensional) una correspondencia entre ellas, aún con el doblado de bandas que se empieza a ver desde la tercera en adelante.}
\end{figure}

\begin{figure}[ht]
 \centering
   \includegraphics[width=0.80\textwidth]{DirTM.png}
   \caption{Comparativa entre epsilon, las densidades de energía y los campos eléctrico y magnético (en su parte real arriba e imaginaria abajo) del arreglo dieléctrico de la Figura 1 en dirección $TM$ para las banda 1 (2 primeras columnas), 2 (2 siguientes columnas) y 3 (2 últimas columnas). Evaluadas en los puntos $(0.5,0)$ (columnas impares) y $(0,0)$ (columnas pares).}
\end{figure}
\pagebreak 
\section{Inserción de Defectos}

Cada gráfico de bandas fotónicas tiene zonas en las que, con la estructura repitiéndose indefinidamente, no se observan bandas. Estos son los llamados \textcolor{red}{gaps}. Estos son barreras que tiene el campo electromagnético para pasar por la estructura a determinados intervalos de frecuencia.  

Para el arreglo de dieléctricos en la Figura 6 se encontraron 2 gaps luego de evaluar usando \textcolor{ForestGreen}{MPB} al evaluar para varias bandas. Luego de esto, si se insertan defectos en la estructura (que pueden ser un cambio en la geometría o en algún material de la misma), es posible ver en los intervalos  donde antes había gaps, \textcolor{red}{Bandas Planas} que representan zonas en las que sí se envía un campo electromagnético, este podría quedar confinado. Estas frecuencias son mostradas en el siguiente gráfico.
\begin{figure}[ht]
 \centering
   \includegraphics[width=0.70\textwidth]{caveps.png}
   \caption{Arreglo de dieléctricos conocido como \textcolor{red}{Espejo de Bragg} o \textcolor{red}{Reflector de Bragg}. Consiste en varios bultos de aire (azul) rodeando 16 bultos de dieléctrico con $\epsilon=12$ (rojo), cuya separación central (entre el octavo y noveno bulto de dieléctrico) va variando para construir el defecto. }
\end{figure}

Para el arreglo de la Figura 6, se define como variable la distancia $d$, que es la distancia variable de la distancia entre el octavo y noveno bulto de dieléctrico. Esta distancia es variada para un intervalo entre $0$ y $0.8$. La idea es que, al producir el defecto se vea en los diagramas de bandas. 

Al observar el diagrama de la Figura 7 se puede asociar cada banda a una frecuencia determinada. Entonces, al excitar el reflector de Bragg en dicha frecuencia habrá \textcolor{red}{resonancia} y se podrán observar y analizar modos. Esto quedará más claro al avanzar hacia evaluar la propagación de las Ecuaciones de Maxwell.
\pagebreak
\begin{figure}[ht]
 \centering
   \includegraphics[width=0.50\textwidth]{graph.png}
   \caption{Gráfico realizado con \textcolor{ForestGreen}{gnuplot} en el que se observan los defectos hallados para el arreglo de dieléctricos mostrado en la Figura 6 (las bandas destacadas en verde). Entonces se encuentran (contando en el sistema 1D) en las bandas 15, 16, 31 y 32, coincidiendo aproximadamente con las frecuencias de $f=0.21$ y $f=0.46$.}
\end{figure}
\begin{figure}[ht]
 \centering
   \includegraphics[width=0.50\textwidth]{Graph0.png}
   \caption{Gráfico realizado con \textcolor{ForestGreen}{xmgrace} marcando las bandas adicionales que se encuentran en donde antes había gaps luego de insertar el defecto. Los puntos en negro corresponden a los defectos más adecuados para evaluar confinamiento (Banda 26 en adelante). Para ellos (aún extendiendo el tamaño del ancho del defecto) se observan campos y densidades de potencial. En la Figura 7 se ven los valores que se ven en este gráfico para $d=0.6 a$}
\end{figure} 

Las variables en la Figura 8 deben interpretarse como: $\frac{d}{a}$ es la distancia entre el octavo y noveno bultos de dieléctrico por la distancia entre 2 bultos de dieléctrico cuando no hay defecto (llamado en adelante $a$). Esto es conocido como \textit{Filling Factor} para un sistema como este que no tiene agujeros.
\pagebreak

Usando el programa \textcolor{ForestGreen}{Meep} (Sigla para \textit{MIT Electromagnetic Equation Propagation})\textcolor{red}{\cite{Meep}}, se excitará el dieléctrico para encontrar los defectos, esta vez usando, en lugar de un programa que calcula bandas, un programa que evalúa la propagación de las Ecuaciones de Maxwell discretizada en el debido intervalo de espacio y tiempo. Técnica conocida como \textcolor{ForestGreen}{FDTD} (sigla de \textit{Finite-Difference Time-Domain Method})\textcolor{red}{\cite{Taflove}}.

Con dicho método se encuentran 2 defectos, uno en $f=0.21$ y otro en $f=0.46$, en la Figura 9 se muestra cómo se distribuyen estos defectos en la estructura de bandas fotónicas como \textcolor{red}{bandas lineales}.



\begin{figure}[ht]
 \centering
   \includegraphics[width=0.70\textwidth]{bandef.png}
   \caption{Gráfico hecho en \textcolor{ForestGreen}{gnuplot} que contiene las bandas para el Reflector de Bragg (azul) Si se añade un defecto a la muestra, en las frecuencias donde se ven los gaps debiese verse confinamiento de campo. Las frecuencias en las que se encuentran los defectos están marcadas como líneas verdes. }
\end{figure}

\pagebreak

Usando de nuevo el programa \textcolor{ForestGreen}{MPB}, se realiza para la banda 31 el cálculo de densidad de energía y campos magnético y eléctrico para distintas distancias $d$. En las Figuras 10 y 11 se observa que el confinamiento de campo ocurre al aumentar la distancia en el intervalo entre $d=0.6$ y $d=0.9$ y luego empieza a dispersarse.
\begin{figure}[ht]
 \centering
   \includegraphics[width=0.75\textwidth]{defecto1.png}
   \caption{Valores de constante dieléctrica, densidad de energía y campos eléctrico y magnético (en su parte real e imaginaria) para el Espejo de Bragg en distintas distancias centrales $d$ en la banda 31. Se observa efecto de confinamiento que se construye al variar la distancia. }
 \end{figure}
 \begin{figure}[ht]
 \centering
   \includegraphics[width=0.75\textwidth]{defecto2.png}
   \caption{Valores de constante dieléctrica, densidad de energía y campos eléctrico y magnético (en su parte real e imaginaria) para el Espejo de Bragg en distintas distancias centrales $d $ en la banda 31. Se observa efecto de confinamiento que se destruye al variar la distancia .}
 \end{figure}
 
 Dicho esto, se terminan encontrando 2 modos: Uno entre las \textcolor{red}{bandas 15 y 16} (para $f=0.21$) y otro entre las \textcolor{red}{bandas 31 y 32} (para $f=0.46$). 
 
 Para los análisis siguientes usando \textcolor{ForestGreen}{FDTD} se trabajará con $d=0.6 a$, donde se observan más claramente modos localizados como se ven en la Figura 12, que fue hecha usando \textcolor{ForestGreen}{MPB}. En esa imagen es posible ver la dinámica de ambos defectos a medida que cambia la distancia $d$, incluso cambiando de banda.
 
\begin{figure}[ht]
 \centering
   \includegraphics[width=0.6\textwidth]{Cavidades.png}
   \caption{Proceso de confinamiento y dispersión de campo electromagnético en un Reflector de Bragg dependiente de la inserción de defectos modificando la geometría del reflector. Se encuentra en las bandas 15, 16, 31 y 32, cuyas frecuencias asociadas corresponden a los modos de campo que se pueden confinar allí. Lo evaluado posteriormente en \textcolor{ForestGreen}{Meep} son para $d=0.6a$ (séptima fila).}
\end{figure}


De los 2 defectos hallados, 1 está dentro del gap y otro en los mapas de campo 2D. Lo que lleva a preguntarse \textcolor{red}{Qué tipo de defecto son}. ¿Cuál es su simetría? ¿Qué clase de variables lo definen?

De acuerdo a lo escrito en \textcolor{red}{\cite{Tío}}, las variables que definen a una cavidad son el \textcolor{red}{Factor de Calidad} ($Q$), \textcolor{red}{Módulo de Volúmen} ($V$) e \textcolor{red}{Índice Refractivo Efectivo} ($n_{eff}^2$). Todas pueden obtenerse usando el programa \textcolor{ForestGreen}{Meep}, usando el programa de cálculo matemático \textcolor{ForestGreen}{harminv} (para $Q$) e integrando los archivos de salida usando \textcolor{ForestGreen}{Python} (para $V$ y $n_{eff}^2$).

El factor de calidad indica qué tan resonante es la cavidad siendo la razón entre la frecuencia de resonancia de ella y el intervalo de frecuencias que permite. Por lo tanto, una cavidad tendrá un mayor $Q$ si su frecuencia resonante es alta y/o permite un intervalo muy pequeño de frecuencias distintas a esta. También se puede considerar como el número de ciclos ópticos que el sistema realiza antes de decaer al orden de $e^{-\pi} \simeq 0.2\%$ de la energía inicial. Siendo $m$ la pendiente del logartimo de la evolución temporal del campo eléctrico:
\begin{equation}\label{eq5.1}Q=\frac{\omega_{resonancia}}{\Delta\omega}=\frac{\omega_{resonancia}log_{10}(e)}{2m}\end{equation}

El módulo de volumen es el valor promedio normalizado respecto al máximo posible de densidad de polarización.
\begin{equation}\label{eq5.2}V=\int\int\int d^3 \vec{r}\frac{\epsilon(\vec{r})\lvert E(\vec{r})\rvert^2}{max(\epsilon(\vec{r})\lvert E(\vec{r})\rvert^2)}\end{equation}
Y el índice efectivo (al cuadrado) en tanto es el cuociente entre los promedios de densidad de polarización y campo eléctrico. 
\begin{equation}\label{eq5.3}n_{eff}^2=\frac{\int\int\int d^3 \vec{r}\epsilon(\vec{r})\lvert E(\vec{r})\rvert^2}{\int\int\int d^3 \vec{r} \lvert E(\vec{r})\rvert^2}\end{equation}

Usando el mismo código hecho en el programa \textcolor{ForestGreen}{Meep} para encontrar los modos se les caracterizará con sus valores de $Q$, $V$ y $n_{eff}^2$.
Para ello el método consistirá en tomar, usando \textcolor{ForestGreen}{Python} (y su librería \textcolor{ForestGreen}{h5py}), los valores de campo eléctrico y constante dieléctrica para luego evaluar integrales discretizadas a lo Riemmann.
Se encontraron 2 confinamientos cuyas frecuencias se encuentran en $0.21$ y $0.46$. Para ambas se uso un parámetro de $d=0.6 a$.

\begin{figure}[ht]
 \centering
  \subfloat{
    \includegraphics[width=0.3\textwidth]{f021.png}}
  \subfloat{
    \includegraphics[width=0.3\textwidth]{f046.png}}
 \caption{Comparación de confinamiento observado con constante dieléctrica para $f=0.21$ (izquierda) y $f=0.46$ (derecha). Se combina en ambas fotos el épsilon del Espejo de Bragg con el campo eléctrico en dirección $y$ a lo largo del sistema. La gran mancha azul en ambas fotos corresponde al campo confinando que se va dispersando hacia afuera (líneas rojas).}
\end{figure}
\begin{figure}[ht]
 \centering
    \includegraphics[width=0.5\textwidth]{Bluered.png}
 \caption{Escala para todos los campos eléctricos y magnéticos vistos, conocida como \textit{bluered} presente en el paquete \textcolor{ForestGreen}{h5topng}. El azul corresponde a valores positivos y el rojo a valores negativos (cuanto más intenso el color, mayor la magnitud de la variable). El blanco representa las zonas de valor nulo.}
\end{figure}

Para el defecto encontrado en $f=0.21$ se obtiene un factor de calidad de $622.305$, un módulo de volumen de $2 (\frac{\lambda}{n})^3$ y un índice efectivo de $\sqrt{2.962}=1.721$. 

Para el defecto encontrado en $f=0.46$ se obtiene un factor de calidad de $598.251$, un módulo de volumen de $2.001 (\frac{\lambda}{n})^3$ y un índice efectivo de $\sqrt{1.835}=1.354$. 

Ambos defectos generan un campo que decae en el tiempo tal como se ve en las próximas figuras:

\begin{figure}[ht]
 \centering
    \includegraphics[width=0.7\textwidth]{field1.png}
 \caption{Decaimiento de la presencia del campo en el centro de la cavidad a lo largo del tiempo luego de excitada, ocurre en las 2 cavidades. Observar el decaímiento permite sacar valores de espectro donde hay resonancia. Evaluar el logaritmo de la evolución permite obtener el factor de calidad de acuerdo a \textcolor{blue}{\ref{eq5.1}}. Gráficos tomados del manual web del programa de pago \textcolor{ForestGreen}{Lumerical}.}
\end{figure}
 Lo realizado acá coincide y se inspira en lo realizado por Notomi, Kuramochi y Taniyama \textcolor{red}{\cite{Cavities}}, que hallaron usando un método similar cavidades con un $Q$ muy alto en estructuras 1D. 

\section{Aplicación: Efecto Purcell}
De acuerdo a la fórmula del Efecto Purcell \textcolor{red}{\cite{Purcell}}:
\begin{equation}\label{eq5.4} F_P=\frac{3}{4\pi^2}(\frac{\lambda_{free}}{n})^3\frac{Q}{V}\end{equation}
el factor de Purcell será evaluado para los 2 modos obtenidos.
\begin{equation}\label{eq5.5}(f=0.21)\frac{Q}{V}=\frac{3}{4\pi^2}\frac{622,305}{2}=23,645\end{equation}
\begin{equation}\label{eq5.6}(f=0.46)\frac{Q}{V}=\frac{3}{4\pi^2}\frac{598,251}{2,001}=22.720\end{equation}
Lo que es un valor esperable para cavidades hechas con dieléctricos.
\begin{thebibliography}{XXX0000}
  \bibitem{abinitio} Joannopoulos J.D. et. al. Photonic Crystals: Molding the Flow of Light, Princeton University Press (2008)
  \bibitem{MPB} Johnson S.G, Joannopoulos J.D., Optics Express 8, no. 3, 173-190 (2001).
  \bibitem{Meep} Oskooi A.,Roundy D. et. al. Computer Physics Communications, Vol. 181, pp. 687-702, (2010).
  \bibitem{Taflove} Taflove A., Hagness S.C., Computational Electrodynamics: The Finite-Difference Time-Domain Method, Artech: Norwood, MA, (2005)
  \bibitem{Tío} R. Coccioli, M. Boroditsky, K. Kim, Y. Rahmat-Samii, and E. Yablonovitch, “Smallest
possible electromagnetic mode volume in a dielectric cavity,” IEE Proc.-Optoelectron.
- 145, 391–397 (1998).
  \bibitem{Cavities} Notomi M. ,Kuramochi E., Taniyama1, H. Optics Express Vol. 16, Issue 15, pp. 11095-11102 (2008)
  \bibitem{Purcell} Purcell E.M.,Torrey H.C. , and Pound R.V., Phys.Rev., Vol. 69, p. 37 (1946)
\end{thebibliography}
\chapter{Ruido Cuántico}
Entendiendo como \textit{ruido} el resultado de fuerzas pequeñas y aleatorias, se puede considerar matemáticamente interesante analizar sus efectos en sistemas de variable continua. De aquí surge el \textcolor{red}{Cálculo Estocástico}.
Este texto funciona como una introducción de esta área aplicada a sistemas físicos simples. Esto con el objetivo de obtener métodos para evaluar sistemas cuánticos abiertos que aprovechan este formalismo.
\section{Ejemplos de Ruido}
\subsection{Movimiento Browniano}
Considerando como ejemplo el movimiento browniano. Este movimiento modela, por ejemplo, el comportamiento del Polen en el agua. Implica la existencia de \textcolor{red}{Fuerzas Aleatorias} que no necesariamente están en la misma dirección, con lo que se introduce el \textcolor{red}{Análisis de Langevin}.
\begin{equation}\label{eq6.1}(dx=u(t)dt): du=-\gamma u dt +\sqrt{f}dW(t)\end{equation} Donde $f$ es una constante y $dW(t)=\xi(t) dt$ inserta la variable aleatoria. El primer sumando corresponde a un elemento de decaimiento y el segundo incluye el ruido con el ingreso de aleatoreidad.
Esta ecuación diferencial tiene como solución.
\begin{equation}\label{eq6.2} u(t)=u(0)e^{-\gamma t}+\sqrt{f}\int_0^t ds e^{-\gamma(t-s)}\xi(s)\end{equation}
\subsection{Procesos Markovianos y Estadística}
Un proceso es \textcolor{red}{Markoviano} si no tiene memoria y $u(t)dt$ solo depende del $t$ anterior. Esto simplifica el análisis estadístico de \ref{eq6.1} usando como propiedad:
\begin{equation}\label{eq6.3}(dW(t)=\xi(t)dt) <\xi(t)>=0, <\xi(t)\xi(t^\prime)>=\delta(t-t^\prime) \end{equation}
Dicho análisis consiste en hallar su promedio y su varianza. En el promedio, por \ref{eq6.3}, solo influye el sumando determinístico
\begin{equation}\label{eq6.4}<u(t)>=u(0)e^{-\gamma t}\end{equation}
Mientras que la desviación estándar o varianza, se obtiene a partir de \ref{eq6.2} y la propiedad
\begin{equation}\label{eq6.5}(\Delta u(t))^2=<u(t)^2>-<u(t)>^2\end{equation}
resultando
\begin{equation}\label{eq6.6}(\Delta u(t))^2=f\int_0^t\int_0^{t^\prime}dt^\prime dt^{\prime\prime} e^{-\gamma(t-t^\prime)}e^{-\gamma(t-t^{\prime\prime})}<\xi(t^\prime)\xi(t^{\prime \prime})>\end{equation}
Y por \ref{eq6.3} el promedio de funciones aleatorias no es más que una delta, dejando solo una integral fácil de resolver.
\begin{equation}\label{eq6.7}(\Delta u(t))^2=f\int_0^t dt^\prime e^{-2\gamma(t-t^\prime)}=\frac{f}{2\gamma}(1-e^{-2\gamma t}) \end{equation}
También se puede evaluar el promedio del desplazamiento integrando lo obtenido en \ref{eq6.2}
\begin{equation}\label{eq6.8}<x(t)>=<\int_0^t ds u(s)>=\int_0^t ds <u(s)> =u(0)\int_0^t ds e^{-\gamma t}\end{equation}
Por lo que el resultado final de \ref{eq6.8} es:
\begin{equation}\label{eq6.9} (\Delta u(t))^2=\frac{u(0)}{\gamma}(1-e^{-\gamma t})\end{equation}
\subsection{Correlaciones y ruido}
Las funciones de correlación son muy ocupadas en Mecánica Cuántica. Un hecho importante a considerar es que 
\begin{equation}\label{eq6.10} <u(t_1)u(t_2)>\neq<u(t_1)><u(t_2)>\end{equation}
De manera análoga a cómo se obtiene varianzas, se puede decir que:
\begin{equation}\label{eq6.11}<u(t_1)u(t_2)>-<u(t_1)><u(t_2)>=\delta u(t_1)\delta u(t_2)\end{equation}
Lo que requiere incluir los sumandos aleatorios de las respectivas expresiones
\begin{equation}\label{eq6.12}\delta u(t_1)\delta u(t_2)=f\int_0^{t_1}\int_0^{t_2}ds_1 ds_2 e^{-\gamma(t_1-s_1)}e^{-\gamma(t_2-s_2)}<\xi(s_1)\xi(s_2)>\end{equation}
Haciendo el reemplazo de \ref{eq6.3} y considerando el tiempo menor (buscando la cota más baja para las correlaciones):
\begin{equation}\label{eq6.13}\delta u(t_1)\delta u(t_2)=f\int_{0}^{min(t_1,t_2)}ds_1 e^{-\gamma(t_1+t_2-2s_1)}=\frac{f}{2\gamma}(e^{\gamma\lvert t_1-t_2\rvert}-e^{-\gamma(t_1+t_2)})\end{equation}
Por lo que se puede concluir que \ref{eq6.10} es cierto y las correlaciones se vuelven no triviales precisamente por el ruido.
\subsection{Valores estacionarios para tiempos infinitos}
Si se calcula el desplazamiento integrando la velocidad
\begin{equation}\label{eq6.14}x(t)=\int_0^t ds u(0)e^{-\gamma s}+\sqrt{f}\int_0^t\int_0^s ds dt^\prime e^{-\gamma(s-t^\prime)} \xi(t^\prime)\end{equation}
Haciendo un cambio de variable para aíslar el término aleatorio \ref{eq6.14} se convierte en 
\begin{equation}\label{eq6.15}x(t)=\int_0^t ds u(0)e^{-\gamma s}+\sqrt{f}\int_0^t \int_{t^\prime}^t dt^\prime \xi(t^\prime) ds e^{-\gamma (s-t^\prime)}\end{equation}
Resolviendo la integral de s en \ref{eq6.15}\begin{equation}\label{eq6.16}x(t)=\int_0^t ds u(0)e^{-\gamma s}+\frac{\sqrt{f}}{\gamma}\int_0^t d t^\prime (1-e^{-\gamma(t-t^\prime)})\xi(t^\prime)\end{equation} Si se calcula la varianza para esta definición de x cuando t tiende a infinito se obtiene
\begin{equation}\label{eq6.17} (\Delta x(t))^2 \textcolor{red}{\Rightarrow} \frac{f}{\gamma^2}\end{equation}
\section{Ruido Clásico}
\subsection{Ruidos blancos y de color}
El ruido, como propiedad, tiene asociado un \textcolor{red}{espectro} de frecuencias, asociado con una función temporal mediante una Transformada de Fourier:
\begin{equation}\label{eq6.18}\tilde{x}(\omega+i\gamma, \tau)=\int_{-\infty}^{\infty} dt e^{(i\omega+\gamma)t}S_Z (\omega)\end{equation}
La función $S_Z(\omega)$ observada aquí se comporta como un límite de valores de expectación:
\begin{equation}\label{eq6.19}S(\omega)=\lim_{\gamma\textcolor{red}{\Rightarrow} 0}\frac{\gamma}{\pi}<\lvert\tilde{x}(\omega +i\gamma,\tau)\rvert^2>\end{equation} Reemplazando \ref{eq6.18} en  \ref{eq6.19} se obtiene
\begin{equation}\label{eq6.20}\tilde{x}(\omega+i\gamma, \tau)=\lim_{\gamma\textcolor{red}{\Rightarrow} 0}\frac{\gamma}{\pi}\int_{-\infty}^{\infty}\int_{-\infty}^0 dt dt^\prime e^{iw(t-t^\prime)}e^{\gamma(t+t^\prime}<X(t+\tau)X(t^\prime+\tau)>\end{equation} El término de 
\begin{equation}\label{eq6.21}\tilde{x}(\omega+i\gamma, \tau)=\lim_{\gamma\textcolor{red}{\Rightarrow} 0}\frac{\gamma}{\pi}\int_{-\infty}^{\infty}\int_{-\infty}^0 dt dt^\prime e^{iw(t-t^\prime)}e^{\gamma(t+t^\prime}<X(t+\tau)X(t^\prime+\tau)>\end{equation}
Luego con el cambio de variable $t_1=t-t^\prime$, $t_2=t+t^\prime$ y usando que el producto dentro del valor de expectación equivale a la suma del promedio y desviación estándar de la función $X$
\begin{equation}\label{eq6.22}=\lim_{\gamma\textcolor{red}{\Rightarrow} 0} \frac{\gamma}{\pi}\int_{-\infty}^\infty \int_{-\infty}^{-\lvert t_1\rvert}dt_1 dt_2 e^{i\omega t_1+\gamma t_2}\{<X>_S^2+g_S(t_1)\}\end{equation}
Integrando en $t_2$ lo anterior es
\begin{equation}\label{eq6.23}=\lim_{\gamma\textcolor{red}{\Rightarrow} 0}\frac{1}{\pi}\int_{-\infty}^\infty dt_1 e^{i\omega t_1 }e^{-i\gamma\lvert t_1\rvert}\{<X>_S^2+g_S(t_1)\}\end{equation}
La integral dependiente solo del promedio por cálculo equivale a una delta, obteniéndose finalmente
\begin{equation}\label{eq6.24}=\delta(\omega)<X>_S^2+\frac{1}{2\pi}\int_{-\infty}^{\infty} dt_1 e^{i\omega t_1}g_s(t_1)\end{equation}
Una forma de ruido simple es una parte en que $<x>_S^2$ es cero (lo que anula el primer sumando). Entonces, el valor de espectro finalmente vale:
\begin{equation}\label{eq6.25}\tilde{x}(\omega+i\gamma,\tau)=\frac{1}{2\pi}\int_0^\infty dt^\prime e^{-i\omega(t-t^\prime)}g_S(t-t^\prime)=S(\omega)\delta(\omega-\omega^\prime)\end{equation}
Lo que equivale a la descomposición espectral de una función $S(\omega)$. Si $S(\omega)$ es constante, se llama \textcolor{red}{Ruido Blanco}. Si no, se le llama \textcolor{red}{Ruido de Color}. Estos modelos de ruido no son realistas, pero representan bien lo correspondiente al ruido cuántico.
\subsection{Ecuación de Langevin Clásica}
La ecuación de Langevin es fundamental para el cálculo estocástico. A nivel clásico (análogamente a lo mostrado en los ejemplos anteriores) corresponde a una ecuación diferencial para la función $x$ dependiente del tiempo.
\begin{equation}\label{eq6.26}\dot{x}(t)=a(x,t)+b(x,t)\xi(t)\end{equation}
Multiplicando \ref{eq6.26} por $dt$ se obtiene finalmente:
\begin{equation}\label{eq6.27}dx(t)=a(x,t)dt+b(x,t)\xi(t)dt=a(x,t)dt+b(x,t)dW(t)\end{equation}
Donde $\xi(t)dt=dW(t)$. Dicha diferencia es fácil de discretizar y programar. Para un ruido simple (blanco o de color) se cumplen las \textcolor{red}{Reglas de Ito}:
\begin{equation}\label{eq6.28}(\forall N\geq 2)dW^2(t)=dt, dW(t)dt=dt^N=dW^{N+1}(t)=0 \end{equation}
Integrando \ref{eq6.26} se obtiene para la función $x(t)$
\begin{equation}\label{eq6.29}x(t)=x(t_0)+\int_{t_0}^t a(x(t^\prime),t^\prime) dt^\prime +(I)\int_{t_0}^t b(x(t^\prime),t^\prime)dW(t^\prime)\end{equation}
Se puede tomar $f(x)$ como una función cualquiera de la variable de $x(t)$ y considerar su expansión de Taylor:
\begin{equation}\label{eq6.30}df(x)=f^\prime(x)dx+\frac{1}{2}f^{\prime\prime}(x)(dx)^2\end{equation}
Aplicando \ref{eq6.27} en \ref{eq6.30}, las reglas de Ito en \ref{eq6.28} sugieren finalmente que, si se hace una aproximación lineal se agregan elementos cuadráticos de la expansión de $x(t)$.
\begin{equation}\begin{aligned}\label{eq6.31}df(x(t))=\{f^\prime(x(t))a(x(t),t)+\frac{1}{2}f^{\prime\prime}(x(t))b(x(t),t)\}dt+ \\f^\prime(x(t))b^\prime(x(t),t)dW(t)\end{aligned}\end{equation}
Para cualquier función $f(x)$ su expansión de Taylor será corregida de esta forma si se agrega ruido en $x$.
\subsection{Ecuación de Fokker-Planck}
A partir de la propiedad condicional de un suceso una vez dada una probabilidad inicial:
\begin{equation}\label{eq6.32}p(x,t)=p(x,t|x_0,t_0)p(x_0,t_0)\end{equation}
Se puede encontrar el valor de expectación de una función de $x$ y $t$, así como su derivada
\begin{equation}\label{eq6.33}<f(x)>=\int dx f(x)(x,t|x_0,t_0) \textcolor{red}{\Rightarrow} \dot{<f(x)>}\int dx \frac{\partial f(x)}{\partial x} p(x,t|x_0,t_0)\end{equation}
Ocupando la regla de Ito la derivada termina valiendo:
\begin{equation}\label{eq6.34}\dot{<f(x)>}=\int dx (f^\prime(x)a(x,t)) +\frac{1}{2}(f^{\prime\prime}(x)b(x,t)))p(x,t|x_0,t_0)\end{equation}
Mediante integración por partes, se obtiene finalmente la \textcolor{red}{Ecuación Fokker-Planck}
\begin{equation}\label{eq6.35}\dot{<f(x)>}=\int dx \{-\frac{\partial}{\partial x}(a(x,t)p(x,t|x_0,t_0)+\frac{1}{2}\frac{\partial^2}{\partial x^2}(b(x,t)^2p(x,t|x_0,t_0))\} f(x)\end{equation}
Comparando con lo visto en \ref{eq6.26} y \ref{eq6.27}, dicha ecuación es obtenible de Langevin y genera las mismas soluciones, a pesar de ser más compleja (al ser no lineal).
\section{Ruido en Sistemas Cuánticos}
\subsection{El ruido es necesario para mantener la cuanticidad}
Conocido es el principio de Incerteza de Heisenberg (acá $\hslash=1$)
\begin{equation}\label{eq6.36}[X,P]=i\end{equation}  pero si se consideran operadores con decaimiento temporal se obtiene algo no físico: \textcolor{red}{Decaímiento de la incerteza}
\begin{equation}\label{eq6.37}(Xe^{-\gamma t}, Pe^{-\gamma t}) [Xe^{-\gamma t}, Pe^{-\gamma t}]=e^{-2\gamma t}i\end{equation}
¿Cómo se evita esto? \textcolor{red}{Agregando Ruido}. ¿Cómo se agrega a nivel cuántico? Considerando un cuadro de Heisenberg, o al menos uno en donde los operadores tengan dependencia temporal. Considerando operadores de subida y bajada típicos de Mecánica Cuántica:
 \begin{equation} \label{eq6.38}\dot{a}(t)=-ka+x\xi(t)\end{equation}
donde $x$ es la variable a definir tal que la incerteza de Heisenberg siempre se cumpla. Para ello se comienza integrando \ref{eq6.38} 
\begin{equation}\label{eq6.39} a(t)=ae^{-kt} +x \int_0^t ds e^{-k(t-s)}\xi(s)\end{equation}
Originalmente $[a(0),a^\dag(0)]=1$. 
\subsection{Ecuación de Langevin Cuántica}
\subsection{Otra derivación de ecuación maestra}
Reemplazando en lo anterior para $a=\rho$ queda la ecuación:
\begin{equation}\end{equation}

\chapter{Óptica Cuántica}
\section{Teoría de Interacción Átomo-Campo de Einstein}
\subsection{Probabilidad de distribución Bose-Einstein}
Si \begin{equation}{ <n>=\frac{1}{e^{\beta h\omega}-1}\textcolor{red}{\Rightarrow} e^{\beta\hslash\omega}=\frac{1}{<n>}+1 \textcolor{red}{\Rightarrow} e^{-\beta\hslash\omega}=\frac{1}{\frac{<n>+1}{<n>}}=\frac{<n>}{<n>+1}}\end{equation}
Se puede escribir la probabilidad como:
\begin{equation}{ P_n=\frac{e^{-\beta\hslash\omega}}{\sum_n e^{-\beta\hslash\omega}}=e^{-\beta(n+1)\hslash\omega}[e^{\beta\hslash\omega}+1]=\frac{1}{<n>}(\frac{<n>}{<n>+1})^{n+1}}\end{equation}
Con lo que se obtiene lo pedido
\begin{equation}{ P_n=\frac{<n>^n}{(<n>+1)^{n+1}}}\end{equation}
\subsection{Variancia en Distribución de Poisson}Se comienza con la definición de fluctuación
\begin{equation}{ (\Delta n)^2=\sum_n P_n (n-<n>)^2= \sum_n P_n n^2+\sum_n P_n <n>^2 -2\sum_n P_n<n> n }\end{equation}
Recordando que $<n>$ no depende de n y está definido por
\begin{equation}{ <n> =\sum_n P_n \textcolor{red}{\Rightarrow} (\Delta n)^2=<n^2>+<n>^2-2*<n>^2=<n^2>-<n>^2}\end{equation}
Lo anterior es válido para cualquier distribución, ahora bien, considerando que es una distribución de Bose Einstein, vamos a tratar de demostrar:
\begin{equation}{ <n>=\frac{1}{e^{\beta\hslash\omega}-1}\textcolor{red}{\Rightarrow} <n^2>=\sum_n \frac{n^2 <n>^n}{(<n>+1)^{n+1}}=\sum_n\frac{n^2 e^{-n\beta\hslash\omega}}{\sum_n e^{-n\beta\hslash\omega}}}\end{equation}
Considerando $\beta\hslash\omega=x$
\begin{equation}{ \sum_n n^2 e^{-nx}= -\frac{d}{dx} \sum_n n e^{-nx}= \frac{d^2}{dx^2} \sum_n e^{-nx}=\frac{d^2}{dx^2}(\frac{1}{1-e^{x}})}\end{equation} \begin{equation}{=-\frac{d}{dx}\frac{-e^{x}}{(1-e^{x})^2}=\frac{e^x(1-e^x)^2+e^{2x}}{(1-e^x)^4}=\frac{e^x}{(1-e^x)^2}+(\frac{e^x}{(1-e^x)^2})^2}\end{equation}
Con esto se obtiene \begin{equation}{ <n^2>=<n> +2<n>^2}\end{equation}
con lo que se obtiene lo pedido. \begin{equation}{ (\Delta n)^2=<n>(<n>+1)=<n>+<n>^2}\end{equation} 
\subsection{Derivación de número de fotones} Para la ecuacion \begin{equation}{ \frac{dN_b}{dt}=-\frac{dN_a}{dt}=A_{ab}N_a+B_{ab}U(\omega)N_a-N_bB_{ba}U(\omega)}\end{equation} Si U es Constante y $g_a=g_b=1$
\begin{equation}{ \frac{dN_b}{dt}=-\frac{dN_a}{dt}=AN_a+BUN_a-N_bBU=(N-N_b)(A+BU)-N_bBU}\end{equation}
\begin{equation}{ \textcolor{red}{\Rightarrow} \frac{dN_b}{dt}=N_b(-A-2BU)+N(A+BU)}\end{equation}
Se ve una ecuación diferencial ordinaria que se puede resolver así:
\begin{equation}{ y^\prime(x)=a*y(x)+b \textcolor{red}{\Rightarrow} y(x)=y_oe^{ax}-\frac{b}{a} \textcolor{red}{\Rightarrow}} \end{equation}
\begin{equation}{ N_b(t)=Ce^{-(A+BU)t}+\frac{N(A+BU)}{A+2BU}}\end{equation}
¿Cómo se obtiene el valor de C? Calculando para $t=0$
\begin{equation}{ N_b^0= C+\frac{N(A+BU)}{A+2BU}\textcolor{red}{\Rightarrow} C=N_b^0-\frac{N(A+BU)}{A+2BU}}\end{equation}
Se termina obteniendo
\begin{equation}{ N_b(t)=(N_b^0-\frac{N(A+BU)}{A+2BU})e^{-(A+2BU)t}+\frac{N(A+BU)}{A+2BU}}\end{equation}
\section{Aproximación Semiclásica a la Teoría Átomo Campo}
\subsection{Ecuación para el elemento de matriz densidad} La solución para $\omega_{ba}=\omega$ y ${\rho_{ba}(0)=\rho_{aa}(0)=0}$ dada es \begin{equation}{\rho_{aa}=\frac{\frac{\lvert\nu\rvert^2}{2}}{\frac{\lvert\gamma\rvert^2}{2}+\lvert\nu\rvert^2}[1-(cos\lambda t+\frac{3\gamma}{4\lambda}sin\lambda t)e^{\frac{-3\gamma t}{4}}]} \end{equation}
Si se toma la ecuación de Bloch óptica para ${\rho_{aa}}$
\begin{equation}{\frac{\rho_{aa}}{dt}=-\frac{i\nu^*}{2}e^{i(\omega_{ba}-\omega)t}+\frac{i\nu}{2}e^{-i(\omega_{ba}-\omega)t}} \end{equation}
\section{Cuantización del Campo Electromagnético}
\subsection{Hamiltoniano en operadores escalera}Empezando con la definición de Energía
    \begin{equation}{ \mathcal{H}=\frac{1}{2}\int(\epsilon_0 E^2+\mu_0H^2) dv=}\end{equation}
    \begin{equation}{\frac{1}{2}\int \sum_m\epsilon_0(i)^2( \sqrt{\frac{\hslash\omega_m}{2\epsilon_0 v}}\lbrace a_me^{i({k}_m\cdot{r}-\omega_r t)}-a_m^\dag e^{-i({k}_m\cdot{r}-\omega_r t)}\rbrace)^2+} \end{equation}
    \begin{equation}{\mu_0(\frac{-i}{c\mu_0})^2(\sqrt{\frac{\hslash\omega_m}{2\epsilon_0 v}}{e}_m \times \hat{{k
    }}_m\lbrace a_me^{i({k}_m\cdot{r}-\omega_r t)}-a_m^\dag e^{-i({k}_m\cdot{r}-\omega_r t)}\rbrace)^2 dv =} \end{equation}
    \begin{equation}{-\frac{1}{2}\int \sum_m\epsilon_0^2\frac{\hslash \omega_m}{2\epsilon_0 v}( \lbrace a_me^{i({k}_m\cdot{r}-\omega_m t)}-a_m^\dag e^{-i({k}_m\cdot{r}-\omega_m t)}\rbrace)^2} \end{equation} \begin{equation}{+(\frac{1}{c^2\mu_0})^2\frac{\hslash\omega_m}{2\epsilon_0 v}({e}_m \times \hat{{k
    }}_m\lbrace a_me^{i({k}_m\cdot{r}-\omega_m t)}-a_m^\dag e^{-i({k}_m\cdot{r}-\omega_m t)}\rbrace)^2 dv} \end{equation}
    Considerando que los vectores polarización y propagación son son ortogonales entre sí.
    \begin{equation}{ \lvert{e}_m \times \hat{{k}}_m\rvert=\lvert{e}_m\rvert\lvert \hat{{k}}_m\rvert=1} \end{equation}
    Además de la ortogonalidad de las funciones ${u_m}$
    \begin{equation}{{u}_m({r})=\frac{{e}_m e^{i({k}_m\cdot{r})}}{\sqrt{v}} :\int{u}_m^{*}({r}){u}_n({r})dv=\delta_{nm}} \end{equation}
    y que ${c^{-2}=\mu_0\epsilon_0}$ Se simplifica lo anterior quedando
    \begin{equation}{ \mathcal{H}=-\int \sum_m\frac{\hslash \omega_m}{2}( \lbrace a_m{u}_m({r})e^{i(-\omega_m t)}-a_m^\dag{u}_m^*({r})e^{-i(-\omega_m t)}\rbrace)^2}\end{equation} \begin{equation}{=\sum_m \frac{\hslash\omega}{2} 2a_ma_m^\dag e^{(i-i)(-\omega_m t)}+\sum_m \frac{\hslash\omega}{2} 2a_m^\dag a_m e^{(i-i)(-\omega_m t)}}\end{equation}
    Por lo tanto se obtiene
    \begin{equation}{ \mathcal{H}=\sum_m \hslash\omega (a_ma_m^\dag+a_m^\dag a_m)}\end{equation}
    \subsection{Conmutador entre elementos de Campo Electromagnético}Usando las definiciones:
    \begin{equation}{ [A_i(\vec{r}),E_j(\vec{r}^\prime)]=-\frac{i\hslash}{2v\epsilon_0}\sum_{l,\sigma}({e}_{l\sigma})_i({e}_{l\sigma})_j e^{i{k}_l({r}-{r}^\prime)}+c.c.} \end{equation}
    Siendo ${{e}_{l1}}$ ${{e}_{l2}}$ y ${\hat{{k}}_l}$ vectores ortogonales entre sí, pudiéndose obtener:
    \begin{equation}{ \ket{{e}_{l1}}\bra{{e}_{l1}}+\ket{{e}_{l2}}\bra{{e}_{l2}}+\ket{\hat{{k}}_l}\bra{\hat{{k}}_l}=1} \end{equation}
    Al tomar los elementos ij de la suma:
    \begin{equation}{ \sum_\sigma ({e}_{l\sigma})_i({e}_{l\sigma})_j=\delta_{ij}-\hat{{e}_{l\sigma}}_i\hat{{e}_{l\sigma}}_j} \end{equation} 
    Sustituyendo en la suma anterior
    \begin{equation}{ [A_i({r}), E_j({r})]=\frac{-i\hslash}{2v_0\epsilon_0}\sum_l [\delta_{ij}-\hat{{e}_{l\sigma}}_i\hat{{e}_{l\sigma}}_j]e^{i{k}_l({r}-{r}^\prime)}} \end{equation}
    Tal como se recomendó en la clase, todos estos conmutadores se pueden trabajar en la imagen de Schrodinger (independiente del tiempo) y serán también ciertos en la imagen de Heisenberg (dependiente del tiempo).
    \subsection{Estado número como excitaciones del vacío}Si se puede escribir 
    \begin{equation}{ a^\dag\ket{n-1}=\sqrt{n}\sqrt{n}\ket{n}} \end{equation}Se puede operar recursivamente \begin{equation}{ (a^\dag)^2\ket{n-2}=\sqrt{n(n-1)}\ket{n}\textcolor{red}{\Rightarrow} (a^\dag)^n\ket{0}=\sqrt{n(n-1)...1}\ket{n}}\end{equation} Y tomando que ${n!=n(n-1)...}1 $, se obtiene finalmente \begin{equation}{ \ket{n}=\frac{(a^\dag)^n}{\sqrt{n!}}\ket{0}}\end{equation}
    \subsection{Conmutador de potencias de operadores escalera} La primera propiedad de conmutadores se demuestra desarrollando
    \begin{equation}{ [a,a^{\dag n}]=[a,a^{\dag(n-1)}]a^\dag+a[a^\dag,a^{\dag(n-1)}]=[a,a^{\dag(n-1)}]a^\dag} \end{equation}
    Esto último se obtiene a p sq2licando la propiedad de conmutadores \begin{equation}[A,BC]=[A,C]B+A[B,C]\end{equation} y considerando que, al ${[a,a^\dag]=1}$, tanto ${a}$ como ${a^\dag}$ conmutan con su conmutador. Aplicando esta propiedad recursivamente
    \begin{equation}{ [a,a^{\dag n}]=[a,a^{\dag (n-1)}]a^\dag=[a,a^{\dag (n-2)}]a^{\dag 2}=...=[a,a^{\dag}]a^{n-1}=na^{\dag n-1}} \end{equation}
    De manera análoga también se obtiene que 
    \begin{equation}{[a^n, a^\dag]=na^{n-1}} \end{equation}
    \subsection{Operación con serie de potencias de operadores escalera} Aplicando lo del punto anterior, se resuelve para un f que es una serie de potencias y puede ser una aproximación de cualquier función derivable. 
    \begin{equation}{ f(a,a^\dag)=\sum_{n,m} c_n a^n c_m a^{\dag m}\textcolor{red}{\Rightarrow} [a, f(a, a^\dag)]=\sum_{n,m} c_n a^n [a,c_m a^{\dag m}]=} \end{equation} \begin{equation}{\sum_{n,m} c_n a^n m c_m a^{\dag (m-1)}=\frac{\partial f(a,a^{\dag})}{\partial a^\dag}} \end{equation} Aprovechando la propiedad BCH, se obtiene también
    \begin{equation}{ e^{-\alpha a^\dag a}f(a, a^\dag)e^{\alpha a^\dag a}=f(ae^\alpha,a^\dag e^-\alpha)} \end{equation}
    \subsection{Más conmutadores entre elementos de campo electromagnético} Usando las definiciones 
    \begin{equation}{A(r,t)=\sum_{m}\sqrt{\frac{\hslash}{2\omega_m\epsilon_0 v}}{e}_m(a_me^{i({k}_m\cdot{r}-\omega_m t)}+cc)} \end{equation}
    \begin{equation}{E(r,t)=i\sum_{m}\sqrt{\frac{\hslash\omega_m}{2\epsilon_0 v}}{e}_m(a_me^{i({k}_m\cdot{r}-\omega_m t)}+cc)} \end{equation}
    \begin{equation}{H(r,t)=\frac{-i}{c\mu_0}\sum_{m}\sqrt{\frac{\hslash\omega_m}{2\epsilon_0 v}}{e}_m(a_me^{i({k}_m\cdot{r}-\omega_m t)}+cc)} \end{equation}
    , y los conmutadores \begin{equation}[a_m,a_n^\dag]=\delta_{mn} \end{equation}, y \begin{equation}[a_m,a_n]=[a_m^\dag, a_n^\dag]=0 \end{equation} :
    \begin{equation}{ [A_i(\vec{r}),E_j(\vec{r}^\prime)]=-\frac{i\hslash}{2v\epsilon_0}\sum_{l,\sigma}({e}_{l\sigma})_i({e}_{l\sigma})_j e^{i{k}_l({r}-{r}^\prime)}+c.c.} \end{equation}
    Siendo ${{e}_{l1}}$, ${{e}_{l2}}$ y ${\hat{{k}}_l}$  vectores ortogonales entre sí, pudiéndose obtener:
    \begin{equation}{ \ket{{e}_{l1}}\bra{{e}_{l1}}+\ket{{e}_{l2}}\bra{{e}_{l2}}+\ket{\hat{{k}}_l}\bra{\hat{{k}}_l}=1} \end{equation}
    Al tomar los elementos ij de la suma:
    \begin{equation}{ \sum_\sigma ({e}_{l\sigma})_i({e}_{l\sigma})_j=\delta_{ij}-\hat{{e}_{l\sigma}}_i\hat{{e}_{l\sigma}}_j} \end{equation} 
    Sustituyendo en la suma anterior
    \begin{equation}{ [A_i({r}), E_j({r}^\prime)]=\frac{-i\hslash}{v_0\epsilon_0}\sum_l [\delta_{ij}-\hat{{k}_{i}}_i\hat{{k}_{j}}_j]e^{i{k}_l({r}-{r}^\prime)}}\end{equation} Sumando sobre todos los ${l}$, tanto positivos como negativos. 
    Si L se vuelve infinito, la suma se puede convertir en una integral: \begin{equation}{\frac{1}{v}\sum_l [\delta_{ij}-\hat{{k}_{i}}_i\hat{{k}_{j}}_j]e^{i{k}_l({r}-{r}^\prime)}\textcolor{red}{\Rightarrow} (\frac{1}{2\pi})^3 \int \int \int d{k} [\delta_{ij}-\hat{{k}_{i}}_i\hat{{k}_{j}}_j]e^{i{k}_l({r}-{r}^\prime)}}\end{equation} Reemplazando en lo anterior (nombrando a la integral $\delta_{ij}^T({r}-{r}^\prime)$), finalmente se obtiene  
    \begin{equation}{  [A_i(\vec{r}),E_j(\vec{r}^\prime)]=\frac{-i\hslash}{\epsilon}\delta_{ij}^T({r}-{r}^\prime)} \end{equation}
    De manera análoga a lo anterior, se obtiene
    \begin{itemize}
        \item \begin{equation}{ [A(\vec{r}),A(\vec{r}^\prime)]=\frac{\hslash}{2\omega_m\epsilon_0 v}\sum_{l,\sigma}({e}_{l\sigma})_i({e}_{l\sigma})_j e^{i{k}_l({r}-{r}^\prime)}+c.c.} \end{equation} \begin{equation}{=\frac{\hslash}{2\omega_m\epsilon_0 v}\sum_{l}[\delta_{ij}-\hat{{k}_{i}}_i\hat{{k}_{j}}_j]e^{i{k}_l({r}-{r}^\prime)}} \end{equation} Con lo que se demuestra que\begin{equation}{ [A(\vec{r}),A(\vec{r}^\prime)]=0} \end{equation}
        \item \begin{equation}{ [E(\vec{r}),E(\vec{r}^\prime)]=-\frac{\hslash\omega_m}{2\epsilon_0 v}\sum_{l,\sigma}({e}_{l\sigma})_i({e}_{l\sigma})_j e^{i{k}_l({r}-{r}^\prime)}+c.c.} \end{equation} \begin{equation} {-\frac{\hslash\omega_m}{2\epsilon_0 v}\sum_{l}[\delta_{ij}-\hat{{k}_{i}}_i\hat{{k}_{j}}_j] e^{i{k}_l({r}-{r}^\prime)}} \end{equation}Con lo que se demuestra que \begin{equation}{ [E(\vec{r}),E(\vec{r}^\prime)]=0} \end{equation}
        \item  \begin{equation}{ [B(\vec{r}),B(\vec{r}^\prime)]=-\frac{\hslash\omega_m}{c^2\mu_0^2 2\epsilon_0v}\sum_{l,\sigma}({e}_{l\sigma})_i({e}_{l\sigma})_j e^{i{k}_l({r}-{r}^\prime)}+c.c.} \end{equation} \begin{equation}{=-\frac{\hslash\omega_m}{c^2\mu_0^2 2\epsilon_0v}\sum_{l}[\delta_{ij}-\hat{{k}_{i}}_i\hat{{k}_{j}}_j] e^{i{k}_l({r}-{r}^\prime)}} \end{equation}Con lo que se demuestra que \begin{equation}{ [B(\vec{r}),B(\vec{r}^\prime)]=0} \end{equation}
    \end{itemize}
\section{Estados del Campo Electromagnético}
\subsection{No existen autoestados del operador creación}Si, de manera análoga a cómo se construyeron los estados del operador de destrucción, se intentan construir los del operador de destrucción
    \begin{equation}{ a^\dag\ket{\beta}=\sum_{n=0}^\infty c_n\sqrt{n+1}\ket{n+1}=\beta\sum_{n=0}^\infty c_n\ket{n}=\beta c_0\ket{0}+\beta \sum_{n=1}^\infty c_n\ket{N} }\end{equation}
    De acá se obtiene que
    \begin{equation}{ c_0=0, c_n\sqrt{n+1}=\beta c_{n+1}} \end{equation}
    Al construir los ${c_n}$ todos dan cero, lo que por contradicción, demuestra que no existen autoestados del operador creación.
    \subsection{Producto de proyector coherente y operador destrucción} Descomponiendo la operación.
    \begin{equation}{a^\dag\ket{\alpha}\bra{\alpha}=a^\dag D(\alpha)\ket{0}\bra{0} D(\alpha)^\dag=D(\alpha) D(\alpha)^\dag a^\dag D(\alpha)\ket{0}\bra{0} D(\alpha)^\dag} \end{equation} \begin{equation}{=D(\alpha)(a^\dag+\alpha^*)\ket{0}\bra{0}D(\alpha)^\dag} \end{equation} Lo útimo se obtiene usando propiedad de \begin{equation}D(\alpha)^\dag a^\dag D(\alpha)=a^\dag+\alpha^*\end{equation} (que se demuestra a partir de expandir el operador desplazamiento). Separando los 2 sumandos (uno incluye agrega un número y el otro un operador): 
    \begin{equation}{ D(\alpha)a^\dag\ket{0}\bra{\alpha}+\alpha^*\ket{\alpha}\bra{\alpha}} \end{equation} Considerando la definición de operador desplazamiento
    \begin{equation}{ D(\alpha)=e^{\alpha a^\dag-\alpha^* a}\textcolor{red}{\Rightarrow} D(\alpha)a^{\dag}=(a D(\alpha))^\dag=(\frac{\partial D(\alpha)}{\partial \alpha^*})^\dag=\frac{\partial D(\alpha)}{d\alpha}} \end{equation}
    Entonces se puede decir que \begin{equation}{ \frac{\partial D(\alpha)}{\partial \alpha}=D(\alpha)a^\dag} \end{equation}
    Y se obtiene
    \begin{equation}{a^\dag\ket{\alpha}\bra{\alpha}=(\alpha^*+\frac{\partial}{\partial\alpha})\ket{\alpha}\bra{\alpha}} \end{equation}
    Y conjugando la expresión se obtiene:
    \begin{equation}{ \ket{\alpha}\bra{\alpha} a=(\alpha+\frac{\partial}{\partial\alpha^*})\ket{\alpha}\bra{\alpha}} \end{equation}
    \subsection{Evolución de estado inicialmente coherente en el tiempo} Un estado inicialmente coherente es un autoestado del operador destrucción en ${t=0}$
    \begin{equation}{ \ket{\phi,0}=\ket{\alpha} \textcolor{red}{\Rightarrow} a(t=0)\ket{\alpha}=\alpha\ket{\alpha} }\end{equation} A los operadores creación y destrucción se les puede agregar evolución temporal
    \begin{equation}{ a_m(t)=a_m(t=0)e^{-i\omega_m t}, a_m^\dag(t)=a_m^\dag(t=0)e^{i\omega_m t}} \end{equation} Tomando el operador Desplazamiento de ${\alpha}$ para tiempo cero, se le puede agregar evolución temporal como se acostumbre en los operadores cuánticos (esta vez considerando los operadores de subida y bajada para un sistema de n osciladores armónicos acoplados).
    \begin{equation}{ D(\alpha(t))=\sum_ne^{in\omega t}D(\alpha)\sum_n e^{-in\omega t}= (e^{i\omega t})^n\sum_n\frac{(\alpha a^\dag-\alpha^* a)^n}{n!}(e^{-i\omega t})^n=} \end{equation}
    \begin{equation}{\sum_n\frac{(e^{i\omega t}(\alpha a^\dag-\alpha^* a)e^{-i\omega t})^n}{n!}=\sum_n\frac{((\alpha e^{-i\omega t})(a^\dag e^{i\omega t})-(\alpha^*e^{i\omega t})(ae^{-i\omega t}))^n}{n!}} \end{equation} Desplazando convenientemente las exponenciales temporales, ya que conmutarían con el resto de operadores al ser independientes del tiempo, se obtiene que el mismo desplazamiento para ${t=0}$ con un valor ${\alpha}$ sirve para desplazar, ya ingresada la evolución temporal en los operadores escalera de manera estándar con un valor ${\alpha e^{-i\omega t}}$, quedando
    :
    \begin{equation}{ \ket{\phi,t}=\ket{\alpha e^{-i\omega t}} }\end{equation}
    \subsection{Combinación de operadores desplazamiento y compresión}
       \begin{itemize} \item Empezando con ${\beta=\alpha cosh r+\alpha^* sinh r}$
       \begin{equation}{S(re^{-i\theta})D(\alpha)S(re^{i\theta})=e^{\frac{re^{i\theta}a^2-re^{-i\theta}a^{\dag 2}}{2}}e^{\alpha a^{\dag}-\alpha * a}e^{\frac{re^{-i\theta}a^2-re^{i\theta}a^{\dag 2}}{2}} }\end{equation} \begin{equation}{ e^{-(re^{-i\theta}a^\dag-\alpha+re^{i\theta}a^\dag)a^\dag+(re^{i\theta}a-\alpha^*+re^{-i\theta}a)a}=D(\alpha cosh r-e^{i\theta}a^\dag sinh r)=D(\beta)} \end{equation}
       Con lo que termina dando
       \begin{equation}{ \textcolor{red}{\Rightarrow} S(\xi)D(\beta)=D(\alpha)S(\xi)} \end{equation}
       \item se empieza calculando
       \begin{equation}{ S_2^\dag(\xi)aS_2(\xi)=e^{\xi^*a^2-\xi a^{\dag 2}}ae^{\xi^*ab-\xi a^\dag b^\dag}} \end{equation}
       \begin{equation}{} \end{equation}
       Al final tiene que dar \begin{equation}{=acosh r - e^{i\theta}b^\dag sinh R} \end{equation}
       \end{itemize}
       \subsection{Valor esperado de Operador Desplazamiento} Hay que calcular el valor esperado para el ${D(\alpha)}$ de Glauber, y para eso puede ser útil calcular los valores de expectación para los operadores escalera
       \begin{equation}{ <a>=\alpha, <a^{\dag}>=\alpha^*} \end{equation}
   \begin{equation}{\textcolor{red}{\Rightarrow} <D(\alpha)>=<e^{aa^{\dag}-\alpha^*a}>=<\sum_n\frac{(aa^{\dag}-\alpha^*a)^n}{n!}>=\sum_n\frac{<(\alpha a^{\dag}-\alpha^* a)^n>}{n!}} \end{equation}
      Un campo termal tiene como funciones de correlación: \begin{equation}{ <a>=<a^\dag>=<a^2>=<a^{2\dag}>=0, <a^\dag a>=<n>, <aa^\dag>=1+<n>} \end{equation}
      Lo que facilita el cálculo sumando por sumando: 
      \begin{equation}{ <\alpha a^\dag-\alpha^*a>= \alpha<a^\dag>-\alpha^*<a>=0} \end{equation}
      \begin{equation}{\bra{\alpha}(\alpha a^\dag-\alpha^* a)^2\ket{\alpha}=\alpha^2<a^{\dag 2}>+\alpha^{*2}<a^2>-\lvert\alpha\rvert^2(<aa^\dag+a^\dag a>)} \end{equation} \begin{equation}{=-\lvert\alpha\rvert^2(2<n>+1)} \end{equation}
      En los elementos de la suma, al aumentar los n, por lo tanto, solo quedarán los exponentes pares, de todas formas, se puede hacer el cambio de variable en la sumatoria de ${n=2n}$ (o lo mismo dicho de forma más rigurosa, primero hacer ${m=2n}$ y luego ${n=m}$ y resulta \begin{equation}{ <D(\alpha)>=\sum_n \frac{(-\lvert\alpha\rvert^2(<n>+\frac{1}{2}))^n}{n!}=e^{-\lvert\alpha\rvert^2(<n>+\frac{1}{2})} }\end{equation}
   \subsection{Valor de espectación del cuadrado de n} Análogamente a lo hecho en el libro para calcular ${<n>}$ \begin{equation}{<n^2>=\bra{\alpha,\xi}a^\dag a a^\dag a\ket{\alpha,\xi}} \end{equation}
   \subsection{Factor de Mandel para estado squeezed} El factor de Mandel es: 
   \begin{equation}{ Q_\alpha=\frac{<(\Delta n)^2>-<n>}{<n>}=\frac{<(\Delta n)^2>}{<n>}-1} \end{equation} Para calcularlo para un estado squeezed se necesita calcular los valores de expectación de ${n}$ y ${n^2}$
   \begin{equation}{<n>=\bra{\alpha,\xi}a^\dag a \ket{\alpha,\xi}=\bra{0}S^\dag(\xi)D^\dag(\alpha)a^\dag aD(\alpha)S(\xi) \ket{0}} \end{equation}
   \begin{equation}{=\bra{0}S^\dag(\xi)D^\dag(\alpha)a^\dag D(\alpha)D^\dag(\alpha)aD(\alpha)S(\xi) \ket{0}} \end{equation}
   \begin{equation}{=\bra{0}S^\dag(\xi)(a^\dag+\alpha^*)(a+\alpha)S(\xi) \ket{0}}\end{equation}
   En el producto que se encuentra adentro, los factores ${\alpha^*a}$ y $\alpha a^\dag$ darán 0 por ser combinaciones lineales de los operadores destrucción y creación promediados en el vacío respectivamente. Por lo que queda:
   \begin{equation}{=\bra{0}S^\dag(\xi)a^\dag aS(\xi) \ket{0}
   +\lvert\alpha\rvert^2\bra{0}S^\dag(\xi)S(\xi) \ket{0}}\end{equation}
    \begin{equation}{=\bra{0}S^\dag(\xi)a^\dag S(\xi)S^\dag(\xi) aS(\xi) \ket{0}+
   \lvert\alpha\rvert^2}\end{equation}
      \begin{equation}{=\bra{0}[a^\dag cosh(r)-ae^{-i\theta}sinh(r)][acosh(r)-a^\dag e^{i\theta}sinh(r)]\ket{0}+
   \lvert\alpha\rvert^2} \end{equation}  Obteniéndose    \begin{equation}{ <n>=sinh^2 r+\lvert\alpha\rvert^2\sim \lvert\alpha\rvert^2} \end{equation}
   Luego, para calcular el valor de expectación de ${n^2}$
   \begin{equation}{<n^2>=\bra{\alpha,\xi}a^\dag a a^\dag a\ket{\alpha,\xi}=\bra{0}S^\dag(\xi)D^\dag(\alpha)a^\dag a a^\dag aD(\alpha)S(\xi) \ket{0} }\end{equation} \begin{equation}{=\bra{0}S^\dag(\xi)D^\dag(\alpha)a^\dag D(\alpha)D^\dag(\alpha)aD(\alpha)D^\dag(\alpha)a^\dag D(\alpha)D^\dag(\alpha)aD(\alpha)S(\xi) \ket{0}} \end{equation}
   \begin{equation}{=\bra{0}S^\dag(\xi)D^\dag(\alpha)(a^\dag+\alpha^*)(a+\alpha)(a^\dag+\alpha^*)(a+\alpha)D(\alpha)S(\xi) \ket{0}} \end{equation}
    En el producto que se encuentra adentro,
    \begin{equation}{(a^\dag a +\alpha^* a +a^\dag \alpha+\lvert\alpha\rvert^2)^2=a^\dag aa^\dag a+\alpha^{*2}aa+\alpha^2 a^\dag a^\dag+\lvert\alpha\rvert^4} \end{equation} \begin{equation}{+\lvert\alpha\rvert^2(a^\dag a +\alpha^* a +a^\dag \alpha)+\alpha *a^\dag a a+\alpha a^\dag a a^\dag+\alpha^* a a^\dag a+\alpha a^\dag a^\dag a} \end{equation} No se escribieron los factores ${\lvert\alpha\rvert^2\alpha^*a}$ y ${\lvert\alpha\rvert^2\alpha a^\dag}$ porque darán 0 por ser combinaciones lineales de los operadores destrucción y creación promediados en el vacío respectivamente. Por lo que queda:
      \begin{equation}{=\bra{0}S^\dag(\xi)D^\dag(\alpha)(a^\dag+\alpha^*)(a+\alpha)(a^\dag+\alpha^*)(a+\alpha)D(\alpha)S(\xi) \ket{0}} \end{equation}
   Por lo tanto dará
   \begin{equation}{ <n^2>=} \end{equation}
   Y el cálculo de las fluctuaciones finalmente es
   \begin{equation}{\textcolor{red}{\Rightarrow} (\Delta n)^2=<n^2>-<n>^2=\lvert\alpha\rvert^2[e^{-2r}cos^2(\phi-\frac{\theta}{2})+e^{2r}sin^2(\phi-\frac{\theta}{2})]} \end{equation} \begin{equation}{+sinh^2 r cosh^2 r\sim \lvert\alpha\rvert^2[e^{-2r}cos^2(\phi-\frac{\theta}{2})+e^{2r}sin^2(\phi-\frac{\theta}{2})]} \end{equation}
\begin{equation}\textcolor{red}{\Rightarrow} (\Delta n)^2=<n^2>-<n>^2=\lvert\alpha\rvert^2[e^{-2r}cos^2(\phi-\frac{\theta}{2})+e^{2r}sin^2(\phi-\frac{\theta}{2})]\end{equation} \begin{equation}{+sinh^2 r cosh^2 r\sim \lvert\alpha\rvert^2[e^{-2r}cos^2(\phi-\frac{\theta}{2})+e^{2r}sin^2(\phi-\frac{\theta}{2})]} \end{equation}
 \begin{equation}{+sinh^2 r cosh^2 r\sim \lvert\alpha\rvert^2[e^{-2r}cos^2(\phi-\frac{\theta}{2})+e^{2r}sin^2(\phi-\frac{\theta}{2})]} \end{equation}
   Las simplificaciones fueron hechas considerando que $ {\lvert\alpha\rvert^2>>e^{2r}}$ Entonces, el factor de Mandel se escribe \begin{equation}{Q=\frac{\lvert\alpha\rvert^2[e^{-2r}cos^2(\phi-\frac{\theta}{2})+e^{2r}sin^2(\phi-\frac{\theta}{2})]}{ \lvert\alpha\rvert^2}=e^{-2r}cos^2(\phi-\frac{\theta}{2})+e^{2r}sin^2(\phi-\frac{\theta}{2})} \end{equation}
   Lo que es mínimo cuando \begin{equation}{\phi=\frac{\theta}{2}\textcolor{red}{\Rightarrow} cos(\phi-\frac{\theta}{2})=1, sin(\phi-\frac{\theta}{2})=0\textcolor{red}{\Rightarrow} e^{-2r}-1} \end{equation}
   y es máximo cuando \begin{equation}{\phi=\frac{\theta}{2}+\frac{\pi}{2}\textcolor{red}{\Rightarrow} cos(\phi-\frac{\theta}{2})=0, sin(\phi-\frac{\theta}{2})=1\textcolor{red}{\Rightarrow} e^{2r}-1} \end{equation}
   \section{Teoría Cuántica de la Coherencia}
   \section{Descripción de Espacio de Fases}
       \subsection{Integral de la función de Wigner} Los momentos de la distribución de Wigner, por integración parcial, corresponden a:
       \begin{equation}{ \int d^2\alpha \alpha^r\alpha^{*s}W(\alpha,\alpha^*)=(\frac{\partial}{\partial \eta})^s(-\frac{\partial}{\partial\eta^*})^rX_W(\eta,\eta^*)|_{\eta=0}}\end{equation}
       Considerando ${r=s=1}$
    \begin{equation}{ \int d^2\alpha \lvert\alpha\rvert^2 W(\alpha,\alpha^*)=-\frac{\partial}{\partial\eta}\frac{\partial}{\partial\eta^*}X_W(\eta,\eta^*)|_{\eta=0} }\end{equation}
    La función ${X_W}$ (que es la transformada de la distribución de Wigner vale 
    \begin{equation}{X_W(\eta,\eta^*)=tr[\rho e^{\eta a^\dag-\eta^* a}]=\sum_{r,s}\frac{\eta^s(-\eta^{*})^{r}}{r!s!}[a^ra^{\dag s}]_{sym}=\eta\eta^*tr(\frac{aa^\dag+a^\dag a}{2})} \end{equation} Su derivada vale:
    \begin{equation}{\frac{\partial}{\partial \eta}\frac{\partial}{\partial \eta^*}X_W(\eta,\eta^*)=\frac{\partial}{\partial \eta}\frac{\partial}{\partial \eta^*}tr[\rho e^{\eta a^\dag-\eta^* a}]=tr(\frac{aa^\dag+a^\dag a}{2})} \end{equation} lo que finalmente se obtiene:
       \begin{equation}{ \int d^2\alpha \lvert\alpha\rvert^2 W(\alpha,\alpha^*)=\frac{<aa^\dag+a^\dag a>}{2}} \end{equation}
       \subsection{Cuanticidad de un estado por funciones de correlación} Se puede ver lo cuántico que es el estado ${ \ket{\phi}=a_0\ket{0}+a_1\ket{1}}$ si evalúa la segunda correlación para ese estado \begin{equation}{ g^{(2)}(0)=\frac{<a^\dag a^\dag a a >}{<a^\dag a>^2}} \end{equation}
       \begin{equation}{a_0\bra{0}a^\dag a^\dag a a\ket{0}+a_1\bra{1}a^\dag a^\dag a a\ket{1}=a_0*0+a_1*0=0} \end{equation}
       \begin{equation}{a_0\bra{0}a^\dag a\ket{0}+a_1\bra{1}a^\dag a\ket{1}=a_0*0+a_1*a_1^*=\lvert a_1\rvert^2} \end{equation}
       Por lo tanto \begin{equation}{ g^{(2)}(0)=\frac{0}{\lvert a_1\rvert^2}=0} \end{equation}
       Y el estado se encuentra en los márgenes no clásicos, por lo que es un estado no clásico. 
   \section{Interacción Átomo-Campo}
   \section{Interacción Sistema-Reservorio}
       \subsection{Ecuación Maestra para sistema simple} La Ecuación Maestra del Oscilador Armónico Amortiguado en el cuadro de interacción (los operadores llevan tilde para indicar que se encuentran ahí) se obtiene a partir de la ecuación de Liouville interpretada en el sistema y trazada en el reservorio:
       \begin{equation}{ \frac{d\tilde{\rho}_A}{dt}=\frac{-1}{\hslash^2}\int_0^{t}dt^\prime Tr[\tilde{H_1}(t),[\tilde{H_1}(t^\prime),\tilde{\rho}_{AB}(t^\prime)]]} \end{equation}
       Considerando la condición markoviana (\begin{equation}\tilde{\rho}_{AB}(t)=\tilde{\rho}_A(t)\otimes\tilde{\rho}_B(0)\end{equation}) y el elemento de interacción para el sistema de oscilador armónico amortiguado con una colección infinita de osciladores acoplados (para representar un campo) \begin{equation}{H_1=\sum_j g_ja^\dag b_j+ab_j^\dag} \end{equation} se termina obteniendo:
       \begin{equation}{  \frac{d\tilde{\rho_a}}{dt}=-i\Delta\omega[a^\dag a,\tilde{\rho}_A]+A([a,\tilde{\rho}_Aa^\dag]+[a\tilde{\rho}_A,a^\dag])+B([a^\dag,\tilde{\rho}_Aa]+[a^\dag\tilde{\rho}_A,a])} \end{equation}
       \subsection{Derivación de Ecuaciones desde Ecuación Maestra} Para un oscilador armónico amortiguado \begin{equation}{\frac{\partial Q}{\partial t}=-2BxQ-(2(A-B)x+2Bx^2)\frac{\partial Q}{\partial x}} \end{equation}
       Usando funciones generatrices
       \subsection{Solución de Ecuación Maestra} Usando método de las características se obtiene como solución
       \begin{equation}{Q(x,t)=\frac{e^{-x\frac{\bar{n}e^{-2(A-B(T}}{1+\frac{B}{A-B}x(1-e^{-2(A-B)t})}}}{[1+\frac{B}{A-B}x(1-e^{-2(A-B)t})]}}\end{equation}
   \section{Teoría Cuántica del Laser}
       \subsection{Dinámica Jaynes-Cummings} La dinámica del modelo Jaynes-Cummings, cuyo hamiltoniano es
       \begin{equation}{H=\frac{\hslash\omega_{ab}}{2}\sigma_z+\hslash\omega a^\dag a+\hslash g(\sigma_+ a+\sigma_- a^\dag)} \end{equation} se puede encontrar considerando que ${C_n=a^\dag a +\frac{\sigma_z}{2}}$ es una constante de movimiento: \begin{equation}{\dot{C}_n=\dot{(a^\dag a +\frac{\sigma_z}{2})}=\ddot{C}_n=\ddot{(a^\dag a +\frac{\sigma_z}{2})} =0} \end{equation} y que \begin{equation}{c=\frac{\delta}{2}\sigma_z+g(\sigma_+a+\sigma_-a^\dag)} \end{equation}
       y \begin{equation}{\delta=\omega_{ab}-\omega\textcolor{red}{\Rightarrow} \omega_{ab}=\delta+\omega} \end{equation} con lo que el hamiltoniano del sistema se puede reescribir:
       \begin{equation}{ H=\hslash c+\hslash\omega C_n} \end{equation}
     Escrito el Hamiltoniano, se pueden calcular las derivadas de cada expresión pedida, aprovechando las propiedades de conmutadores siguientes
       \begin{equation}{[\sigma_z,\sigma_+]=\sigma_+,[\sigma_z,\sigma_-]=-\sigma_-, [\sigma_+,\sigma_-]=2\sigma_z} \end{equation} \begin{equation}{[a,a]=[a^\dag,a^\dag]=0, [a,a^\dag]=1} \end{equation}
       \begin{equation}{ [A+B,C]=[A,C]+[B,C], [A,B+C]=[A,B]+[A,C]}\end{equation} \begin{equation}{[AB,C]=A[B,C]+[A,C]B,[A,BC]=[A,B]C+B[A,C],} \end{equation} \begin{equation}{ [A,B]=-[B,A]} \end{equation}
       Para obtener la ecuación diferencial de ${\sigma_-}$
       \begin{equation}{ \textcolor{red}{\Rightarrow} \dot{\sigma_-}=\frac{1}{i\hslash}[\sigma_-,H]=\frac{1}{i\hslash}(\frac{\hslash(\omega+\delta)}{2}[\sigma_-,\sigma_z]+\hslash g([\sigma_-,\sigma_+]a)+[\sigma_-,\sigma_-]a^\dag)} \end{equation}
       \begin{equation}{=\frac{1}{i\hslash}(\frac{\hslash(\omega+\delta)}{2}[\sigma_-,\sigma_z]+\hslash g[\sigma_-,\sigma_+]a)=\frac{1}{i}(\frac{\omega+\delta}{2}\sigma_--2 g\sigma_za)} \end{equation}
       Para seguir con los cálculos, se puede ver que reemplazando ${\sigma_-}$ por ${\sigma_z}$ se obtiene:
       \begin{equation}{ [\sigma_z,H]=\hslash g([\sigma_z,\sigma_+]a+[\sigma_z,\sigma_-]a^\dag)=\hslash g(\sigma_+ a-\sigma_- a^\dag)} \end{equation}
       Y la segunda derivada por lo tanto vale: 
       \begin{equation}{ \ddot{\sigma}_-=\frac{1}{i\hslash}[\dot{\sigma}_-, H]=\frac{1}{i\hslash}[-i(\frac{\omega+\delta}{2}\sigma_--2g\sigma_za^\dag), H]} \end{equation}
       \begin{equation}{ \frac{1}{\hslash}(\frac{\omega+\delta}{2}[\sigma_-,H]-2g[\sigma_za^\dag,H]} )\end{equation}
       Para obtener la ecuación diferencial de ${a}$ \begin{equation}{ \textcolor{red}{\Rightarrow} \dot{a}=\frac{1}{i\hslash}[a,H]=\frac{1}{i\hslash}(\hslash\omega[a,a^\dag a]+\hslash g(\sigma_+[a,a]+\sigma_-[a,a^\dag]))} \end{equation}
       \begin{equation}{=\frac{1}{i\hslash}[a,H]=\frac{1}{i\hslash}(\hslash\omega[a,a^\dag]a+\hslash g\sigma_-[a,a^\dag])=\frac{1}{i}(\omega a+ g\sigma_-)} \end{equation}
       Y la segunda derivada por lo tanto vale:
       \begin{equation}{ \ddot{a}=\frac{1}{i\hslash}[\dot{a},H]=\frac{1}{i\hslash}[-i(\omega a+g\sigma_-),H]=\frac{1}{\hslash}(\omega[a,H]+g[\sigma_-,H])} \end{equation}
       Con lo que se obtiene finalmente       \begin{equation}{\ddot{\sigma}_-+2i(\omega-c)\dot{\sigma}_-+(2\omega c-\omega^2+g^2)\sigma_-=0}\end{equation}
        \begin{equation}{ \ddot{a}+2i(\omega-c)\dot{a}+(2\omega c-\omega^2+g^2)a=0} \end{equation} 
\chapter{Información Cuántica}
\section{Información y Entropía: Introducción}
\subsection{Propiedades de Entropía de Von Neumann}
\begin{enumerate}
\item Ocupando la definición de probabilidad conjunta y condicional $$p(x,y)=p(y|x)p(x)\textcolor{red}{\Rightarrow}$$ $$H(X,Y)=-\sum_{x\in J_x}\sum_{y\in J_y}p(x,y)log p(x,y)=-\sum_{x\in J_x}\sum_{y\in J_y}p(x,y)log [p(y|x)p(x)]=$$ $$-\sum_{x\in J_x}\sum_{y\in J_y}p(x,y)[log p(y|x)+log p(x)]=H(Y|X)-\sum_{x\in J_x}\sum_{y\in J_y}p(x)p(y)log p(x)=$$ $$ H(Y|X)+H(X) $$
\item 
Ocupando la definición de probabilidad conjunta y condicional $$p(x,y)=p(x|y)p(y)\textcolor{red}{\Rightarrow}$$ $$H(X,Y)=-\sum_{x\in J_x}\sum_{y\in J_y}p(x,y)log p(x,y)=-\sum_{x\in J_x}\sum_{y\in J_y}p(x,y)log[ p(x|y)p(y)]=$$ $$-\sum_{x\in J_x}\sum_{y\in J_y}p(x,y)[log p(x|y)+log p(y)]=H(X|Y)-\sum_{x\in J_x}\sum_{y\in J_y}p(x)p(y)log p(y)=$$ $$ H(X|Y)+H(Y) $$
\end{enumerate}
\subsection{Ejemplo simple de Capacidad de un canal}\begin{enumerate}
\item Empezando de forma análoga a como se hizo en la clase
$$ I(X:Y)=H(Y)-H(Y|X)=H(Y)-\sum_{x=0}^1 p(x)H(Y|X=x)$$. A difrerencia de ese ejercicio, los valores no serán simétricos.
$$ H(Y|x=0)= -p(0|0)log(p(0|0))-p(1|0)log(p(1|0))=-1log(1)-0log(0)=0$$
$$ H(Y|x=1)= -p(0|1)log(p(0|1))-p(1|1)log(p(1|1))=-plog(p)-(1-p)log(1-p)=h(p)$$
$$ \textcolor{red}{\Rightarrow} I(X:Y)=H(Y)-(1-q)h(p)$$
¿A qué equivale H(Y)?
$$ H(Y)=-p(y=0)log(p(y=0)-p(y=1)log(p(y=1)) $$ $$=-(q+(1-q)p)log(q+(1-q)p)-((1-q)(1-p))log((1-q)(1-p))=h((1-q)(1-p))$$
$$\textcolor{red}{\Rightarrow} I(X:Y)=(1-q)(-plog(p)-(1-p)log(1-p))+$$ $$ (q+(1-q)p)log(q+(1-q)p)+((1-q)(1-p))log((1-q)(1-p))$$
Maximizando la expresión anterior
$$ 0=\frac{d I}{dq}=(-1)(-plog(p)-(1-p)log(1-p))+ $$ $$[(1-p)log(q+(1-q)p)+1-p]+[(p-1)(log(1-q)+log(1-p))+p-1]$$
Ocupando propiedades de logaritmo, $-(1-p)=1-p$ y $q+(1-q)p=1-(1-q)(1-p)$
$$ \textcolor{red}{\Rightarrow} \frac{plog(p)}{p-1}=log(\frac{1-(1-q)(1-p)}{1-q}) \textcolor{red}{\Rightarrow} p^{\frac{p}{p-1}}=\frac{1}{1-q}-(1-p)$$
$$\frac{1}{p^{\frac{p}{p-1}}-p+1}=1-q \textcolor{red}{\Rightarrow} q= 1-\frac{1}{p^{\frac{p}{p-1}}-p+1}=\frac{p^{\frac{p}{p-1}}-p}{p^{\frac{p}{p-1}}-p+1}$$
Resultando: $$q=\frac{p^{\frac{p}{p-1}}-p}{p^{\frac{p}{p-1}}-p+1} $$
E insertando en la definición de información mútua, la capacidad en función de p queda:
$$C=\frac{h(p)}{p^{\frac{p}{p-1}}-p+1}-h(\frac{(p-1)}{p^{\frac{p}{p-1}}-p+1})$$


\item Si $p=\frac{1}{2}$ la información mutua vale:
$$ I(X:Y)=1-q+\frac{1+q}{2}log(\frac{1+q}{2})+\frac{1-q}{2}log(\frac{1-q}{2})$$
Derivando se obtiene
$$ \frac{d I(X:Y)}{dq}=-1+\frac{1}{2}log(\frac{1+q}{2})+\frac{1}{2}-\frac{1}{2}log(\frac{1-q}{2})-\frac{1}{2}=0 $$
$$\textcolor{red}{\Rightarrow} \frac{1}{2}log(\frac{1+q}{1-q})=1 \textcolor{red}{\Rightarrow} \frac{1+q}{1-q}=4 \textcolor{red}{\Rightarrow} 1+q=4-4q\textcolor{red}{\Rightarrow} q=\frac{3}{5}=0.6$$
Insertándolo en la definición de información mútua quedaría
$$ C=\frac{2}{5}+\frac{4}{5}log(\frac{4}{5})+\frac{1}{5}log(\frac{1}{5})=0.321$$

\end{enumerate}
\subsection{Análisis del operador de Hadamard} Teniendo la matriz de Hadamard
$$ H= \frac{1}{\sqrt{2}}\begin{pmatrix} 1 & 1 \\ 1 & -1\end{pmatrix} \textcolor{red}{\Rightarrow} H^\dag=H$$
Se puede ver también que 
$$ HH^\dag=\frac{1}{2}\begin{pmatrix} 2& 0 \\ 0 & 2\end{pmatrix}= I $$
Por lo que es una matriz (y por ende una operación) unitaria y se comprueba la propiedad pedida. Sus valores y vectores propios son:
$$ \frac{1}{\sqrt{2}}|\begin{matrix} 1-x & 1\\ 1& -1-x \end{matrix}|= (1-x)(-1-x)-1=x^2-1-1=0\textcolor{red}{\Rightarrow} x=\pm 1$$
$$ x=1 \textcolor{red}{\Rightarrow} \frac{1}{\sqrt{2}}\begin{pmatrix} 1-\sqrt{2} & 1\\ 1 & -1-\sqrt{2}\\ \end{pmatrix}\textcolor{red}{\Rightarrow} <\begin{pmatrix} 1+\sqrt{2} \\ 1 \end{pmatrix}> \textcolor{red}{\Rightarrow} (1+\sqrt{2})(\ket{0}+\ket{1})$$
$$ x=-1 \textcolor{red}{\Rightarrow} \frac{1}{\sqrt{2}}\begin{pmatrix}1+\sqrt{2} & 1\\ 1&  -1+\sqrt{2}\\ \end{pmatrix}\textcolor{red}{\Rightarrow} <\begin{pmatrix} 1-\sqrt{2} \\ 1 \end{pmatrix}> \textcolor{red}{\Rightarrow} (1+\sqrt{2})(\ket{0}-\ket{1})$$
\subsection{Teorema de Bell para estado de Bell}se empieza considerando el estado
$$\ket{\phi_{AB}}=\frac{1}{\sqrt{2}}(\ket{0}\ket{\phi_0}-\ket{1}\ket{\phi_1})$$ $$= \frac{1}{\sqrt{2}}(\ket{0}(a\ket{0}+b\ket{1})-\ket{1}(b\ket{0}+a\ket{1}))=\frac{1}{\sqrt{2}}(a\ket{\Phi^-}+b\ket{\Psi^-})$$ 
(Dato que puede ser interesante: El ejemplo visto en clase es con $b=1$ y $a=0$
Escribiendo las matrices de Pauli como
$$ X=\ket{0}\bra{1}+\ket{1}\bra{0}, Y=-i\ket{0}\bra{1}+i\ket{1}\bra{0}, Z=\ket{0}\bra{0}-\ket{1}\bra{1}$$
Los valores de evaluar combinaciones de las matrices de Pauli en esos kets quedan evaluados en función de las constantes a y b y los estados de Bell, considerando que $X$ cambia de $\Phi$ a $\Psi$ el estado (agregando un signo menos solo si la medición se hace desde Alice) y $Z$ cambia el signo del estado (de $+$ a $-$ y viceversa).
$$ X_AX_B\ket{\phi_{AB}}=\frac{-1}{\sqrt{2}}(a\ket{\Phi^-}+b\ket{\Psi^-})=-\ket{\phi_{AB}}$$
$$ X_AZ_B\ket{\phi_{AB}}=-\frac{1}{\sqrt{2}}(a\ket{\Psi_+}+b\ket{\Phi_+})$$
$$ Z_AX_B\ket{\phi_{AB}}=\frac{1}{\sqrt{2}}(a\ket{\Psi_+}+b\ket{\Phi_+})$$
$$ Z_AZ_B\ket{\phi_{AB}}=\frac{1}{\sqrt{2}}(a\ket{\Phi^-}+b\ket{\Psi^-})=\ket{\phi_{AB}}$$
\begin{enumerate}
    \item Si A y B miden los observables usando, como se vio en la clase $$ W_\theta=sin \theta X+cos\theta Z \textcolor{red}{\Rightarrow} <W_\theta W_\theta^\prime>= sin\theta sin\theta^\prime<X_AX_B>+$$ $$sin\theta cos\theta^\prime<X_AZ_B>+cos\theta sin\theta^\prime<Z_AX_B>+cos\theta cos\theta^\prime<Z_AZ_B>$$
    Se obtienen los valores esperados para los distintos operadores
    $$ <X_AX_B>=\frac{-a^2-b^2}{2}=\frac{-1}{2}, <X_AZ_B>=0$$
    $$ <Z_AX_B>=0, <Z_AZ_B>=\frac{a^2-b^2}{2}\textcolor{red}{\Rightarrow} $$ $$ <W_\theta W_\theta^\prime>=a^2(cos\theta cos\theta^\prime-sin\theta sin\theta^\prime)-b^2(cos\theta cos\theta^\prime+sin\theta sin\theta^\prime)$$ 
    $$ $$
    Eligiendo las siguientes direcciones de medición:
    $$ A_1=W_0, B_1=W_{\frac{\pi}{4}}, A_2=W_{\frac{\pi}{2}}, B_2=W_{\frac{3\pi}{4}}$$ el valor esperado de Q es 
    $$<A_1 B_1>=2a^2cos(\frac{\pi}{4})-2b^2cos(-\frac{\pi}{4})=\frac{1}{\sqrt{2}}(a^2-b^2)$$ 
    $$<A_1 B_2>=2a^2cos(\frac{3\pi}{4})-2b^2cos(-\frac{3\pi}{4})=\frac{1}{\sqrt{2}}(-a^2+b^2)$$ 
    $$<A_2 B_1>=2a^2 cos(\frac{3\pi}{4})-2b^2cos(-\frac{\pi}{4})=\frac{1}{\sqrt{2}}(-a^2-b^2)$$ 
    $$<A_2 B_2>=2a^2cos(\frac{5\pi}{4})-2b^2cos(-\frac{\pi}{4})=\frac{1}{\sqrt{2}}(-a^2-b^2)$$
    $$\textcolor{red}{\Rightarrow} <Q>=<A_1B_1>-<A_1B_2>+<A_2B_1>+<A_2B_2>=-2\sqrt{2}b^2$$ Entonces se cumplirá la desigualdad CHSH para el intervalo
    $$ b^2\leq \frac{1}{\sqrt{2}} \textcolor{red}{\Rightarrow} b\leq 0.8409$$
    El mayor nivel de violación de la desigualdad ocurrirá cuando $b=1$, que es el caso visto en clase.
    \item Si $b=\frac{1}{\sqrt{2}}$, $a=\frac{1}{\sqrt{2}}$ y Q se evalúa como
    $$ <Q>=-2\sqrt{2}\frac{1}{2}=-\sqrt{2}$$ 
    Por lo que está en la zona de cumplimiento de la desigualdad de CHSH.
    \item Si $a=1$, $b=0$ y reemplazando en la definición de valores esperados de medición
    $$ <W_\theta W_\theta^\prime>=cos(\theta+\theta^{\prime}) $$
    Para que $<Q>=2\sqrt{2}$, hay que definir los ángulos
    $$cos(\alpha+\beta)-cos(\alpha+\delta)+cos(\gamma+\beta)+cos(\gamma+\delta)=2\sqrt{2}$$ Para esto, debiese bastar
    $$ cos(\alpha+\beta)=-cos(\alpha+\delta)=cos(\gamma+\beta)=cos(\gamma+\delta)=\frac{1}{\sqrt{2}}$$ 
    $$ \alpha=\frac{\pi}{4}-\beta \textcolor{red}{\Rightarrow} -cos(\frac{\pi}{4}-\beta+\delta)=cos(\gamma+\beta)=cos(\gamma+\delta)=\frac{1}{\sqrt{2}}$$
    $$ \frac{\pi}{4}-\beta+\delta=\frac{3\pi}{4}\textcolor{red}{\Rightarrow} \delta-\beta=\frac{\pi}{2}\textcolor{red}{\Rightarrow} cos(\gamma+\beta)=cos(\gamma+\frac{\pi}{2}+\beta)=\frac{1}{\sqrt{2}}$$
    $$ \textcolor{red}{\Rightarrow} \gamma+\beta=-\frac{\pi}{4}$$
    Dejando $\delta=0$, se reemplaza en todas las ecuaciones anteriores quedando $$ \delta=0, \beta=-\frac{\pi}{2}, \alpha=\frac{3\pi}{4}, \gamma=\frac{\pi}{4}$$
    Lo que equivale a dejar las medidas como
    $$A_1=W_{\frac{3\pi}{2}}, A_2=W_{\frac{\pi}{4}}, B_1=W_{-\frac{\pi}{2}}, B_2=W_{0}$$
    \end{enumerate}
\section{Operadores densidad y Traza Parcial}
\subsection{Propiedades de Matriz Densidad} Para el operador
$$ \rho_A=Tr_B\rho_{AB}$$
\begin{enumerate}
    \item Considerando que para la matriz densidad de los 2 sistemas es cierto
    $$ \bra{\psi_A\phi_b}\rho_{ab}\ket{\psi_A\phi_B}\geq 0$$
    Se intenta construir la expresión análoga para la matriz densidad parcial
    $$ \bra{\phi_A}\rho_A\ket{\phi_A}=\bra{\phi_A}Tr_B\rho_{AB}\ket{\phi_A}=\bra{\psi_A\phi_b}\rho_{ab}\ket{\psi_A\phi_B}\geq 0$$
    \item Considerando que para la matriz densidad de los 2 sistemas es cierto
    $$ Tr(\rho_{AB})=1$$ Usando la definición de matriz parcial $$Tr_A(\rho_a)=Tr_A(Tr_B\rho_{ab})= Tr(\rho_{AB})=1$$
    \item Definiendo como operador sobre el que se quiere calcular valor esperado:
    $$ M_{AB}=M_a\otimes\mathbf{I}_B \textcolor{red}{\Rightarrow} <M_{AB}>=tr(M_{AB}\rho_{AB})=tr_A(tr_B(M_a\otimes\mathbf{I}_B\rho_{AB}))$$
    Trasladando la operación traza de B hacia dentro de los paréntesis queda 
    $$M_{AB}=tr_A(tr_B(M_a\otimes\mathbf{I}_B\rho_{AB}))=tr_A(M_a tr_B(\rho_{AB}))=tr_A (M_a\rho_A)$$
\end{enumerate}
Con todo esto se demuestra que $\rho_A$ es un operador densidad válido.
\subsection{Estado totalmente incoherente en otra base} El estado completamente incoherente se escribe en la base lógica como 
$$ \rho=\frac{1}{2}(\ket{0}\bra{0}+\ket{1}\bra{1})=\frac{\mathbf{I}}{2}$$
Una base arbitraria (distinta de la lógica) para el sistema de un qubit no será más que la base lógica rotada:
$$ \begin{pmatrix}\ket{\phi_0}\\ \ket{\phi_1}\end{pmatrix}= 
 \begin{pmatrix} cos \theta& sin\theta\\ -sin\theta& cos\theta\end{pmatrix} \begin{pmatrix} \ket{0}\\\ket{1} \end{pmatrix} \textcolor{red}{\Rightarrow}  \begin{pmatrix}\ket{0}\\ \ket{1}\end{pmatrix}= 
 \begin{pmatrix} cos \theta& -sin\theta\\ sin\theta& cos\theta\end{pmatrix} \begin{pmatrix} \ket{\phi_0}\\\ket{\phi_1} \end{pmatrix}$$
 Reescribiendo la matriz densidad pedida se obtiene: 
 $$ \rho=\frac{1}{2}((cos\theta\ket{\phi_0}-sin\theta\ket{\phi_1})(cos\theta\bra{\phi_0}-sin\theta\bra{\phi_1})+ $$ $$(sin\theta\ket{\phi_0}+cos\theta\ket{\phi_1})(sin\theta\bra{\phi_0}+cos\theta\bra{\phi_1}))$$
 $$ =\frac{1}{2}((cos^2\theta+sin^2\theta)\ket{\phi_0}\bra{\phi_0}+(cos\theta sin\theta-cos\theta sin\theta)(\ket{\phi_0}\bra{\phi_1}+\ket{\phi_1}\bra{\phi_0})+($$ $$cos^2\theta+sin^2\theta)\ket{\phi_1}\bra{\phi_1})=\frac{1}{2}(\ket{\phi_0}\bra{\phi_0}+\ket{\phi_1}\bra{\phi_1})$$
\subsection{Angulo entre puntos del estado de Bloch es el doble del ángulo entre estados} Definiendo el estado puro $\psi$ como el vacío
$$ \ket{\phi}=\ket{0},\ket{\phi}=\frac{1}{2}(\mathbf{I}+\begin{pmatrix}0\\0\\1\end{pmatrix}\sigma)$$ Se puede escribir el estado $\phi$ con los valores estándar como ket:
$$ \ket{\phi}=cos\frac{\theta}{2}\ket{0}+e^{i\phi}sin\frac{\theta}{2}\ket{1} \textcolor{red}{\Rightarrow} \bra{\psi}\ket{\phi}=cos\frac{\theta}{2}$$
Cualquier qubit se puede escribir como un punto en la esfera de Bloch
$$ \ket{\psi}=\frac{1}{2}(\mathbf{I}+\begin{pmatrix}a_1\\a_2\\a_3\end{pmatrix}\sigma)=\frac{1}{2}(\mathbf{I}+\begin{pmatrix}cos\theta sin \phi \\sin\theta sin\phi \\cos\phi \end{pmatrix}\sigma)$$
Lo último considerando coordenadas esféricas. En la situación específica del ket 0 se escribe
$$  \ket{\phi}=\ket{0} \textcolor{red}{\Rightarrow}  \ket{\psi}=\frac{1}{2}(\mathbf{I}+\begin{pmatrix}0\\0\\1\end{pmatrix}\sigma)=\frac{1}{2}\begin{pmatrix} 1+1 & 0 \\ 0 & 1-1\end{pmatrix}=\begin{pmatrix} 1&0\\ 0&0 \end{pmatrix}$$Para obtener el ángulo entre los vectores $\vec{a}$, basta hallar el producto punto.
$$\begin{pmatrix} 0\\ 0\\ 1\end{pmatrix}\cdot\begin{pmatrix}cos\theta sin\phi\\ sin\theta\sin\phi \\ cos\theta\end{pmatrix}=cos\theta $$
Con lo que se comprueba que entre 2 vectores el ángulo que se forma entre sus imágenes en la esfera de Bloch es el doble del ángulo que se forma entre ellos algebraicamente vía producto punto.
\subsection{Trazas y mediciones en estado tripartito}
\begin{enumerate}
    \item Si los sistemas B y C miden en la base solicitada ($\ket{\pm x}=\frac{\ket{0}\pm\ket{1}}{\sqrt{2}}$), implica que proyectarán dicha medición en dicha base. Si les llega desde el sistema A
    $$\rho_{BC}=\sum_{j,k=0}^1 p_{jk}\ket{j}\bra{j}\otimes\ket{k}\bra{k}=$$
    La medición implicará al operador proyección en el sistema en el que se encuentran:
    $$ \rho_C=tr_B\rho_{BC}=\bra{j}\sum_{j,k=0}^1 p_{jk}\ket{j}\bra{j}\otimes\ket{k}\bra{k}\ket{j}=\sum_{j,k=0}^1 \bra{j}p_{jk}\ket{j}\bra{j}\ket{j}\otimes\ket{k}\bra{k}$$
    Definiendo el elemento traza:
    $$ \lambda_k=\sum_{k=0}^1\bra{j}p_{jk}\ket{j}\textcolor{red}{\Rightarrow} \rho_C=\sum_{k=0}^1 \lambda_k \ket{k}\bra{k}$$ 
    $$ \rho_B=tr_C\rho_{BC}=\bra{k}\sum_{j,k=0}^1 p_{jk}\ket{j}\bra{j}\otimes\ket{k}\bra{k}\ket{k}=\sum_{j,k=0}^1 \bra{k}p_{jk}\ket{k}\bra{k}\ket{k}\otimes \ket{j}\bra{j}$$
    Definiendo el elemento traza:
    $$ \lambda_j=\sum_{j=0}^1\bra{k}p_{jk}\ket{k}\textcolor{red}{\Rightarrow} \rho_B=\sum_{j=0}^1 \lambda_j \ket{j}\bra{j}$$ 
    Ahora, usando que el sistema A envió todo en base lógica
    $$ \rho_{BC}=p_{00}\ket{00}\bra{00}+p_{01}\ket{01}\bra{01}+p_{10}\ket{10}\bra{10}+p_{11}\ket{11}\bra{11}$$
    Se debe trazar para los sistemas B y C usando la base $\ket{\pm X}=\frac{1}{\sqrt{2}}(\ket{0}\pm\ket{1})$
    $$ \rho_B=tr_C\rho_{BC}=\bra{+X}_C\rho_{BC}\bra{+X}_C+\ket{-X}_C\rho_{BC}\ket{-X}_C$$
    $$ \rho_C=tr_B\rho_{BC}=\bra{+X}_B\rho_{BC}\bra{+X}_B+\ket{-X}_B\rho_{BC}\ket{-X}_B$$
    Calculando los valores necesarios:
    $$ \bra{+X}\ket{0}\bra{0}\ket{+X}= \frac{1}{2}(\bra{0}+\bra{1})\ket{0}\bra{0}(\ket{0}+\ket{1})=\frac{1}{2}$$
    $$ \bra{+X}\ket{1}\bra{1}\ket{+X}= \frac{1}{2}(\bra{0}+\bra{1})\ket{1}\bra{1}(\ket{0}+\ket{1})=\frac{1}{2}$$
    $$ \bra{-X}\ket{0}\bra{0}\ket{-X}= \frac{1}{2}(\bra{0}-\bra{1})\ket{0}\bra{0}(\ket{0}-\ket{1})=\frac{1}{2}$$
    $$ \bra{-X}\ket{1}\bra{1}\ket{-X}= \frac{1}{2}(\bra{0}-\bra{1})\ket{1}\bra{1}(\ket{0}-\ket{1})=\frac{1}{2}$$
    Reemplazando en lo anterior
    $$ \rho_{BC}=p_{00}\ket{0}_B\ket{0}_C\bra{0}_B\bra{0}_C+p_{01}\ket{0}_B\ket{1}_C\bra{0}_B\bra{1}_C $$ $$+p_{10}\ket{1}_B\ket{0}_C\bra{1}_B\bra{0}_C+p_{11}\ket{1}_B\ket{1}_C\bra{1}_B\bra{1}_C$$
    $$\textcolor{red}{\Rightarrow}\rho_B=tr_C\rho_{BC}=\frac{1}{2}((p_{00}+p_{01})\ket{0}\bra{0}+(p_{10}+p_{11})\ket{1}\bra{1})=\frac{1}{2}(p_0\ket{0}\bra{0}+p_1\ket{1}\bra{1})$$
    $$\textcolor{red}{\Rightarrow}\rho_C=tr_B\rho_{BC}=\frac{1}{2}((p_{00}+p_{10})\ket{0}\bra{0}+(p_{01}+p_{11})\ket{1}\bra{1})=\frac{1}{2}(p_0\ket{0}\bra{0}+p_1\ket{1}\bra{1})$$
    Lo último se debe a que la se suman las probabilidades de ocurrencia en el otro sistema, dejando solo como variables las del sistema propio. Dado que las probabilidades de ocurrencia son iguales, no hay correlación.
    \item La purificación se puede encontrar considerando que la matriz densidad original es diagonal.
    $$ \rho_{BC}=p_{00}\ket{00}\bra{00}+p_{01}\ket{01}\bra{01}+p_{10}\ket{10}\bra{10}+p_{11}\ket{11}\bra{11}$$
    Al no requerir diagonalización, se puede proponer numerar los estados (que son 4 en total) 
    $$ \ket{\Phi_{BC}}= P_1\ket{\Phi_1}+P_2\ket{\Phi_2}+P_3\ket{\Phi_3}+P_4\ket{\Phi_4}$$y usar un sistema de 4 estados que nos permita escribir todo en la forma
    $$ \ket{\psi_{ABC}}=\sum_i\sqrt{p_i}\ket{i}_A\ket{\Phi_{BC}}$$
    De manera que 
    $$ \rho_{BC}=tr_A(\ket{\Phi_{BC}}\bra{\Phi_{BC}})$$
    Con lo que se purifica el estado.
\end{enumerate}
\subsection{Descomposición para estado de Werner}
\begin{enumerate}
    \item  El estado de Werner:
    $$\rho_\lambda=\lambda\ket{\Psi_-}\bra{\Psi_-}+\frac{1-\lambda}{3}(\ket{\Psi_+}\bra{\Psi_+}+\ket{\Phi_+}\bra{\Phi_+}+\ket{\Phi_-}\bra{\Phi_-})$$
    Se puede reescribir considerando que los vectores de Bell forman una base:
    $$\ket{\Psi_-}\bra{\Psi_-}+\ket{\Psi_+}\bra{\Psi_+}+\ket{\Phi_+}\bra{\Phi_+}+\ket{\Phi_-}\bra{\Phi_-}=\mathbf{I}$$
    $$ \textcolor{red}{\Rightarrow} \ket{\Psi_+}\bra{\Psi_+}+\ket{\Phi_+}\bra{\Phi_+}+\ket{\Phi_-}\bra{\Phi_-}=\mathbf{I}-\ket{\Psi_-}\bra{\Psi_-}$$
    $$\textcolor{red}{\Rightarrow} \rho_\lambda=\lambda\ket{\Psi_-}\bra{\Psi_-}+\frac{1-\lambda}{3}(\mathbf{I}-\ket{\Psi_-}\bra{\Psi_-})=\frac{1-\lambda}{3}\mathbf{I}+\frac{4\lambda-1}{3}\ket{\Psi_-}\bra{\Psi_-} $$
    \item Representando una transformación unitaria como una rotación
    $$U=\begin{pmatrix}cos\theta_U&sin\theta_U\\-sin\theta_U&cos\theta_U\end{pmatrix}, U^\dag=\begin{pmatrix}cos\theta_U&-sin\theta_U\\sin\theta_U&cos\theta_U\end{pmatrix}$$
     OJO: para que se cumpla lo pedido es necesario que la matriz unitaria sea la misma para los 2 sistemas. Esto será facil de observar al hacer el desarrollo, aplicando al elemento en que no es trivial la invariancia rotacional: En el estado de Bell $ \ket{\Psi_-}$
     $$ U_{AB}\ket{\Psi_-}=\frac{1}{\sqrt{2}}(U\ket{0}\otimes U\ket{1}-U\ket{1}\otimes U\ket{0})$$
     Aplicar la transformación unitaria a los vectores lógicos implica:
     $$ U\ket{0}=cos\theta_U\ket{0}-sin\theta_U\ket{1}, U\ket{1}=sin\theta_U\ket{0}+cos\theta_U\ket{1}$$
     $$\textcolor{red}{\Rightarrow} U\ket{0}\otimes U\ket{1}=(cos\theta_U\ket{0}-sin\theta_U\ket{1})\otimes(sin\theta_U\ket{0}+cos\theta_U\ket{1})$$ $$=cos\theta_U sin\theta_U \ket{00}+cos^2\theta_U\ket{01}-sin^2\theta_U \ket{10}-sin\theta_U cos \theta_U \ket{11}$$
      $$\textcolor{red}{\Rightarrow} U\ket{1}\otimes U\ket{0}=(sin\theta_U\ket{0}+cos\theta_U\ket{1})\otimes(cos\theta_U\ket{0}-sin\theta_U\ket{1})$$ $$=sin\theta_U cos\theta_U \ket{00}-sin^2\theta_U\ket{01}+cos^2\theta_U \ket{10}-cos\theta_U sin \theta_U \ket{11}$$
      Al hacer la resta de ambos términos se borran los elementos diagonales y queda
      $$U_{AB}\ket{\Phi_-}=(cos^2\theta_U+sin^2\theta_U)\ket{01}+(-cos^2\theta_U-sin^2\theta_U)\ket{10}=\ket{01}-\ket{10}$$
      Con lo que termina dando 
      $$ U\ket{\Psi_-}=\ket{\Psi_-}$$
    \item Se pide hacer un análisis análogo al hecho en la pregunta 4 de la Tarea 1: recordando lo hecho allí, los operadores de Bell y la identidad cambian así: 
    $$ X_AX_B\ket{\Psi_-}=-\ket{\Psi_-} \textcolor{red}{\Rightarrow} <X_AX_B>=tr(X_AX_B\rho_\lambda)=-\frac{4\lambda-1}{3}$$
    $$ X_AZ_B\ket{\Psi_-}=-\ket{\Phi_+} \textcolor{red}{\Rightarrow} <X_AZ_B>=tr(X_AX_B\rho_\lambda)=0$$
    $$ Z_AX_B\ket{\Psi_-}=\ket{\Phi_+} \textcolor{red}{\Rightarrow} <Z_AX_B>=tr(X_AX_B\rho_\lambda)=0$$ 
    $$ Z_AZ_B\ket{\Psi_-}=\ket{\Psi_-} \textcolor{red}{\Rightarrow} <Z_AZ_B>=tr(X_AX_B\rho_\lambda)=-\frac{4\lambda-1}{3}$$
    $$ \textcolor{red}{\Rightarrow} <W_\theta W_{\theta^\prime}>=sin\theta sin \theta^\prime<X_AX_B>+sin\theta cos\theta^\prime<X_AZ_B>+$$ $$cos\theta sin\theta^\prime <Z_AX_B>+cos\theta cos\theta^\prime<Z_AZ_B>=-\frac{4\lambda-1}{3}(sin\theta sin\theta^\prime+cos\theta cos\theta^\prime)$$
    Con lo que la medición en A y B que se debe calcular para ángulos definidos es 
    $$<W_\theta W_{\theta^\prime}>=-\frac{4\lambda-1}{3}cos(\theta-\theta^\prime) $$
    A mide en los ángulos $\theta=0$ y $\theta=\frac{\pi}{2}$. B mide en los ángulos $\theta^\prime=\frac{\pi}{4}$ y $\theta^\prime=\frac{3\pi}{4}$. Para las 4 posibilidades:
    $$\theta=0,\theta^\prime=\frac{\pi}{4}\textcolor{red}{\Rightarrow} <W_\theta W_{\theta^\prime}>=-\frac{4\lambda-1}{3}cos(-\frac{\pi}{4})=-\frac{4\lambda-1}{3\sqrt{2}}$$ 
    $$\theta=0, \theta^\prime=\frac{3\pi}{4}\textcolor{red}{\Rightarrow}  <W_\theta W_{\theta^\prime}>=-\frac{4\lambda-1}{3}cos(-\frac{3\pi}{4})=\frac{4\lambda-1}{3\sqrt{2}}$$
    $$\theta=\frac{\pi}{2},\theta^\prime=\frac{\pi}{4}\textcolor{red}{\Rightarrow}  <W_\theta W_{\theta^\prime}>=-\frac{4\lambda-1}{3}cos(\frac{\pi}{4})=-\frac{4\lambda-1}{3\sqrt{2}}$$ 
    $$\theta=\frac{\pi}{2}, \theta^\prime=\frac{3\pi}{4}\textcolor{red}{\Rightarrow}  <W_\theta W_{\theta^\prime}>=-\frac{4\lambda-1}{3}cos(-\frac{\pi}{4})=-\frac{4\lambda-1}{3\sqrt{2}}$$
    Por lo tanto, el valor para evaluar la desigualdad de Bell es 
    $$\textcolor{red}{\Rightarrow} <Q>=<A_1B_1>-<A_1B_2>+<A_2B_1>+<A_2B_2>=-(2\sqrt{2})\frac{4\lambda-1}{3} $$
    La desigualdad se cumplirá para
    $$ -(2\sqrt{2})\frac{4\lambda-1}{3} \leq -2\textcolor{red}{\Rightarrow} \sqrt{2}\frac{4\lambda-1}{3} \geq 1\textcolor{red}{\Rightarrow} \lambda\geq\frac{1}{4}(\frac{3}{\sqrt{2}}+1)\sim 0.7803$$
    Para valores mayores de $\lambda$, la desigualdad CHSH se violará.
\end{enumerate}
\section{Entrelazamiento de estados}
\subsection{Generalización de descomposición de Schmidt}
Antes de hacer cualquier problema, se realizará el cálculo de descomposición de Schmidt para un sistema de 2 qubits arbitrario, que será útil en los primeros 3 problemas:
$$\ket{\phi}=\alpha\ket{00}+\beta\ket{01}+\gamma\ket{10}+\delta\ket{11}\textcolor{red}{\Rightarrow} \rho_{AB}=\begin{pmatrix}\lvert\alpha\rvert^2 &\alpha\beta^* &\alpha\gamma^*&\alpha\delta^*\\ \beta\alpha^*&\lvert\beta\rvert^2 &\beta\gamma^* &\beta\delta^*\\ \gamma\alpha^*&\gamma\beta^*&\lvert\gamma\rvert^2& \gamma\delta^*\\ \delta\alpha^*&\delta\beta^*&\delta\gamma^*& \lvert\delta\rvert^2 \end{pmatrix}$$
Los coeficientes de Schmidt se obtendrán de diagonalizar una matriz densidad reducida (por convención se usará $\rho_A$, aunque con $\rho_B$ también se puede hacer): 
$$\rho_A=\begin{pmatrix}\lvert\alpha\rvert^2+\lvert\beta\rvert^2 & \alpha\gamma^*+\beta\delta^*\\ \gamma\alpha^*+\delta\beta^* & \lvert\gamma\rvert^2+\lvert\delta\rvert^2\end{pmatrix}$$ 
Para una matriz cualquiera de 2x2 sus valores propios son
$$A=\begin{pmatrix} a&b\\c&d\end{pmatrix}\textcolor{red}{\Rightarrow} \lambda_\pm=\frac{a+d\pm\sqrt{(a+d)^2-\left|\begin{matrix}a &b\\c&d\end{matrix}\right|}}{2}$$ Y sus vectores propios correspondientes normalizados son
$$\vec{v}_+=\frac{1}{\sqrt{(\lambda_+-d)^2+c^2}}\begin{pmatrix} \lambda_+-d\\c\end{pmatrix}, \vec{v}_-=\frac{1}{\sqrt{(\lambda_--d)^2+c^2}}\begin{pmatrix} \lambda_--d\\c\end{pmatrix}$$ Los coeficientes de Schmidt serán las raíces cuadradas de los valores propios de la matriz reducida, y la base para el subsistema A será considerando los vectores propios de dicha matriz. Llamando $a=\lvert\alpha\rvert^2+\lvert\beta\rvert^2$,$b= \alpha\gamma^*+\beta\delta^*$, $c= \gamma\alpha^*+\delta\beta^*$ y $d=\lvert\gamma\rvert^2+\lvert\delta\rvert^2$, y además usando que $a+d=\lvert\alpha\rvert^2+\lvert\beta\rvert^2+\lvert\gamma\rvert^2+\lvert\delta\rvert^2=1$, se obtiene que
$$\textcolor{red}{\Rightarrow}\lambda_\pm=\frac{1\pm\sqrt{1-\left|\begin{matrix}a &b\\c&d\end{matrix}\right|}}{2}$$ Y sus vectores propios correspondientes normalizados son
$$\vec{v}_+=\frac{1}{\sqrt{(\lambda_+-d)^2+c^2}}\begin{pmatrix} \lambda_+-d\\c\end{pmatrix}, \vec{v}_-=\frac{1}{\sqrt{(\lambda_--a)^2+b^2}}\begin{pmatrix} b\\ \lambda_--a\end{pmatrix}$$
Estos vectores propios sugerirán la base para el sistema A en la descomposición:
$$ \ket{+}=\frac{((\lambda_+-d)\ket{0}+c\ket{1})}{\sqrt{(\lambda_+-d)^2+c^2}}
\ket{-}=\frac{(b\ket{0}+(\lambda_--a)\ket{1})}{\sqrt{(\lambda_--a)^2+b^2}}$$
Bastará insertarlos en la definición para encontrar la base de b por inspección. 
\subsection{Ejemplo de Entropía de Von Neumann para un estado} Teniendo el estado ($\alpha=cos\theta$, $\delta=sin\theta$, $\beta=\gamma=0$) $$ \ket{\Psi}_{AB}=cos\theta\ket{00}+sin\theta\ket{11} $$ 
Los coeficientes de Schmidt serán:
$$ \rho_A=\begin{pmatrix} cos^2\theta & 0\\ 0 & sin^2\theta \end{pmatrix}\textcolor{red}{\Rightarrow} C_+=cos\theta, C_-=sin\theta$$
$$ E(\ket{\psi})=-tr\rho_A log(tr\rho_A)=-tr\rho_B \log(tr\rho_B)=-cos^2\theta log_2cos^2\theta-sin^2\theta log_2cos^2\theta$$
Si se aplica una transformación unitaria
$$ \ket{\Psi^\prime}_{AB}=(U\otimes U)\ket{\Psi}_{AB}=(U\otimes U)(cos\theta\ket{00}+sin\theta\ket{11})$$
$$ U=\begin{pmatrix} a&b\\ -e^{i\phi}b^*&e^{i\phi}a^*\end{pmatrix}\textcolor{red}{\Rightarrow} \ket{\Psi^\prime}_{AB}=cos\theta (a\ket{0}-e^{i\phi}b^*\ket{1})\otimes(a\ket{0}-e^{i\phi}b^*\ket{1})$$ $$+sin\theta(b\ket{0}+e^{i\phi}a^*\ket{1})\otimes(b\ket{0}+e^{i\phi}a^*\ket{1})=(cos\theta a^2+sin\theta b^2)\ket{00}+$$ $$e^{i\phi}ab^*(sin\theta-cos\theta)\ket{01}+e^{i\phi}b^*a(sin\theta-cos\theta)\ket{10}+e^{2i\phi}(cos\theta b^{*2}+sin\theta a^{*2})\ket{11}$$
Se puede calcular la entropía de Shannon del estado resultante para ver qué tan entrelazado queda el estado. Para esto se diagonaliza la traza parcial para el sistema A, obteniéndose sus coeficientes de Schmidt. $$ Tr_A\rho_A=\bra{0}\rho_A\ket{0}+\bra{1}\rho_A\ket{1}=2(cos\theta a^2+sin\theta b^2)(cos\theta a^{*2}+sin\theta b^{*2})+$$ $$2\lvert ab\rvert^2(sin\theta-cos\theta)^2=cos^2\theta +sin^2\theta$$
$$ \textcolor{red}{\Rightarrow} E(\ket{\psi})=-cos^2\theta log_2cos^2\theta-sin^2\theta log_2cos^2\theta$$
Ya que la entropía es la misma antes y después de la tranformación, el entrelazamiento del estado no cambia.
\subsection{Ejemplos de descomposiciones de estados entrelazados}
\begin{enumerate}
    \item Para el estado ($\alpha=\delta=a$,$\beta=\gamma=b$)$$\ket{\Psi^{AB}}=a\ket{\Phi^{AB}_+}+b\ket{\Psi^{AB}_+}=a(\ket{00}+\ket{11})+b(\ket{01}+\ket{10})$$ su descomposición de Schmidt se obtendrá de escribir su matriz densidad reducida (acá se ocupa que la traza de la matriz es 1 y por lo tanto $a^2+b^2=\frac{1}{2}$
    $$ \rho_A=\begin{pmatrix} \frac{1}{2}& 2ab\\ 2ab &\frac{1}{2}\end{pmatrix} \textcolor{red}{\Rightarrow} \lvert A\rvert=\frac{1}{4}-4a^2b^2$$ $$\lambda_\pm=\frac{1\pm\sqrt{1-1+16a^2b^2}}{2}=\frac{1\pm4ab}{2}=\frac{1}{2}\pm 2ab$$
    Los valores propios son los cuadrados de los coeficientes de Schmidt. Es claro que $\lambda_++\lambda_-=\frac{1}{2}+2ab+\frac{1}{2}-2ab=1$. 
    $$\lambda_+\textcolor{red}{\Rightarrow}\begin{pmatrix}-2ab&2ab\\2ab&-2ab\end{pmatrix}\textcolor{red}{\Rightarrow}<\begin{pmatrix} 1\\1\end{pmatrix}>\textcolor{red}{\Rightarrow} \ket{+X}$$
    $$\lambda_-\textcolor{red}{\Rightarrow}\begin{pmatrix}2ab&2ab\\2ab&2ab\end{pmatrix}\textcolor{red}{\Rightarrow} <\begin{pmatrix}1\\ -1\end{pmatrix}>\textcolor{red}{\Rightarrow} \ket{-X}$$ Con lo que la descomposición quedaría como 
    $$ \sqrt{\frac{1}{2}+2ab}\ket{+X,+X}+\sqrt{\frac{1}{2}-2ab}\ket{-X,-X}$$
    \item Para el estado ($\alpha=a+b$, $\delta=a-b$, $\beta=\gamma=0$) 
    La matriz densidad queda
    $$ \rho_A=\begin{pmatrix} (a+b)^2& 0\\ 0 & (a-b)^2 \end{pmatrix}$$ La matriz densidad reducida queda diagonal, por lo que la descomposición de Schmidt es trivial y queda como
    $$ (a+b)\ket{00}+(a-b)\ket{11}$$
\end{enumerate}
\subsection{Separabilidad de un estado general} Para un estado (con $\alpha$,$\beta$,$\gamma$ y $\delta$ complejos y cumpliendo condiciones de normalización):
$$ \ket{\Psi}_{AB}=\alpha\ket{00}+\beta\ket{01}+\gamma\ket{10}+\delta\ket{11}$$ será separable si la entropía es 0. Para encontrar la entropía bastará encontrar los coeficientes de Schmidt y aplicarlos en la fórmula
$$ E(\rho)=C_+^2 log_2 C_+^2+C_-^2 log_2 C_-^2$$
Los coeficientes de Schmidt elevados al cuadrado no son más que los autovalores de la matriz densidad reducida.
$$\textcolor{red}{\Rightarrow}\lambda_\pm=\frac{1\pm\sqrt{1-\left|\rho_A\right|}}{2}$$ Si el determinante de dicha matriz vale 0, los autovalores se vuelven 0 y 1, con los que la entropía se termina volviendo 0. Esto equivale a la condición de que las 2 filas o columnas de la matriz densidad tengan los mismos valores. 
\subsection{Máquina de copiado covariante}\begin{enumerate}\item 
Para una máquina de copiado covariante de fase en el estado$$\ket{\phi}=\frac{1}{\sqrt{2}}(\ket{0}+e^{i\phi}\ket{1})\textcolor{red}{\Rightarrow}\ket{\phi}\otimes\ket{0}=\frac{1}{\sqrt{2}}(\ket{00}+e^{i\phi}\ket{10})$$ La máquina de copiado llevará el estado a 
$$\ket{\Sigma}=\frac{1}{\sqrt{2}}(\ket{00}+e^{i\phi}(cos\eta\ket{10}+sin\eta\ket{01}))$$ $$\textcolor{red}{\Rightarrow} \rho_{AB}=\ket{\Sigma}\bra{\Sigma}=\frac{1}{2}(\ket{00}\bra{00}+e^{-i\phi}cos\eta\ket{00}\bra{10}+e^{-i\phi}sin\eta\ket{00}\bra{01}+ $$ $$ e^{i\phi}cos\eta\ket{10}\bra{00}+e^{i\phi}sin\eta\ket{01}\bra{00}+cos\eta sin\eta (\ket{10}\bra{01}+\ket{01}\bra{10}+$$ $$ cos^2\eta\ket{10}\bra{10}+sin^2\eta\ket{01}\bra{01}$$
Para calcular fidelidades hay que obtener las matrices parciales
$$\rho_A=Tr_B\rho_{AB}=\frac{1}{2}(\ket{0}\bra{0}+e^{-i\theta}cos\eta\ket{0}\bra{1}+e^{i\theta}cos\theta\ket{1}\bra{0}+cos^2\eta\ket{1}\bra{1}+sin^2\eta\ket{0}\bra{0})$$
$$\rho_B=Tr_A\rho_{AB}=\frac{1}{2}(\ket{0}\bra{0}+e^{-i\phi}sin\eta\ket{0}\bra{1}+e^{i\phi}sin\eta\ket{1}\bra{0}+cos^2\eta\ket{0}\bra{0}+sin^2\eta\ket{1}\bra{1})$$
La fidelidad de ambas copias resultará de aplicar la operación producto escalar ("sandwich") con el estado inicial
$$ F_A=\bra{\phi}\rho_A\ket{\phi}=\frac{1}{2}(\bra{0}+e^{-i\phi}\bra{1})\rho_A(\ket{0}+e^{i\phi}\ket{1})=\frac{1}{4}(1+2cos\eta+cos^2\eta+sin^2 \eta)$$ $$=\frac{1}{4}(2+2cos\eta)=\frac{1}{2}(1+cos\eta)$$
$$ F_B=\bra{\phi}\rho_B\ket{\phi}=\frac{1}{2}(\bra{0}+e^{-i\phi}\bra{1})\rho_B(\ket{0}+e^{i\phi}\ket{1})=\frac{1}{4}(1+2sin\eta+cos^2\eta+sin^2 \eta)$$ $$=\frac{1}{4}(2+2sin\eta)=\frac{1}{2}(1+sin\eta)$$ 
$$ $$Ahora, si las fidelidades son iguales $$cos\eta=sin\eta=\frac{1}{\sqrt{2}}\textcolor{red}{\Rightarrow} F_A=F_B=\frac{1}{2}(1+\frac{1}{\sqrt{2}})\sim 0.8535$$
\item Con la máquina clonado fase covariante aplicada al estado:
$$ \ket{\phi}=\frac{1}{2}(\sqrt{3}\ket{0}+\ket{1})$$
$$\textcolor{red}{\Rightarrow}  \ket{\phi}\ket{0}=\frac{1}{2}(\sqrt{3}\ket{00}+\ket{10} \textcolor{red}{\Rightarrow}  \ket{\phi_F}=\frac{1}{2}(\sqrt{3}\ket{00}+cos\eta\ket{10}+sin\eta\ket{01}$$
La matriz densidad del vector resultante es:
$$\rho_{AB}=\ket{\phi_F}\bra{\phi_F}=\frac{3}{4}\ket{00}\bra{00}+\frac{\sqrt{3}cos \eta}{4}\ket{00}\bra{10}+\frac{\sqrt{3}sin \eta}{4}\ket{00}\bra{01}+$$ $$\frac{\sqrt{3}cos \eta}{4}\ket{10}\bra{00}+\frac{cos^2\eta}{4}\ket{10}\bra{10}+\frac{cos\eta sin\eta}{4}\ket{10}\bra{01}+ $$ $$\frac{\sqrt{3}cos \eta}{4}\ket{01}\bra{00}+\frac{sin\eta cos\eta}{4}\ket{01}\bra{10}+\frac{sin^2\eta}{4}\ket{10}\bra{10}$$
Trazando para A
$$\rho_A= \frac{3}{4}\ket{0}\bra{0}+\frac{\sqrt{3}cos\eta}{4}(\ket{0}\bra{1}+\ket{1}\bra{0})+\frac{cos^2\eta}{4}\ket{1}\bra{1}+\frac{sin^2\eta}{4}\ket{1}\bra{1}$$
Con esto se calcula la fidelidad
$$F_A=\bra{\phi}\rho_A\ket{\phi}=\frac{1}{16}(9+6cos\eta+cos^2\eta+3sin^2\eta)=\frac{(3+cos\eta)^2}{16}+\frac{3sin^2\eta}{16}$$
Paralelamente, se puede trazar para B
$$\rho_B= \frac{3}{4}\ket{0}\bra{0}+\frac{\sqrt{3}sin\eta}{4}(\ket{0}\bra{1}+\ket{1}\bra{0})+\frac{sin^2\eta}{4}\ket{0}\bra{0}+\frac{cos^2\eta}{4}\ket{0}\bra{0}$$
y se obtiene como fidelidad
$$F_B=\bra{\phi}\rho_A\ket{\phi}=\frac{1}{16}(9+6sin\eta+sin^2\eta+3cos^2\eta)=\frac{(3+sin\eta)^2}{16}+\frac{3cos^2\eta}{16}$$
Si ambas fidelidades son iguales
$$F_A=F_B=\frac{1}{16}((3+\frac{1}{\sqrt{2}})^2+3(\frac{1}{\sqrt{2}})^2)\sim 0.9526650...>\frac{5}{6}$$
Con lo que se obtiene finalmente que para este estado la máquina de clonado fase covariante copia con mayor fidelidad que la máquina de clonado universal. Esto es curioso, porque en principio este estado no pertenece a la zona dónde, de acuerdo a lo visto en clase, la máquina de fase covariante es mejor clonadora que la universal. Aunque esto tiene sentido si se observa que el estado está bastante inclinado hacia $\ket{0}$, que en esta máquina se clona sin error.
\end{enumerate}
\subsection{Discriminación de estados usando POVM} Para $\beta\in[0,\frac{\pi}{4}]$, se puede obtener un discriminador de los siguientes 2 estados usando POVM:
$$\ket{\phi_0}=cos\beta\ket{0}+sin\beta\ket{1}, \ket{\phi_1}=sin\beta\ket{0}+cos\beta\ket{1}$$
Para describir la medida generalizada corresponde hallar los operadores $\Pi$ correspondientes. Lo que se sabe de ellos son las probabilidades de éxito (denotadas $p_{s0}$ y $p_{s1}$) y sus proababilidades de error (denotadas $p_{e0}$ y $p_{e1}$).  
$$ p_{e0}=\bra{\phi_0}\Pi_1\ket{\phi_0}, p_{e1}=\bra{\phi_1}\Pi_0\ket{\phi_1}$$
Considerando que los operadores se pueden escribir como proyectores en el espacio de 1 qubit en una base desconocida
$$\Pi_0=\ket{0^\prime}\bra{0^\prime},\Pi_1=\ket{1^\prime}\bra{1^\prime} $$
$$\ket{0^\prime}=x\ket{0}+y\ket{1}, \ket{1^\prime}=-y\ket{0}+x\ket{1} $$
Con $x^2+y^2=1$, siendo $x$ e $y$ números reales. Se calcula la probabilidad de falla considerando la redefinición de los operadores $\Pi$
$$p_{e0}=\bra{\phi_0}\Pi_1\ket{\phi_0}=\bra{\phi_0}\ket{1^\prime}\bra{1^\prime}\ket{\phi_0}=\lvert\bra{1^\prime}\ket{\phi_0}\rvert^2$$ $$=\lvert(-y\bra{0}+x\bra{1})(cos\beta\ket{0}+sin\beta\ket{1})\rvert^2=(xsin\beta-ycos\beta)^2$$
$$p_{e1}=\bra{\phi_1}\Pi_0\ket{\phi_1}=\bra{\phi_1}\ket{0^\prime}\bra{0^\prime}\ket{\phi_1}=\lvert\bra{0^\prime}\ket{\phi_1}\rvert^2$$ $$=\lvert(x\bra{0}+\ket{1})(sin\beta\ket{0}+cos\beta\ket{1})\rvert^2=(xsin\beta+ycos\beta)^2$$
La probabilidad de error total será (considerando que ambos estados son igualmente preparados)
$$p_e=\frac{p_{e0}+p_{e1}}{2}=x^2sin^2\beta+y^2cos^2 \beta=sin^2\beta+y^2(cos^2\beta-sin^2\beta)$$
Esto último considerando que $x^2+y^2=1$. Se elige convenientemente y en vez de x dado que para comparar buscamos un intervalo en que el término $cos^2\beta-sin^2\beta$ sea siempre positivo, y para $\beta\in[0,\frac{\pi}{4}]$ así lo será. Se deriva lo anterior para obtener el valor mínimo de error.
$$\frac{dp_e}{dy}=2y(cos^2\beta-sin^2\beta)=0\textcolor{red}{\Rightarrow} x=1,y=0 \textcolor{red}{\Rightarrow} p_e^{min}=sin^2\beta $$ Lo que tiene sentido porque si $y=0$ y $\beta=0$   la base de los operadores del POVM se convierte en la base lógica y copia sin error (algo que se puede hacer si los estados son ortogonales entre si).Además, si $\lvert\bra{\phi_0}\ket{\phi_1}\rvert^2=\alpha$
$$\bra{\phi_0}\ket{\phi_1}=2cos\beta sin\beta\textcolor{red}{\Rightarrow}\alpha=4cos^2\beta sin^2\beta=4sin^2\beta(1-sin^2\beta)$$ 
De acá se obtiene una ecuación cuadrátrica que permitiría tener la probabilidad de error como función de $\textcolor{blue}{\alpha}$
$$sin^4\beta+sin^2\beta+\frac{\alpha}{4}=0\textcolor{red}{\Rightarrow} sin^2\beta=\frac{1\pm\sqrt{1-\alpha^2}}{2}$$
Se obtienen 2 valores para el $sin^2\beta$ dependientes de $\alpha$, que corresponderán tanto a la probabilidad de error mínima como a la probabilidad de éxito.
$$p_s=\frac{1+\sqrt{1-\alpha^2}}{2}, p_e^{min}=\frac{1-\sqrt{1-\alpha^2}}{2}$$
\section{Operaciones Cuánticas}
\subsection{Ejemplo de Fidelidad de Canales Cuánticos}\begin{itemize}
\item para bit-flip la matriz resultante es:
$$ \begin{pmatrix} \lvert\alpha\rvert^2 & \alpha\beta^* \\ \alpha^*\beta& \lvert\beta\rvert^2 \end{pmatrix} \textcolor{red}{\Rightarrow} \begin{pmatrix} (1-p)\lvert\alpha\rvert^2+p\lvert\beta\rvert^2 & (1-p)\alpha\beta^*+p\alpha^*\beta \\ (1-p)\alpha^*\beta+p\alpha\beta^*& (1-p)\lvert\beta\rvert^2 +p\lvert\alpha\rvert^2\end{pmatrix}=\begin{pmatrix} x_1 & y_1 \\ y_1^* &1-x_1 \end{pmatrix} $$
Y la fidelidad entonces es:
$$F=\bra{\phi}\rho^\prime\ket{\phi}=\begin{pmatrix}\alpha^*&\beta^*\end{pmatrix}\begin{pmatrix} x_1 & y_1 \\ y_1^* &1-x_1 \end{pmatrix}\begin{pmatrix}\alpha\\ \beta\end{pmatrix}=x_1\lvert\alpha\rvert^2+y_1\alpha^*\beta+\beta^*y_1^*\alpha+(1-x_1)\lvert\beta\rvert^2$$
Reemplazando x e y finalmente resulta
$$ F=((1-p)\lvert\alpha\rvert^2+p\lvert\beta\rvert^2)\lvert\alpha\rvert^2+( (1-p)\alpha\beta^*+p\alpha^*\beta)\alpha^*\beta+((1-p)\alpha^*\beta+p\alpha\beta^*)\beta^*\alpha+$$ $$((1-p)\lvert\beta\rvert^2+p\lvert\alpha\rvert^2)\lvert\beta\rvert^2= 1-p+2p\lvert\alpha\rvert^2\lvert\beta\rvert^2(1+\frac{(\alpha\beta^*)^2+(\beta\alpha^*)^2}{2\lvert\alpha\rvert^2\lvert\beta\rvert^2})$$
Luego de simplificar (usando que $\lvert\alpha\rvert^2+\lvert\beta\rvert^2$) y considerando $\frac{\alpha}{\lvert\alpha\rvert}=e^{i\phi_\alpha}\textcolor{red}{\Rightarrow} \frac{\alpha^*}{\lvert\alpha\rvert}=e^{-i\phi_\alpha}$ y $\frac{\beta}{\lvert\beta\rvert}=e^{i\phi_\beta}\textcolor{red}{\Rightarrow} \frac{\beta^*}{\lvert\beta\rvert}=e^{-i\phi_\beta}$, se puede reescribir la última expresión como función trigonométrica, quedando 
$$F_A= 1-p+2p\lvert\alpha\rvert^2\lvert\beta\rvert^2(1+cos(2(\phi_\beta-\phi_\alpha))$$
\item para depolarizing channel la matriz resultante es:
$$ \begin{pmatrix} \lvert\alpha\rvert^2 & \alpha\beta^* \\ \alpha^*\beta& \lvert\beta\rvert^2 \end{pmatrix} \textcolor{red}{\Rightarrow} \begin{pmatrix} (1-p)\lvert\alpha\rvert^2+\frac{p}{2} & (1-p)\alpha\beta^* \\ (1-p)\alpha^*\beta & (1-p)\lvert\beta\rvert^2+\frac{p}{2} \end{pmatrix} =\begin{pmatrix} x_2 & y_2 \\ y_2^* &1-x_2 \end{pmatrix}$$
Y la fidelidad entonces es:
$$F=\bra{\phi}\rho^\prime\ket{\phi}=\begin{pmatrix}\alpha^*&\beta^*\end{pmatrix}\begin{pmatrix} x_2 & y_2 \\ y_2^* &1-x_2 \end{pmatrix}\begin{pmatrix}\alpha\\ \beta\end{pmatrix}=x_2\lvert\alpha\rvert^2+y_2\alpha^*\beta+\beta^*y_2^*\alpha+(1-x_2)\lvert\beta\rvert^2$$
Reemplazando en y finalmente resulta
$$ F=((1-p)\lvert\alpha\rvert^2+\frac{p}{2})\lvert\alpha\rvert^2+((1-p)\alpha\beta^*)\alpha^*\beta+((1-p)\alpha^*\beta)\beta^*\alpha+((1-p)\lvert\beta\rvert^2+\frac{p}{2})\lvert\beta\rvert^2$$
$$=\frac{p}{2}+(1-p)(\lvert\alpha\rvert^4+\lvert\beta\rvert^4+2\lvert\alpha\rvert^2\lvert\beta\rvert^2)=\frac{p}{2}+(1-p)(\lvert\alpha\rvert^2+\lvert\beta\rvert^2)^2=\frac{p}{2}+(1-p)=1-\frac{p}{2}$$
Acá se simplificó usando $\lvert\alpha\rvert^2+\lvert\beta\rvert^2=1$
\end{itemize}
\subsection{Resultado de hacer un doble amplitude damping} Para el canal Amplitude Damping el estado evoluciona a $$ \begin{pmatrix} \lvert\alpha\rvert^2 & \alpha\beta^* \\ \alpha^*\beta& \lvert\beta\rvert^2 \end{pmatrix} \textcolor{red}{\Rightarrow} \begin{pmatrix} \lvert\alpha\rvert^2+p\lvert\beta\rvert^2 &\sqrt{1-p}\alpha\beta^* \\ \sqrt{1-p}\alpha^*\beta& (1-p)\lvert\beta\rvert^2\end{pmatrix}= $$
Esto se obtiene de calcular los operadores de Kraus
$$E_0=\ket{0}\bra{0}+\sqrt{1-p}\ket{1}\bra{1}, E_1=\sqrt{p}\ket{0}\bra{1}$$
$$\rho^\prime=E_0\rho E_0^\dag+E_1^\dag \rho E_1^\dag$$
Entonces, hacer un amplitude damping seguido de otro es
$$\rho{\prime\prime}=E_0(p_2)\rho^\prime E_0^\dag(p_2)+E_1(p_2)\rho^\prime E_1^\dag(p_2)=E_0(p_2)(E_0(p_1)\rho E_0^\dag(p_1)+E_1(p_1) \rho E_1^\dag (p_1))E_0^\dag(p_2)$$ $$+E_1(p_2)(E_0(p_1)\rho E_0^\dag(p_1)+E_1(p_1) \rho E_1^\dag (p_1)) E_1^\dag(p_2)$$
Haciendo el producto de los operadores de Kraus
$$E_0(p_2)E_0(p_1)=(\ket{0}\bra{0}+\sqrt{1-p_2}\ket{1}\bra{1})(\ket{0}\bra{0}+\sqrt{1-p_1}\ket{1}\bra{1})=\ket{0}\bra{0}$$ $$+\sqrt{1-p_2}\sqrt{1-p_1}\ket{1}\bra{1} $$ 
$$E_0(p_2)E_1(p_1)=(\ket{0}\bra{0}+\sqrt{1-p_2}\ket{1}\bra{1})(\sqrt{p_1}\ket{0}\bra{1})=\sqrt{p_1}\ket{0}\bra{1}$$
$$E_1(p_2)E_0(p_1)=(\sqrt{p_2}\ket{0}\bra{1})(\ket{0}\bra{0}+\sqrt{1-p_1}\ket{1}\bra{1})=\sqrt{p_2}\sqrt{1-p_1}\ket{0}\bra{1}$$
$$E_1(p_2)E_1(p_1)=(\sqrt{p_2}\ket{0}\bra{1})(\sqrt{p_1}\ket{0}\bra{1})=0$$ Ingresando los cálculos en la matriz resultado de las 2 operaciones $$\rho^{\prime\prime}=\ket{0}\bra{0}\rho\ket{0}\bra{0}+(1-p_2)(1-p_1)\ket{1}\bra{1}\rho\ket{1}\bra{1}+(p_1+p_2-p_2p_1)\ket{0}\bra{1}\rho\ket{1}\bra{0} $$
Definiendo
$$ E_0(p_1,p_2)=\ket{0}\bra{0}+\sqrt{(1-p_2)(1-p_1)}\ket{1}\bra{1}$$
$$ E_1(p_1,p_2)=\sqrt{p_1+p_2-p_2p_1}\ket{0}\bra{1}=\sqrt{1-(1-p_1)(1-p_2)}\ket{0}\bra{1}$$
Se demuestra que la doble operación equivale a hacer un solo amplitude damping con parámetro $(1-p_1)(1-p_2)$.
$$\rho^{\prime\prime}=E_0(p_1,p_2)\rho E_0^\dag(p_1,p_2)+E_1(p_1,p_2)\dag \rho E_1^\dag(p_1,p_2)$$
\subsection{Entrelazamientos para estados de Werner} Para el estado $$\rho_{AB}=p\ket{\Phi^+}\bra{\Phi^+}+(1-p)\ket{\Phi^-}\bra{\Phi^-}=\frac{1}{2}\begin{pmatrix}p+1-p&0&0&p-1+p\\0&0&0&0\\0&0&0&0\\p-1+p&0&0&p+1-p\end{pmatrix} $$
El operador Y aplicado a los estados de la base lógica y la base de Bell es:
$$ Y\ket{0}=-i\ket{1}, Y\ket{1}=i\ket{0} $$
$$\textcolor{red}{\Rightarrow} (Y\otimes Y)\ket{00}=-\ket{11},(Y\otimes Y)\ket{01}=\ket{10} $$
$$\textcolor{red}{\Rightarrow} (Y\otimes Y)\ket{10}=\ket{01},(Y\otimes Y)\ket{11}=-\ket{00} $$
$$\textcolor{red}{\Rightarrow} (Y\otimes Y)\ket{\Phi_+}=-\ket{\Phi_+},(Y\otimes Y)\ket{\Phi_-}=\ket{\Phi_-}$$
$$\textcolor{red}{\Rightarrow} (Y\otimes Y)\ket{\Psi_+}=\ket{\Psi_+},(Y\otimes Y)\ket{\Psi_-}=-\ket{\Psi_-}$$
Por lo tanto $\Tilde{\rho}_{AB}=\rho_{AB}$ y hay que encontrar los valores propios de $\rho_{AB}^2$
$$\begin{pmatrix}1&0&0&2p-1\\0&0&0&0\\0&0&0&0\\2p-1&0&0&1\end{pmatrix}\begin{pmatrix}1&0&0&2p-1\\0&0&0&0\\0&0&0&0\\2p-1&0&0&1\end{pmatrix}=\begin{pmatrix}4p^2-4p+2&0&0&4p-2\\0&0&0&0\\0&0&0&0\\4p-2&0&0&4p^2-4p+2\end{pmatrix}$$
Los valores propios normalizados entonces son $0$,$0$,$p^2$ y$(1-p)^2$. Entonces la concurrencia valdrá:
$$C(\rho)=max(0,1-p-p)=1-2p$$
Lo que implica que la concurrencia valdrá 1 cuando p vale 0 y valdrá 0 cuando $p=\frac{1}{2}$ El entrelazamiento de formación vale entonces:
$$E_f(\rho_{AB})=h(\frac{1+\sqrt{1-(1-2p)^2}}{2})$$ Que, al igual que la concurrencia valdrá 0 cuando $p=0$ y 1 cuando $p=\frac{1}{2}$.
$$\rho_{AB}^{T_B}=\frac{p}{2}(\ket{00}\bra{00}+\ket{01}\bra{10}+\ket{10}\bra{01}+\ket{11}\bra{11})$$ $$+\frac{1-p}{2}(\ket{00}\bra{00}+\ket{01}\bra{10}-\ket{10}\bra{01}+\ket{11}\bra{11})$$
Escribiéndolo matricialmente se puede obtener los valores y vectores propios:
$$ \begin{pmatrix} \frac{1}{2}&0&0&0\\0&0&p-\frac{1}{2}&0\\0&p-\frac{1}{2}&0&0\\0&0&0&\frac{1}{2}\end{pmatrix}\textcolor{red}{\Rightarrow} \lambda_1=\lambda_2=\frac{1}{2}, \lambda_3=p-\frac{1}{2}, \lambda_4=\frac{1}{2}-p$$
El valor de $\lambda$ será negativo para $[0,\frac{\pi}{2})$, por lo que el estado será entrelazado para ese intervalo. Entonces, construyendo la norma de la matriz (considerando que $\lambda_4$ será siempre positivo en el intervalo y por lo tanto se puede considerar como norma de $\lambda_3$ y $\lambda_4$) se obtiene la negatividad y su respectiva medición de entrelazamiento:
$$\lVert\rho_{AB}^{T_B}\lVert=\frac{1}{2}+\frac{1}{2}+\frac{1}{2}-p+\frac{1}{2}-p=2-2p$$
$$\textcolor{red}{\Rightarrow} N(\rho_{AB})=\frac{\lVert\rho_{AB}^{T_B}\rVert-1}{2}=\frac{2-2p-1}{2}=\frac{1}{2}-p$$ 
$$\textcolor{red}{\Rightarrow} E_N(\rho_{AB}=log_2\lVert\rho_{AB}^{T_B}\rVert=log_2(2-2p)$$
El entrelazamiento, siendo distinto al anterior, también valdrá distinto de 0 para $[0,\frac{1}{2})$. (Todo lo anterior tiene sentido considerando que cuando p vale 0 se tiene un estado maximalmente entrelazado y cuando vale $\frac{1}{2}$ se vuelve separable).
\subsection{Ejemplo de entrelazamiento de estados} Para el estado $$\rho_{AB}=\frac{1}{2}(\ket{00}\bra{00}+\ket{11}\bra{11})+\frac{z}{2}(\ket{00}\bra{11}+\ket{11}\bra{00})=\begin{pmatrix}\frac{1}{2}&0&0&\frac{z}{2}\\0&0&0&0\\0&0&0&0\\ \frac{z}{2}&0&0&\frac{1}{2}\end{pmatrix}$$ A partir de los cálculos para el operador Y hechos en el ejercicio anterior se obtiene también que  $\Tilde{\rho}_{AB}=\rho_{AB}$ Entonces se necesita calcular los autovalores de $\rho_{AB}^2$
$$\begin{pmatrix}\frac{1}{2}&0&0&\frac{z}{2}\\0&0&0&0\\0&0&0&0\\ \frac{z}{2}&0&0&\frac{1}{2}\end{pmatrix}\begin{pmatrix}\frac{1}{2}&0&0&\frac{z}{2}\\0&0&0&0\\0&0&0&0\\ \frac{z}{2}&0&0&\frac{1}{2}\end{pmatrix}=\begin{pmatrix}\frac{1+z^2}{4}&0&0&\frac{z}{2}\\0&0&0&0\\0&0&0&0\\ \frac{z}{2}&0&0&\frac{1+z^2}{4}\end{pmatrix}$$ Los valores propios entonces son $0$,$0$,$\frac{(z+1)^2}{4}$ y $\frac{(1-z)^2}{4}$. Entonces la concurrencia valdrá: 
$$C(\rho)=max(0,\frac{z+1}{2}-\frac{1-z}{2})=z$$ Entonces 0 avanzará de igual forma que la constante z, por lo tanto el entrelazamiento de formación será: $$E_f(\rho_{AB})=h(\frac{1+\sqrt{1-z^2}}{2})$$. Cuando $z=0$ el entrelazamiento de formación vale 0 (lo que tiene sentido al ser un estado separable en esa situación y vale 1 cuando $z=1$ (que es un estado maximalmente entrelazado). En cuanto a la negatividad, usando la matriz parcialmente traspuesta en el segundo qubit:
$$\rho_{AB}^{T_B}=\frac{1}{2}(\ket{00}\bra{00}+\ket{11}\bra{11}+\frac{z}{2}(\ket{01}\bra{10}+\ket{10}\bra{01})=\begin{pmatrix}\frac{1}{2}&0&0&0\\0&0&\frac{z}{2}&0\\0&\frac{z}{2}&0&0\\0&0&0&\frac{1}{2}\end{pmatrix}$$
Obteniendo los valores propios se obtiene la norma de matriz, y por lo tanto la negatividad y su respectivo entrelazamiento. Se observa que $\lambda_4$ es negativo para $(0,1]$, entonces el estado estará entrelazado en el mismo intervalo. Considerando entonces, que la norma para $\lambda_3$ y $\lambda_4$ es $\lambda_3$, la negatividad finalmente vale:
$$\lambda_1=\lambda_2=\frac{1}{2},\lambda_3=\frac{z}{2},\lambda_4=-\frac{z}{2}\textcolor{red}{\Rightarrow} \lVert\rho_{AB}^{T_B}\rVert=\frac{1}{2}+\frac{1}{2}+\frac{z}{2}+\frac{z}{2}=z+1$$
$$\textcolor{red}{\Rightarrow} N(\rho_{AB})=\frac{\lVert\rho_{AB}^{T_B}\rVert-1}{2}=\frac{z}{2}$$ 
$$\textcolor{red}{\Rightarrow} E_N(\rho_{AB})=log_2\lVert\rho_{AB}^{T_B}\rVert=log_2(z+1)$$
\subsection{Teleportación de entrelazamiento} Nombrando $\omega=e^{i\frac{2\pi}{d}}$, se escribe el cambio de base generalizado de una base lógica a una base maximalmente entrelazada (que en dimensión 2 será la base de Bell y en base 3 incluirá al vector GHZ) como:
$$\ket{\psi_{jk}}_{12}=\frac{1}{\sqrt{d}}\sum_n \omega^{jn}\ket{n}_1\ket{n-k}_2$$
Entonces, escribiendo un producto entre un estado arbitrario en base lógica y los estados maximalmente entrelazados en función de la base lógica:
$$\ket{\chi}_1=\sum_m\alpha_m\ket{m}\textcolor{red}{\Rightarrow} \ket{\chi}_1\ket{\psi_{jk}}_{23}=\frac{1}{\sqrt{d}}\sum_{nm} \omega^{jn}\alpha_m\ket{m}_1\ket{n}_2\ket{n-k}_3$$
Ingresando una transformada inversa para escribir el estado en los sistemas 1 y 2 en la base maximalmente entrelazada:
$$\ket{\chi}_1\ket{\psi_{jk}}_{23}=\frac{1}{d}\sum_{nm} \omega^{jn}\alpha_m(\sum_l \omega^{-lm}\ket{\psi_{l,m-n}}_{12})\ket{n-k}_3$$
Se puede reescribir esto considerando el cambio de variable $\nu=m-n\textcolor{red}{\Rightarrow} n=m-\nu$
$$=\frac{1}{d}\sum_{m\nu l}\omega^{j(m-\nu)}\omega^{-lm}\alpha_m\ket{\phi_{l\nu}}_{12}\ket{m-\nu-k}_3 $$
Usando los operadores definidos se reescribe:
$$X=\sum_{n=0}^{d-1}\ket{n+1}\bra{n} \textcolor{red}{\Rightarrow} X^{-1}=\sum_{n=0}^{d-1}\ket{n-1}\bra{n}, Z=\sum_{n=0}^{d-1}e^{\frac{i2\pi n}{d}}\ket{n}\bra{n}$$
$$ \textcolor{red}{\Rightarrow} \frac{1}{d}\sum_{l\nu}\ket{\phi_{l\nu}}_{12}\omega^{-j\nu}X^{-(\nu+k)}Z^{j-l}\sum_m\alpha_m\ket{m}_3$$
Y llamando a la combinación de operadores unitarios $U$, finalmente se prueba que
$$ U(l,m)=X^{-(k+m)}Z^{j-l},  \textcolor{red}{\Rightarrow} \ket{\chi}_1\ket{\psi_{jk}}_{23}=\frac{1}{d}\sum_{l,m=0}^{d-1}e^{\frac{-i2\pi jm}{d}}\ket{\psi_{lm}}_{12}U(l,m)\ket{\chi}_3$$
\subsection{Intercambio de entrelazamiento} Reescribiendo los estados en la base de Bell como estados en la base lógica, usando el procedimiento de la parte anterior.  
$$\ket{\psi_{\alpha \beta}}_{12}=\frac{1}{\sqrt{d}}\sum_n^{d-1} \omega^{\alpha n}\ket{n}_1\ket{n-\beta}_2$$
$$\ket{\psi_{\mu\nu}}_{34}=\frac{1}{\sqrt{d}}\sum_k^{d-1} \omega^{\mu k}\ket{n}_3\ket{k-\nu}_4$$
$$\textcolor{red}{\Rightarrow} \ket{\psi_{\alpha \beta}}_{12}\ket{\psi_{\mu\nu}}_{34}=\frac{1}{d}\sum_{n,k}^{d-1} \omega^{\alpha n}\ket{n}_1\ket{n-\beta}_2 \omega^{\mu k}\ket{k}_3\ket{k-\nu}_4$$
Reescribiendo en la sumatoria, escribiendo 2 y 3 y 1 y 4 como pares en la base maximalmente entrelazada: $$=\frac{1}{d^2}\sum_{nk}^{d-1}\omega^{\alpha n}\omega^{\mu k}(\sum_{l}\omega^{-l(n-\beta)}\ket{\psi_{l,n-\beta-k}}_{23})(\sum_{r}\omega^{-rn}\ket{\psi_{r,n-k+\nu}}_{14})$$
Haciendo el cambio de letras en la sumatoria $m=n-\beta-k\textcolor{red}{\Rightarrow} n=m+\beta+k \textcolor{red}{\Rightarrow} n-k=m+\beta$
$$=\frac{1}{d^2}\sum_{mlkr}\omega^{\alpha (m+\beta+k)}\omega^{\mu k}\omega^{-l(m+k)}\omega^{-r(m+\beta+k)}\ket{\psi_{l,m}}_{23}\ket{\psi_{\alpha+\mu-l,m+\beta+\nu}}_{14}$$
Reordenando los elementos en los exponentes de $\omega$ $$ \alpha(m+\beta+k)+\mu k-l(m+k)-r(m+\beta+k)=k(\alpha+\mu-l-r)+\alpha(\beta+m)-lm-rm-r\beta$$
Se puede simplificar lo anterior usando la propiedad de los números complejos que convierte la sumatoria de las raíces de la unidad en un delta de Kroenecker:
$$\sum_{k=0}^{d-1}\omega^{k(\alpha+\mu-l-r)}=d\delta_{r,\alpha+\mu-l}\textcolor{red}{\Rightarrow} =\sum_{ml}^{d-1} \omega^{\alpha(\beta+m)-lm}\omega^{-(\alpha+\mu-l)(m+\beta)}\ket{\psi_{l,m}}_{23}\ket{\psi_{\alpha+\mu-l,m+\beta+\nu}}_{14} $$
Simplificando los elementos en los exponentes de $\omega$
$$\alpha(\beta+m)-lm-(\alpha+\mu-l)(m+\beta)=-\mu(m+\beta)+l\beta$$
$$\textcolor{red}{\Rightarrow} \ket{\psi_{\alpha,\beta}}_{12}\otimes\ket{\psi_{\mu,\nu}}_{34} =\sum_{ml}^{d-1} \omega^{-\mu(m+\beta)}\omega^{l\beta}\ket{\psi_{l,m}}_{23}\ket{\psi_{\alpha+\mu-l,m+\beta+\nu}}_{14}$$
Considerando la operación unitaria:
$$ U(l,\beta)=e^{\frac{i2\pi}{d}l\beta}(Z_1)^{-l}, Z_1=\sum_{n=0}^{d-1}\omega^n\ket{n}_1\bra{n}$$
$$\textcolor{red}{\Rightarrow} U(l,\beta)\ket{\psi_{jk}}_{14}=\omega^{l\beta}\ket{\psi_{j-l,k}}_{14}$$
Se obtiene que 
$$\ket{\psi_{\alpha,\beta}}_{12}\otimes\ket{\psi_{\mu,\nu}}_{34}=\frac{1}{d}\sum_{l,m=0}^{d-1}e^{\frac{-i2\pi }{d}\mu(m+\beta)}\ket{\psi_{l,m}}_{23}\otimes U(l,\beta)\ket{\psi_{\alpha+\mu,\beta+m+\nu}}_{14}$$
\subsection{Ejemplo de Discordia Cuántica} Para el sistema AB $$\rho_{AB}=\frac{1}{2}(\ket{00}\bra{00}+\ket{11}\bra{11})+\frac{z}{2}(\ket{00}\bra{11}+\ket{11}\bra{00})=\begin{pmatrix}\frac{1}{2}&0&0&\frac{z}{2}\\0&0&0&0\\0&0&0&0\\ \frac{z}{2}&0&0&\frac{1}{2}\end{pmatrix}$$\begin{enumerate}
    \item A y B están inicialmente en $$\rho_A=Tr_B\rho_{AB}=\frac{1}{2}\mathbb{I}, \rho_B=Tr_A\rho_{AB}=\frac{1}{2}\mathbb{I}$$ Ambas matrices densidad tienen entropía de Von Neumann 1.
    \item La información mutua cuántica se obtiene de manera similar a lo que se hizo en la tarea anterior, calculando la entropía de Von Neumann a las matrices reducidas y luego a la matriz densidad completa. Para las reducidaas es trivial dada su forma:(diagonales con 2 términos que valen $\frac{1}{2}$, por lo que la entropía valdrá 1). Para la matriz completa:
    $$\rho_{AB}=\begin{pmatrix}\frac{1}{2}&0&0&\frac{z}{2}\\0&0&0&0\\0&0&0&0\\  \frac{z}{2}&0&0&\frac{1}{2}\end{pmatrix} \textcolor{red}{\Rightarrow} \lambda_1=0,\lambda_2=0,\lambda_3=\frac{1+z}{2},\lambda_4=\frac{1-z}{2}$$ $$\textcolor{red}{\Rightarrow} S(AB)=-(\frac{1+z}{2})log(\frac{1+z}{2})-(\frac{1-z}{2})log(\frac{1-z}{2})$$ $$I(\rho_{AB})=S(A)+S(B)-S(AB)=2-S(AB)$$
    $$ =2+(\frac{1+z}{2})log(\frac{1+z}{2})+(\frac{1-z}{2})log(\frac{1-z}{2})$$ Cuando z vale 0 la información mutua es 1 (estado separable, solo correlaciones clásicas) y cuando z vale 1 es 2 (estado maximalmente entrelazado, 1 de correlación clásica y otro de quantum discord).  
    \item Las correlaciones clásicas se obtienen de hacer una medida proyectiva a la base del sistema: 
    $$\ket{0^\prime}=cos\theta\ket{0}+e^{i\epsilon}sin\theta\ket{1},\ket{1^\prime}=-e^{-i\epsilon}sin\theta\ket{0}+cos\theta\ket{1}$$
    Se obtiene la matriz resultante de la medición proyectiva en B para obtener sus autovalores
    $$\Tilde{\rho}_{A,0^\prime}=\frac{1}{2}(\ket{0}_A\bra{0}\lvert\bra{0^\prime}\ket{0}\rvert^2+\ket{1}_A\bra{1}\lvert\bra{0^\prime}\ket{1}\rvert^2)$$ $$+\frac{z}{2}(\ket{0}_A\bra{1}\bra{0^\prime}\ket{0}\bra{1}\ket{0^\prime}+\ket{1}_A\bra{0}\bra{0^\prime}\ket{1}\bra{0^\prime}\ket{0})$$
       $$=\frac{1}{2}(\ket{0}_A\bra{0}cos^2\beta+\ket{1}_A\bra{1}sin^2\beta)$$ $$+\frac{z}{2}(\ket{0}_A\bra{1}e^{i\epsilon}cos\beta sin\beta+\ket{1}_A\bra{0}e^{-i\epsilon}sin\beta cos\beta)$$
    $$\Tilde{\rho}_{A,1^\prime}=\frac{1}{2}(\ket{0}_A\bra{0}\lvert\bra{1^\prime}\ket{0}\rvert^2+\ket{1}_A\bra{1}\lvert\bra{1^\prime}\ket{1}\rvert^2)$$ $$+\frac{z}{2}(\ket{0}_A\bra{1}\bra{1^\prime}\ket{0}\bra{1}\ket{1^\prime}+\ket{1}_A\bra{0}\bra{1^\prime}\ket{1}\bra{1^\prime}\ket{0})$$
    $$=\frac{1}{2}(\ket{0}_A\bra{0}sin^2\beta+\ket{1}_A\bra{1}cos^2\beta)$$ $$-\frac{z}{2}(\ket{0}_A\bra{1}e^{i\epsilon}cos\beta sin\beta+\ket{1}_A\bra{0}e^{i\epsilon}cos\beta sin\beta)$$
    Entonces, tanto al medir 0, como al medir 1, obtendremos los mismos autovalores, que se obtienen de la matriz normalizada: 
    $$ \rho_{A,0^\prime}= \begin{pmatrix} cos^2 \beta& zsin\beta cos\beta e^{i\epsilon} \\ z sin\beta cos\beta e^{-i\epsilon} & sin^2\beta\end{pmatrix}\textcolor{red}{\Rightarrow} \lambda_\pm=\frac{1\pm\sqrt{1-4\lvert\rho^\prime_A\rvert}}{2}$$
    Siendo el determinante de la matriz densidad:
    $$ \lvert\rho^\prime_A\rvert=cos^2\beta sin^2\beta-z^2sin^2\beta cos^2\beta=(1-z^2)cos^2\beta sin^2\beta$$ Simplificando los valores propios: $$ \sqrt{1-4(1-z)cos^2\beta sin^2\beta}=\sqrt{(z^2-1)sin^22\beta+1}$$
    Si z vale 0, los valores propios son $cos\theta$ y $sin\theta$, si vale 1, valdrá 1 y 0. Las entropías valen: $$S(\rho_{A,0^\prime})=-\frac{1+\sqrt{(z^2-1)sin^22\beta+1}}{2}log(\frac{1+\sqrt{(z^2-1)sin^22\beta+1}}{2})$$ $$-\frac{1-\sqrt{(z^2-1)sin^22\beta+1}}{2}log(\frac{1-\sqrt{(z^2-1)sin^22\beta+1}}{2}) $$
    Entonces, considerando que ambos estados tienen probabilidad $\frac{1}{2}$, la entropía condicional vale. $$S(S|\Pi_B)=\frac{S(\rho_{A,0^\prime})+S(\rho_{A,1^\prime})}{2}$$
    Usando lo anterior, las correlaciones clásicas (base dependiente) valen: 
    $$J(A|\Pi_B)=S(A)-S(A|\Pi_B)=1-S(A|\Pi_B)$$ 
    La base de la medida proyectiva tendrá mínima entropía cuando $sin^2 2\beta=0 \textcolor{red}{\Rightarrow} 2\beta=\beta=0$, la que valdrá
    $$S(\rho_{A,0^\prime}^{max})=-\frac{1+1}{2}log(\frac{1+1}{2})-\frac{1-1}{2}log(\frac{1-1}{2})=0$$por lo que las correlaciones clásicas valdrán:
    $$J(A|B)=1$$
    \item Restando la información mutua y la correlación clásica, el quantum discord vale $$D(A|B)=I(A|B)-J(A|B)=1+(\frac{1+z}{2})log(\frac{1+z}{2})+(\frac{1-z}{2})log(\frac{1-z}{2})$$ 
    \item El estado final conjunto es luego de una medida proyectiva en B $$\rho^\prime_{AB}=\sum_b p_b \ket{b}\bra{b}\rho_a^b=\frac{1}{2}(\begin{pmatrix}cos^2\beta&zsin\beta cos\beta e^{i\epsilon}\\zsin\beta cos\beta e {-i\epsilon}&sin^2\beta\end{pmatrix}_A\otimes\ket{0^\prime}_B\bra{0^\prime}$$ $$+\begin{pmatrix}sin^2\beta&-zsin\beta cos\beta e^{i\epsilon}\\-zsin\beta cos\beta e^{-i\epsilon}&cos^2\beta\end{pmatrix}_A\otimes\ket{1^\prime}_B\bra{1^\prime})$$
    \item A y B terminan en
    $$\rho_A^\prime=Tr_B\rho_{AB}^\prime=\frac{1}{2}(\begin{pmatrix}sin^2\beta+cos^2\beta&(z-z)sin\beta cos\beta e^{i\epsilon}\\(z-z)sin\beta cos\beta e^{-i\epsilon}&cos^2\beta+sin^2\beta\end{pmatrix})=\frac{\mathbb{I}_A}{2}=\rho_A$$ $$\rho_B^\prime=Tr_A\rho_{AB}^\prime=\frac{1}{2}(\ket{0^\prime}\bra{0^\prime}(cos^2\beta+sin^2\beta)+\ket{1^\prime}\bra{1^\prime}(sin^2\beta+cos^2\beta))=\frac{\mathbb{I}_B^\prime}{2}$$ 
    \item   \begin{figure}[ht][h]
    \includegraphics[width=8cm]{tarea.png} 
    \end{figure} Se grafica el quantum discord y el entrelazamiento del estado $\rho_{AB}$ respecto a z usando gnuplot. Se observa que tanto el quantum discord (las correlaciones cuánticas en la información mutua del sistema) como el entrelazamiento de formación valen 0 cuando z vale z y 1 cuando 1 vale 1, aunque el quantum discord en el intervalo termina siendo menor que el entrelazamiento de formación.
\end{enumerate}

\section{{Propiedades de la Entropía Cuántica}}
    \subsection{{Entropía para estados puros y mixtos}}  De acuerdo a la definición
    \begin{equation}\label{eq8.1}{S(\rho)=-tr(\rho log \rho)=-\sum_{x=0}^{d-1} \lambda_x log \lambda_x} \end{equation}
    Donde los logaritmos son en base 2 y se considera la descomposición espectral de la matriz densidad (es decir su descomposición en los autovalores y los proyectores de sus respectivos autoestados.
    \begin{equation}\label{eq8.2}{\rho=\sum_{x=0}^{d-1}\lambda_x\ket{x}\bra{x}}\end{equation}
    Si un estado es puro, la matriz densidad será solo el proyector del ket
    \begin{equation}\label{eq8.3}{\rho=\ket{i}\bra{i}\textcolor{red}{\Rightarrow} \lambda_i=1, \lambda_j=0 [\forall j\neq i, i, j \in x]}\end{equation}
    Y usando la definición de \ref{eq8.1} la entropía entonces será
    \begin{equation}\label{eq8.4}{S(\rho)=(Dim(x)-1)*(-0log0)-1*log 1=0-0=0}\end{equation}
    Para cualquier estado mixto (mezcla) se tiene que, usando \ref{eq8.1}
    \begin{equation}\label{eq8.5}{ S(\rho)=-\sum_{x=0}^{d-1}\lambda_xlog\lambda_x}\end{equation}
    Los $\lambda_x$ son los autoestados de una matriz densidad, por lo que són números reales entre 0 y 1 (lo mínimo que se le pide a una matriz densidad). Dicho esto, los logaritmos base 2 de dichos números serán negativos:
    \begin{equation}\label{eq8.6} {\lambda_x\geq 0 \wedge log(\lambda_x)\leq 0 \textcolor{red}{\Rightarrow} \lambda_x log(\lambda_x)\leq 0 \textcolor{red}{\Rightarrow} S(\rho)\geq 0}\end{equation}
    Esto último usando la definición dada en \ref{eq8.5}. Sumar negativos da negativo, y multiplicados por un signo menos da positivo.
    \subsection{{Cota superior para la entropía}} Sabiendo cómo está acotada por abajo la entropía, hay que ver cómo acota por arriba. Haciendo algo de álgebra dentro de la definición de \ref{eq8.5}
      \begin{equation}\label{eq8.7}{ S(\rho)=-\sum_{x=0}^{d-1}\lambda_x(log(1-(1-\lambda_x))\leq \sum_{x=0}^{d-1} \frac{\lambda_x}{ln 2}(1-\lambda_x)}\end{equation}
      En la última igualdad se ocupó la expansión de Taylor del logaritmo en general
          \begin{equation}\label{eq8.8}{log_2(1-x)=\frac{ln(1-x)}{ln 2}=\frac{1}{ln 2}(-x-\frac{x^2}{2}-\frac{x^3}{3}+...)}\end{equation}
   Manipulando la sumatoria  resultante de \ref{eq8.7}
   \begin{equation}\label{eq8.9}{\sum_{x=0}^{d-1}\frac{\lambda_x-\lambda_x^2}{ln 2}\leq \sum_{x=0}^{d-1}\frac{\lambda_x}{ln 2}\leq  \sum_{x=0}^{d-1}\frac{1}{ln 2}=\frac{d-1}{ln(2)}} \end{equation}
   Si se vuelve a ocupar la expansión de \ref{eq8.8}
   \begin{equation}\label{eq8.10}{ log d =log(1-(1-d)) \leq -(1-d) \textcolor{red}{\Rightarrow} d-1 \leq log d}\end{equation}
   Finalmente se acota como:
   \begin{equation}{\label{eq8.11}S(\rho)\leq \frac{d-1}{ln 2} \leq \frac{ln d}{ln 2} = log_2 d}\end{equation}
    \subsection{{Las entropías parciales de un estado compuesto puro son iguales}} En un sistema compuesto puro, usando \ref{eq8.3}, la entropía total será cero 
    \begin{equation}{\label{eq8.12} \ket{\phi}_{AB} \textcolor{red}{\Rightarrow} S(\ket{\phi}_{AB}\bra{\phi}_{AB})=0}\end{equation}
    Escribiendo el estado en bases arbritrarias de A y B
    \begin{equation}\label{eq8.13} {\ket{\phi}_{AB}=\sum_{x,y} c_{xy}\ket{x}_A\ket{y}_B=\sum_{i}\sqrt{\lambda_i}\ket{i}_A\ket{i}_B}\end{equation}
    Para lo último se usa que un estado conjunto puro se puede escribir como una descomposición de Schmidt. Entonces se escriben las matrices densidad reducida.
    \begin{equation}\label{eq8.14}{ \rho_{A}=\sum_{i,j,k} \sqrt{\lambda_i\lambda_j}\bra{k}_B\ket{i}_A\ket{i}_B\bra{j}_A\bra{j}_B \ket{k}_B}\end{equation}
    \begin{equation}\label{eq8.15}{ \rho_{B}=\sum_{i,j,k} \sqrt{\lambda_i\lambda_j}\bra{k}_A\ket{i}_A\ket{i}_B\bra{j}_A\bra{j}_B \ket{k}_A}\end{equation}
    Usando la ortogonalidad de las bases se simplifican \ref{eq8.14} y \ref{eq8.15}, obtieniéndose lo mismo en ambas para cada sistema.
    \begin{equation} \label{eq8.16}{ \forall (i,j)\bra{i}\ket{j}=\delta_{ij}\textcolor{red}{\Rightarrow} \rho_A=\sum_k\lambda_k\ket{k}_A\bra{k}_A, \rho_B=\sum_k\lambda_k\ket{k}_B\bra{k}_B}\end{equation}
    Al tener la misma forma, las matrices densidad reducida tienen los mismos autovalores. Con lo que usando la definición de \ref{eq8.1}:
    \begin{equation} \label{eq8.17}{ S(\rho_A)=S(\rho_B)}\end{equation}
    \subsection{{Entropía de una combinación probabilista de estados}} Partiendo de una matriz densidad de la forma
    \begin{equation}\label{eq8.18}{\rho=\sum_i p_i\rho_i=\sum_{i}p_i\sum_{x_i=0}^{j_i-1}\lambda_{x_i} \ket{x_i}\bra{x_i} }\end{equation}
    En la que las matrices densidad $\rho_i$ tienen soportes en espacios ortogonales, por lo que se comportan como proyectores ortogonales
    \begin{equation}\label{eq8.19}{ Tr(\rho_i\rho_j)=\delta_{ij} }\end{equation}
    Usando la definición en \ref{eq8.1}, se calcula la entropía (considerando $\lambda_{x_i}$ el autoestado $x_i$ de $\rho_i$
    \begin{equation}\label{eq8.20}{ S(\rho)=-\sum_{i}\sum_{x_i=0}^{j_i-1}(p_i\lambda_{x_i})log(p_i\lambda_{x_i})=-\sum_{i}\sum_{x_i=0}^{j_i-1}(p_i\lambda_{x_i})(log(p_i)+log(\lambda_{x_i}) }\end{equation}
    En lo último se ocupó la propiedad de logaritmos de productos.Para el primer sumando
    \begin{equation}\label{eq8.21}{ -\sum_i\sum_{x_i=0}^{j_i-1}p_i\lambda_{x_i}log(p_i)=-\sum_ip_ilogp_i\sum_{x_i=0}^{j_1-1}\lambda_{x_i}=-\sum_ip_i logp_i=H(p_i)}\end{equation}
    En la última parte se ocupa que los $\lambda_{x_i}$ son autovalores de matrices densidad $\rho_i$, por lo que sumarlas siempre dará uno. Se termina obteniendo la entropía clásica para la distribución de probabilidad $p_i$ Mientras que para el segundo sumando
    \begin{equation}\label{eq8.22}{-\sum_i\sum_{x_i=0}^{j_i-1}p_i\lambda_{x_i}log(\lambda_{x_i})=-\sum_i p_i\sum_{x_i=0}^{j_i-1}\lambda_{x_i}log(\lambda_{x_i})=\sum_i p_i S(\rho_i)}\end{equation}
    Reordenando se obtienen las entropías para cada $\rho_i$ Con lo que finalmente se obtiene
    \begin{equation}\label{eq8.23}{\rho=\sum_i p_i\rho_i \textcolor{red}{\Rightarrow} S(\rho)=H(p_i)+\sum_i p_i S(\rho_i)}\end{equation}
    \subsection{{Entopía de un estado Clásico-Cuántico}} Si ahora se tiene una matriz densidad de la forma
    \begin{equation}\label{eq8.24}{\rho=\sum_i p_i \ket{i}\bra{i}\otimes \rho_i=\sum_{i}p_i\ket{i}\bra{i}\sum_{x_i=0}^{j_i-1}\lambda_{x_i} \ket{x_i}\bra{x_i}}\end{equation}
    Si se calcula la entropía usando la definición de \ref{eq8.1}
         \begin{equation}\label{eq8.25}{ S(\rho)=-\sum_{i}\sum_{x_i=0}^{j_i-1}(p_i\lambda_{x_i})log(p_i\lambda_{x_i})) }\end{equation}
    se obtiene la misma fórmula que para la situación anterior, por lo que el resultado final también será el mismo.
    \begin{equation}\label{eq8.26}{\rho=\sum_i p_i \ket{i}\bra{i}\otimes \rho_i \textcolor{red}{\Rightarrow} S(\rho)=H(p_i)+\sum_i p_i S(\rho_i)}\end{equation}

    \section{{Entropía de un Estado Producto}} Para un sistema producto de 2 matrices densidad:
    \begin{equation}\label{eq8.27}{ \rho=\sum_x p_x \ket{x}\bra{x}, \sigma=\sum_y q_y \ket{y}\bra{y}}\end{equation}
    La entropía para el sistema producto, de acuerdo a la definición en \ref{eq2.1} es 
    \begin{equation}\label{eq8.28}{S(\rho\otimes\sigma)=-\sum_{x,y}(p_xq_y)log(p_xq_y)=-\sum_{x,y}(p_xq_y)log(p_x)-\sum_{x,y}(p_xq_y)log(q_y)}\end{equation}
    En lo último se ocupó la propiedad de logaritmos de productos. Para el primer sumando
    \begin{equation}\label{eq8.29}{ -\sum_{x,y}(p_xq_y)log(p_x)=\sum_x p_x log (p_x) \sum_y q_y= S(\rho)}\end{equation}
    Mientras que para el segundo sumando
    \begin{equation}\label{eq8.30}{ -\sum_{x,y}(p_xq_y)log(q_y)=\sum_x p_x\sum_y q_ylog(q_y)=S(\sigma)}\end{equation}
    Con lo que sumando \ref{eq8.29} y \ref{eq8.30} resulta
    \begin{equation}\label{eq8.31}{S(\rho\otimes\sigma)=S(\rho)+S(\sigma)}\end{equation}
\section{{Entrelazamiento de Sistemas Compuestos Puros}} Si el estado conjunto es puro, su entropía total es 0. Usando la definición de Información Mútua cuántica
\begin{equation}\label{eq8.32}{ S(A,B) =0 \textcolor{red}{\Rightarrow} I(A:B)=S(B)-S(B|A)}\end{equation}
Y usando también la definición de Entropía condicional:
\begin{equation}\label{eq8.33}{S(B|A)=S(AB)-S(B)=-S(B) \textcolor{red}{\Rightarrow} I(A:B)= S(A)+S(B)= 2S(B)}\end{equation}
En lo último se ocuparon las 2 propiedades de entropías de estado puro: Que la entropía conjunta es 0 (\ref{eq8.12}) y que las entropías en los subsistemas (si es bipartito), serán iguales (\ref{eq8.17}). Para que la información mutua sea mayor que 0 (es 0 si es estado separable y en cualquier otra situación estará entrelazado) $S(B)$ tiene que ser positivo, lo que de acuerdo a lo evaluado en \ref{eq2.33}, implica que $S(B|A)$ tiene que ser negativa.
\section{{Información Mútua como Entropía Relativa}}
    Sean 2 matrices densidad descompuestas espectralmente con valores propios $\alpha_i$ y $\beta_j$
    \begin{equation}\label{eq8.34}{\rho_A=\sum_i^N \alpha_i\ket{i}_A\bra{i}_A, \rho_B=\sum_j^M\beta_j\ket{j}\bra{j}}\end{equation}
    El logaritmo para la matriz producto será una matriz escrita en las bases de los autovalores para ambos espacios cuyos argumentos en la diagonal serán los logaritmos de los elementos de las matriz producto, que son todos los posibles productos entre los autovalores de las 2 matrices:
    \begin{equation}\label{eq8.35}{log(\rho_A\otimes\rho_B)=diag(log(\alpha_1\beta_1),...,log(\alpha_N\beta_1),...,log(\alpha_1\beta_M),...,log(\alpha_N\beta_M))}\end{equation}
    Usando la propiedad de que el logaritmo de un producto es la suma de los logaritmos de los factores se puede separar la matriz en 2, una solo con logaritmos de elementos de $\rho_A$ y otra solo con logaritmos de elementos de $\rho_B$
    \begin{equation}\label{eq8.36}{ log(\rho_A\otimes\rho_B)=\begin{pmatrix}log(A)&0&0\\0&...&0\\0&0&log(A)\end{pmatrix}+\begin{pmatrix}log(\beta_1)\mathbb{I}&0&0\\ 0&...&0 \\0&0&log(\beta_M)\mathbb{I}\end{pmatrix}}\end{equation}
    Ambas matrices pueden escribirse como productos tensoriales entre las densidades reducidad y la identidad del otro espacio. Por lo que finalmente se puede escribir:
    \begin{equation}\label{eq8.37}{log(\rho_A)\otimes\mathbb{I}_B+\mathbb{I}_A\otimes log(\rho_B)}\end{equation}
\section{{Entropia condicional como Entropía}}
De acuerdo a la definición de entropía relativa:
    \begin{equation}\label{eq8.38}{ S(\rho||\sigma)=tr(\rho log\rho)-tr(\rho log \sigma)}\end{equation}
    Se puede escribir lo pedido como
    \begin{equation}\label{eq8.39}{ S(\rho^{AB}||\rho^A\otimes\rho^B)=tr(\rho^{AB}log\rho^{AB})-tr(\rho^{AB}log(\rho^A\otimes\rho^B))}\end{equation}
    Por la definición en \ref{eq8.1}, el primer sumando es $-S(\rho^{AB})$. Para el segundo sumando se tiene que, usando lo probado en \ref{eq8.37} \begin{equation} \label{eq8.40}{ -tr(\rho^{AB}log(\rho^A\otimes\rho^B))=-tr(\rho^{AB}log(\rho_A)\otimes\mathbb{I}_B)-tr(\rho^{AB}\mathbb{I}_A\otimes log(\rho_B))}\end{equation}
    Aplicando para el primer sumando resultante la traza primero en B y luego en A, se obtiene la entropía para A
    \begin{equation}\label{eq8.41}{ -tr_A(tr_B(\rho^{AB}log(\rho_A)\otimes\mathbb{I}_B))=-tr_A(\rho^Alog(\rho^A))=S(\rho^A)}\end{equation}
    Y de manera análoga para el segundo sumando, se traza primero en A y luego en B, obtieniéndose la entropía para B
    \begin{equation}\label{eq8.42}{ -tr_B(tr_A(\rho^{AB}\mathbb{I}_A\otimes log(\rho_B)))=-tr_B(\rho^Blog(\rho^B))=S(\rho^B)}\end{equation}
    Por lo que, sumando los 3 términos y usando la definición de información mútua se obtiene:
    \begin{equation}\label{eq8.43}{ S(\rho^{AB}||\rho^A\otimes\rho^B)=-S(\rho^{AB})+S(\rho^A)+S(\rho^B)=I(A:B)}\end{equation}
    \section{{Entropía de ensemble Clásico Cuántico}} Partiendo de \ref{eq8.38}, se puede escribir lo pedido como:
    \begin{equation}\label{eq8.44}{S(\rho^{AB}||\mathbb{I}^A\otimes\rho^B)=tr(\rho^{AB} log\rho^{AB})-tr(\rho^{AB}log(\mathbb{I}^A\otimes\rho^B))\rho}\end{equation}
    Al igual que en el ejercicio anterior, el primer sumando es $-S(\rho^{AB})$. Usando lo calculado en \ref{eq8.42}, el segundo sumando es $S(\rho^B)$. Por lo que, sumando los 2 términos y usan la definición de entropía condicional:
    \begin{equation}\label{eq8.45}{S(\rho^{AB}||\mathbb{I}^A\otimes\rho^B)=-S(\rho^{AB})+S(\rho^B)=-S(A|B)}\end{equation}
    \section{{La entropía es invariante bajo transformaciones unitarias.}}Escribiendo las matrices $\rho$ y $\sigma$ en su forma espectral
    \begin{equation}\label{eq8.46}{\rho=\sum_i \alpha_i\ket{i}\bra{i}, \sigma=\sum_j \beta_j\ket{j}\bra{j}}\end{equation}
    Usando la definición de entropía relativa expresada en \ref{eq8.38}
    \begin{equation} \begin{aligned}\label{eq8.47}{S(\rho||\sigma)=\sum_i \alpha_i log \alpha_i - \sum_i\bra{i}\rho log \sigma \ket{i}} \\ { =\sum_{i}\alpha_i log \alpha_i-\sum_{ij}\alpha_i log \beta_j \lvert\bra{i}\ket{j}\rvert^2} \end{aligned} \end{equation}
    Se define una constante cuyo valor serán los productos escalares entre los autoestados, la que permite simplificar la notación:
    \begin{equation}\label{eq8.48}{P_{ij}=\lvert\bra{i}\ket{j}\rvert^2 \textcolor{red}{\Rightarrow} S(\rho||\sigma)=\sum_i \alpha_i log \alpha_i -\sum_ij \alpha_i log \beta_j P_{ij}}\end{equation}
    Si se multiplica ambas matrices densidad por la misma operación unitaria, sus descomposiciones espectrales quedan:
    \begin{equation}\label{eq8.49}{u\rho u^\dag=\sum_i \alpha_i u\ket{i}\bra{i}u^\dag, u\sigma u^\dag=\sum_j \beta_j u\ket{j}\bra{j}u^\dag}\end{equation}
    Resultan los mismos autovalores y un cambio de base para los autoestados. Escribiendo su respectiva entropía relativa:
    \begin{equation}\label{eq8.50}{ S(u\rho u^\dag||u\sigma u^\dag)=\sum_i \alpha_i log \alpha_i - \sum_i\bra{i} U^\dag (u\rho u^\dag) log (u\sigma u^\dag) U\ket{i}}\end{equation}
    El primer sumando es más simple dado que aparecen los mismos autovalores y por enede la definición de entropía es la misma. El segundo sumando de \ref{eq2.50} requerirá más revisión. Llamando a los autoestados resultantes de aplicar la transformación unitaria:
    \begin{equation}\label{eq8.50a}{ \ket{u_i}= u\ket{i} \textcolor{red}{\Rightarrow} \bra{u_i}=\ket{i}U^\dag}\end{equation}
     \begin{equation}\label{eq8.50b}{ \ket{v_j}= u\ket{j} \textcolor{red}{\Rightarrow} \bra{v_j}=\ket{j}U^\dag}\end{equation}
     Se pueden reescribir las definiciones de \ref{eq8.49} como
     \begin{equation}\label{eq8.50c}{u\rho u^\dag=\sum_i \alpha_i \ket{u_i}\bra{u_i}, u\sigma u^\dag=\sum_j \beta_j \ket{v_j}\bra{v_j}}\end{equation}
     con lo que la definición de \ref{eq8.50} se simplifica
     \begin{equation}\label{eq8.50d}{S(u\rho u^\dag||u\sigma u^\dag)=\sum_i \alpha_i log \alpha_i - \sum_i\bra{u_i} (u\rho u^\dag) log (u\sigma u^\dag) \ket{u_i} }\end{equation}
     Y reescribiendo los elementos del segundo sumando de \ref{eq8.50d}, agregando la descomposición espectral de las matrices densidad: 
     \begin{equation}\label{eq8.50e}{  \sum_i\bra{u_i} (u\rho u^\dag) log (u\sigma u^\dag)=\sum_i\bra{u_i}(\sum_k \alpha_k \ket{u_k}\bra{u_k})log(\sum_j \beta_j \ket{v_j}\bra{v_j})\ket{u_i}}\end{equation}
     Al ser descomposición espectral, se pueden juntar las sumas y el logaritmo de manera conveniente para lo anterior:
     \begin{equation}\label{eq8.50f}{=\sum_{ijk}\alpha_k\bra{u_i}\ket{u_k}\bra{u_k}\ket{v_j} log(\beta_j)\bra{v_j}\ket{u_i}}\end{equation}
     Por la ortogonalidad de los autoestados y por la definición en \ref{eq8.48} \begin{equation}{ \label{eq8.50g} \bra{u_i}\ket{u_k}=\delta_{ik}, \bra{u_k}\ket{v_j}=\bra{k}U^\dag U \ket{j} = \bra{k}\ket{j}\textcolor{red}{\Rightarrow} \bra{u_i}\ket{v_j}\bra{v_j}\ket{u_i}=P_ij}\end{equation}
     Se insertan ambos en la suma de \ref{eq8.50f}, la que se simplifica
    \begin{equation}\label{eq8.50h}{=\sum_{ij}\alpha_i log\beta_j P_{ij}}\end{equation} 
    con lo que se recupera el resultado de \ref{eq8.48}, lo que prueba que 
    \begin{equation}\label{eq8.51}{S(U\rho U^\dag||U\sigma U^\dag)=S(\rho||\sigma)}\end{equation}
\section{{Entropía de estados producto 1}} Partiendo de la definición en \ref{eq8.38}, lo pedido se escribe como
\begin{equation}\label{eq8.52}{S(\rho_1\otimes\rho_2||\sigma_1\otimes\sigma_2)=-S(\rho_1\otimes\rho_2)-tr((\rho_1\otimes\rho_2)log(\sigma_1\otimes\sigma_2))}\end{equation}
Usando la propiedad del logaritmo demostrada en \ref{eq8.37}
    \begin{equation}\label{eq8.53}{ =-S(\rho_1\otimes\rho_2)-tr(\rho_1\otimes\rho_2(log(\sigma_1)\otimes \mathbb{I}_2+\mathbb{I}_1\otimes log(\sigma_2))}\end{equation}
    Calculando análogamente a \ref{eq8.41} y \ref{eq8.42}, es decir, trazando parcialmente en 1 y 2, además de usar lo demostrado en \ref{eq8.32} se obtiene:
    \begin{equation}\label{eq8.54}{=-S(\rho_1)-S(\rho_2)-tr(\rho_1 log\sigma_1)-tr(\rho_2 log\sigma_2)}\end{equation}
    Que, usando la definición de \ref{eq8.38}, no es más que:
    \begin{equation}\label{eq8.55}{S(\rho_1\otimes\rho_2||\sigma_1\otimes\sigma_2)=S(\rho_1||\sigma_1)+S(\rho_2||\sigma_2)}\end{equation}
    \section{{Entropía de estados producto 2}} Usando la propiedad demostrada en \ref{eq8.55}, se encuentra rápidamente que:
    \begin{equation} \label{eq8.56}{S(\rho^{\otimes n}||\sigma^{\otimes n})=\sum_nS(\rho||\sigma)=nS(\rho||\sigma)}\end{equation}
    \section{{Entropía de 2 ensembles Clásico-Cuántico}} Se comienza usando la definición de \ref{eq8.38} para 2 estados clásico-cuántico
    \begin{equation}\label{eq8.57}\begin{aligned}{ S(\rho^{XB}||\sigma^{XB})=S(\sum_i p_i \ket{i}\bra{i}\otimes \rho_i || \sum_i p_i \ket{i}\bra{i}\otimes \sigma_i)} \\{ =-S(\rho^{XB})-tr(\rho^{XB}log(\sigma^{XB}))}\end{aligned} \end{equation}
    Esto se puede descomponer en las componentes de las matrices $\rho_x$ y $\sigma_x$ usando definiciones parecidas a las que se encuentran en \ref{eq8.18}
    \begin{equation}\label{eq8.58}{\rho_i=\sum_{x_i=0}^{j_1-1}\lambda_{x_i}^\rho \ket{x_i^\rho}\bra{x_i^\rho}, \sigma_i=\sum_{x_i=0}^{j_1-1}\lambda_{x_i}^\sigma \ket{x_i^\sigma}\bra{x_i^\sigma}}\end{equation}
    Con lo que lo obtenido en \ref{eq8.57} se puede escribir, usando \ref{eq8.26} para el primer sumando
    \begin{equation}\label{eq8.59}{-S(\rho^{XB})=-H(p_i)-\sum_i p_iS(\rho_i)}\end{equation}
    Y usando la propiedad de producto de los logaritmos en el segundo sumando:
    \begin{equation}\label{eq8.60}{-tr(\rho^{XB}log(\sigma^{XB}))=-\sum_i\sum_{x_i=0}^{j_i-1} p_i\lambda_{x_i}^\rho log (p_i)-\sum_i\sum_{x_i=0}^{j_i-1} p_i \lambda_{x_i}^\rho log(\lambda_{x_i}^\sigma)}\end{equation}
    Para el primer sumando de \ref{eq8.60} se puede usar que solo existe la dependencia de los $x_i$ en los autovalores $\lambda_{x_i^\rho}$ (que además suman 1) para decir que 
    \begin{equation}\label{eq8.61}\begin{aligned}{-\sum_i\sum_{x_i=0}^{j_i-1} p_i\lambda_{x_i}^\rho log (p_i)=-\sum_{x_i=0}^{j_i-1} \lambda_{x_i}^\rho \sum_ip_i log (p_i)}\\{ =- \sum_i p_i log(p_i)=H(p_i)}\end{aligned}\end{equation}
    Mientras que para el segundo sumando de \ref{eq8.60}, aprovechando que tanto $\rho_i$ como $\sigma_i$ fueron preparados para el ensemble clásico cuántico con la misma probabilidad $p_i$ para cualquier $i$
    \begin{equation}\label{eq8.62}{-\sum_i\sum_{x_i=0}^{j_i-1} p_i \lambda_{x_i}^\rho log(\lambda_{x_i}^\sigma)=-\sum_ip_i Tr(\rho_ilog(\sigma_i))}\end{equation}
    Entonces, sumando los resultados de \ref{eq8.61} y \ref{eq8.62} se reescribe \ref{eq8.60} como
    \begin{equation}\label{eq8.63}{-tr(\rho^{XB}log(\sigma^{XB})=H(p_i)-\sum_i p_i Tr(\rho_ilog(\sigma_i))}\end{equation}
    Y sumando \ref{eq8.63} con \ref{eq8.59} se puede reescribir lo obtenido en \ref{eq8.57}
    \begin{equation}\label{eq8.64}{S(\rho^{XB}||\sigma^{XB})=-H(p_i)-\sum_i p_iS(\rho_i)+H(p_i)-\sum_i p_i Tr(\rho_ilog \sigma_i)}\end{equation}
    Se observa que las entropías de Shannon se cancelan y finalmente se llega a:
    \begin{equation}\label{eq8.65}{S(\rho^{XB}||\sigma^{XB})=\sum_i p_i (-S(\rho_i)-Tr(\rho_ilog\sigma_i)=\sum_i p_iS(\rho_i||\sigma_i)}\end{equation}
\section{{Las medidas proyectivas suben la entropía}}
\begin{equation}\label{eq8.66}{\rho\textcolor{red}{\Rightarrow}\rho^\prime=\sum_i P_i \rho P_i}\end{equation}
De acuerdo a la desigualdad de Klein:
\begin{equation}\label{eq8.67}{S(\rho ||\rho^\prime)=Tr(\rho log\rho)-Tr(\rho log \rho^\prime) \geq 0}\end{equation}
Separando los elementos de la resta se tiene 
\begin{equation}\label{eq8.68}{-S(\rho)\geq Tr(\rho log \rho^\prime)}\end{equation}
Y multiplicando todo por $-1$
\begin{equation}\label{eq8.69}{\textcolor{red}{\Rightarrow} S(\rho)\leq -Tr(\rho log \rho^\prime)}\end{equation}
Para el segundo término en la desigualdad de \ref{eq8.69}, se puede usar un \textit{1 conveniente} hecho de operadores de la medida al cuadrado 
\begin{equation}\label{eq8.70}{ -Tr(\rho log \rho^\prime)=-Tr(\sum_i P_i^2 \rho log \rho^\prime)=-Tr(\sum_i P_i \rho log \rho^\prime P_i) }\end{equation}
Esto último debido a que la traza es cíclica. Si se combina los operadores de proyección con la matriz densidad luego de medir
\begin{equation}\label{eq8.71}{ P_i\rho^\prime-\rho^\prime P_i=P_i(P_i\rho P_i)-(P_i\rho P_i)P_i=(P_i\rho P_i)-(P_i\rho P_i)=0}\end{equation}
se observa que estos conmutan (se ocupa que en matrices proyección $P_i^2=P_i$). Lo que implica que en lo último de $\ref{eq8.67}$ se puede cambiar de lugar a $P_i$ de después del logaritmo a antes de él.
\begin{equation}\label{eq8.72}{-Tr(\sum_i P_i \rho log \rho^\prime P_i)=-Tr(\sum_i P_i \rho P_i log\rho^\prime )=S(\rho^\prime)}\end{equation}
Lo que implica, usando \ref{eq8.72} en \ref{eq8.70}
\begin{equation}\label{eq8.73}{-Tr(\rho log \rho^\prime)=S(\rho^\prime)}\end{equation}
Y, por lo tanto, la desigualdad \ref{eq8.69} se convierte en 
\begin{equation}\label{eq8.74}{S(\rho)\leq S(\rho^\prime)}\end{equation}
Esto confirma lo pedido. Pareciera paradójico que una medida proyectiva disminuya la entropía, pero si se considera un reservorio para el sistema que se está analizando es posible considerar aumentos de la entropía en el ambiente que la mantienen en su tendencia hacia el crecimiento. 
\section{{Los POVM pueden bajar la entropía.}} De acuerdo al enunciado, los operadores de POVM para un estado de dimensión 2:
\begin{equation}\label{eq8.75}{M_1=\ket{0}\bra{0}, M_2=\ket{0}\bra{1}}\end{equation}
Se observa que pueden ser un POVM porque la suma de los operadores ponderados es la identidad:
\begin{equation}\label{eq8.76}{M_1^\dag M_1+M_2^\dag M_2=\ket{0}\bra{0}\ket{0}\bra{0}+\ket{1}\bra{0}\ket{0}\bra{1}=\mathbb{I}}\end{equation}
Entonces, el resultado de la medida será
\begin{equation}\label{eq8.77}{\rho^\prime=M_1\rho M_1^\dag+M_2\rho M_2^\dag=\ket{0}\bra{0}\rho\ket{0}\bra{0}+\ket{0}\bra{1}\rho\ket{1}\bra{0}
}\end{equation}
Lo que se puede escribir como elementos solo presentes en el primer elemento de la matriz densidad
\begin{equation}\label{eq8.78}{\rho^\prime=(\bra{0}\rho\ket{0}+\bra{1}\rho\ket{1})\ket{0}\bra{0}=\ket{0}\bra{0}}\end{equation}
En lo último se ocupó que ambos elementos correponden a la diagonal de $\rho$, y por lo tanto suman 1. Este estado tiene entropía 0. Entonces, hacer la medida generalizada disminuyó la entropía hacia 0, el menor valor posible.

\section{{La Distancia es invariante ante transformaciones unitarias.}}
 De acuerdo a la definición la distancia traza es
\begin{equation}{D(\rho, \sigma) =\frac{1}{2}tr(\rho-\sigma)} \end{equation}
Al igual que una matriz densidad estándar, la resta entre las dos matrices de la definición también se puede escribir en su descomposición espectral, lo que simplifica la definición de distancia traza. 
\begin{equation} {\rho-\sigma=\sum_k \mu_i \ket{k} \bra{k}  \textcolor{red}{\Rightarrow} D(\rho, \sigma) = \sum_k \lvert \mu_k \rvert^2} \end{equation}
Al considerar la descomposición espectral de una matriz densidad, el aplicarle una transformación unitaria sólo produce efecto en los autoestados, manteniendo los autovalores Iguales.  Entonces, bajo una transformación unitaria la matriz densidad tiene exactamente los mismos autovalores. 
\begin{equation}\begin{aligned} {\rho=\sum_i r_i \ket{i}\bra{i} \textcolor{red}{\Rightarrow} (U\ket{i} = \ket{u_i} ) U\rho U^\dag =\sum_i r_i \ket{u_i} \bra{u_i} } \\ {\sigma=\sum_j s_j \ket{j} \bra{j} \textcolor{red}{\Rightarrow}  (U\ket{j} =\ket{v_j} ) U \sigma U^\dag = \sum_j s_j \ket{v_j}\bra{v_j}} \end{aligned} \end{equation}
La suma de dos operaciones transformadas con la misma transformación equivale a hacer la transformación de la suma de los operadores sin la transformación
\begin{equation} {U\rho U^\dag - U\sigma U^\dag= U(\rho-\dag) U^\dag } \end{equation}
Entonces al ser la definición de distancia traza usando la descomposición espectral sólo dependiente de los  autovalores al aplicar una transformación unitaria, la distancia traza es igual. 
\begin{equation} {U(\rho-\sigma) U^\dag= \sum_k \mu_k U\ket{k} \bra{k}U^\dag \textcolor{red}{\Rightarrow} D(U\rho U^\dag, U\sigma U^\dag) =D(\rho, \sigma)} \end{equation}
\section{{Probabilidad de Discriminación de Mínimo Error y Distancia}}
La discriminación con mínimo error puede ser representada con un POVM cuyos operadores representan el proyector en el estado 1 y el proyector en el estado 2. (esto considerando la identidad como la suma de todos los proyectores en la base de un espacio).
\begin{equation} {\Pi_1+\Pi_2= \mathbb{I}} \end{equation}
Por lo tanto las probabilidades de error para ambos estados corresponderán a los casos en los que se tiene un estado pero se encuentra con el proyector del otro estado o con el proyector que lleva a que la máquina no entrega información. 
\begin{equation} \begin{aligned}{\rho_1: p_e= tr(\Pi_2 \rho 1) = tr((\mathbb{I} - \Pi_1)\rho_1) } \\ {\rho_2: p_e =tr( \Pi_1 \rho_2) =tr((\mathbb{I} - \Pi_2)\rho_2)} \end{aligned} \end{equation}
Considerando las probabilidades de preparación para los estados 1 y 2 se obtiene la probabilidad de error total
\begin{equation} {p_e= \eta_1 tr(\Pi_2\rho_1)+\eta_2 tr(\Pi_1\rho_2) } \end{equation}
Aprovechando la propiedad de , se puede reescribir lo anterior de 2 formas equivalentes:
\begin{equation} {p_e= \eta_1 tr((\mathbb{I}-\Pi_1)\rho_1)+\eta_2 tr(\Pi_1\rho_2) =\eta_1+tr(\Pi_1(\eta_2\rho_2-\eta_1\rho_1))} \end{equation}
\begin{equation} {p_e= \eta_1 tr(\Pi_2\rho_1)+\eta_2 tr((\mathbb{I}-\Pi_2)\rho_2) =\eta_2+tr(\Pi_2(\eta_1\rho_1-\eta_2\rho_2))} \end{equation}
Sumando ambos términos y dividiendo por 2 se obtiene
\begin{equation}\begin{aligned}{p_e=\frac{\eta_1+\eta_2}{2}+\frac{tr(\Pi_1(\eta_2\rho_2-\eta_1\rho_1))+tr(\Pi_2(\eta_1\rho_1-\eta_2\rho_2))}{2}}\\ {=\frac{1}{2}(1+tr(\Pi_1(\eta_2\rho_2-\eta_1\rho_1))-tr(\Pi_2(\eta_2\rho_2-\eta_1\rho_1)))}\end{aligned}\end{equation}
En el último paso se ocupó que ${\eta_1+\eta_2=1}$ y se invirtio el signo de la resta en la segunda traza. Para simplificar notación, se escribe ${\Lambda=\eta_2\rho_2-\eta_1\rho_1}$ y la definición anterior queda:
\begin{equation}{p_e=\frac{1}{2}(1+tr(\Pi_1\Lambda)-tr(\Pi_2\Lambda))}\end{equation}
La matriz $\Lambda$ (que {NO es una matriz densidad}), se puede descomponer espectralmente, obteniendo autovalores positivos y negativos (${D}$ de es la dimensión del espacio) :
\begin{equation}{\Lambda=\sum_{k=0}^D \lambda_k\ket{\phi_k}\bra{\phi_k}}\end{equation}
La expresión obtenida para la probabilidad de error se minimiza cuando se considera los elementos que \textit{restan más}. Entiéndase que se pueden escribir los operadores de medida como los que entregan los autoestados convenientes.
Si se consideran los intervalos:
\begin{equation}{\{ \lambda_0 \leq \lambda_k \leq \lambda_{d_-} : \lambda_k <0 \}}\end{equation}
\begin{equation}{\{ \lambda_{d_-} \leq \lambda_k \leq \lambda_{d_+} : \lambda_K =0 \}}\end{equation}
\begin{equation}{\{ \lambda_{d_+} \leq \lambda_k \leq \lambda_D : \lambda_k >0 \}}\end{equation}
en los que el autovalor de la matriz ${\Lambda}$ es negativo, cero y positivo respectivamente, los operadores de la medida que minimizan el error son:
\begin{equation}{\Pi_1=\sum_{k=0}^{d_-}\ket{\phi_k}\bra{\phi_k},\Pi_2=\sum_{k=d_+}^{D}\ket{\phi_k}\bra{\phi_k} }\end{equation}
Con lo que la expresión de  es:
\begin{equation}{p_e=\frac{1}{2}(1+tr(\sum_{k=0}^{d_-} \lambda_k\ket{\phi_k}\bra{\phi_k})-tr(\sum_{k=d_+}^{D} \lambda_k\ket{\phi_k}\bra{\phi_k}))}\end{equation}
Como los autoestados de la primera sumatoria son todos negativos y los de la segunda sumatoria son todos positivos, se puede reescribir como
\begin{equation}\begin{aligned}{p_e=\frac{1}{2}(1-tr(\sum_{k=0}^{d_-}\lvert\lambda_k\rvert\ket{\phi_k}\bra{\phi_k})-tr(\sum_{k=d_+}^{D} \lvert\lambda_k\rvert\ket{\phi_k}\bra{\phi_k}))=} \\ {\frac{1}{2}(1-tr(\sum_{k=0}^{D}\lvert\lambda_k\rvert\ket{\phi_k}\bra{\phi_k}))=\frac{1}{2}(1-tr(\lvert\Lambda\rvert))=\frac{1}{2}(1-tr\lvert\eta_2\rho_2-\eta_1\rho_1\rvert)}\end{aligned}\end{equation}
Usando la definición de distancia traza, se obtiene finalmente que:
\begin{equation} {p_e=\frac{1}{2}(1-tr\lvert\eta_2\rho_2-\eta_1\rho_2\rvert}) \end{equation}
\section{{Probabilidad de Discriminación para Estados Puros}}
Considerando dos estados qubit arbitrarios (sin perder generalidad, se puede elegir el primero como cero y el segundo dependiendo de un solo coeficiente.
\begin{equation} {\ket{\phi_0}=\ket{0}, \ket{\phi_1}=\alpha\ket{0}+\sqrt{1-\lvert\alpha\rvert^2}\ket{1} } \end{equation}
Los estados se pueden escribir como matriz densidad:
\begin{equation}{\rho_0=\begin{pmatrix} 1&0\\0&0\end{pmatrix}, \rho_1=\begin{pmatrix}\lvert\alpha\rvert^2&\alpha\sqrt{1-\lvert\alpha\rvert^2}\\ \alpha^*\sqrt{1-\lvert\alpha\rvert^2}&1-\lvert\alpha\rvert^2\end{pmatrix} }\end{equation}
La matriz a la cual se le calcula la traza entonces es:
\begin{equation}{\Lambda=\eta_0\rho_0-\eta_1\rho_1=\begin{pmatrix}\eta_0-\eta_1(\lvert\alpha\rvert^2)& -\eta_1(\alpha\sqrt{1-\lvert\alpha\rvert^2})\\ -\eta_1(\alpha^*\sqrt{1-\lvert\alpha\rvert^2})& -\eta_1(1-\lvert\alpha\rvert^2)\end{pmatrix}}\end{equation}
Calculando su distancia traza:
\begin{equation}{tr\lvert\eta_0\rho_0-\eta_1\rho_1\rvert=\lvert\eta_0-\eta_1\lvert{\alpha}\rvert^2\rvert+\lvert \eta_1 (1-\lvert\alpha^2\rvert) \rvert}\end{equation}
Entonces, se dan 3 posibles situaciones
\begin{itemize}
    \item Si ${\eta_0=\eta_1\lvert\alpha\rvert^2}$, el primer módulo sumado se anula y entonces:
    \begin{equation}{tr\lvert\eta_0\rho_0-\eta_1\rho_1\rvert= \eta_1(1-\lvert\alpha\rvert^2)=\eta_0\frac{1-\lvert\alpha\rvert^2}{\lvert\alpha\rvert^2}}\end{equation}
    Por lo que la probabilidad de error se escribe:
    \begin{equation}{p_e=\frac{1}{2}(1-\eta_0\frac{1-\lvert\alpha\rvert^2}{\lvert\alpha\rvert^2})}\end{equation}
    \item Si ${\eta_0>\eta_1\lvert\alpha\rvert^2}$, el primer módulo equivale al término sin módulo y entonces
    \begin{equation}{tr\lvert \eta_0\rho_0-\eta_1\rho_1\rvert=\eta_0-\eta_1\lvert{\alpha}\rvert^2+\eta_1 (1-\lvert\alpha^2\rvert)=\eta_0+\eta_1-2\eta_1\lvert\alpha\rvert^2}\end{equation}
        Por lo que la probabilidad de error se escribe:
    \begin{equation}{p_e=\frac{1}{2}(1-1-2\eta_1\lvert\alpha\rvert^2)}\end{equation}
    \item Si ${\eta_0<\eta_1\lvert\alpha\rvert^2}$,  el primer módulo equivale a menos el término sin módulo y entonces
    \begin{equation}{tr\lvert\eta_0\rho_0-\eta_1\rho_1\rvert= \eta_1\lvert{\alpha}\rvert^2-\eta_0+ \eta_1 (1-\lvert\alpha^2\rvert)=\eta_1-\eta_0 }\end{equation}
        Por lo que la probabilidad de error se escribe:
    \begin{equation}{p_e=\frac{1}{2}(1-\eta_1-\eta_0)}\end{equation}
\end{itemize}


\section{{El estado bipartito es purificación para la matriz densidad.}}
Escribiendo un proyector horario de la forma entregada a otro estado unitario de la misma forma
\begin{equation} {\ket{\Phi}_{RA} \bra{\Phi}=\sum_{ij}^d (\mathbb{I} _R\otimes\sqrt{\rho}_A) \ket{i}_R\ket{i}_A \ket{j}_R \ket{j}_A (\mathbb{I}_R\otimes \sqrt{rho}_A) } \end{equation}
Se escribe la operación traza en la misma base en la que se escribió la parte de la matriz densidad en R. 
\begin{equation} {Tr_R\ket{\Phi}_{RA} \bra{\Phi}=\sum_{ijk}^d \ket{k}_R (\mathbb{I} _R\otimes\sqrt{\rho}_A) \ket{i}_R\ket{i}_A \bra{j}_R \bra{j}_A (\mathbb{I}_R\otimes \sqrt{\rho}_A)\ket{k} =\sum_{ijk}^d \bra{k}\mathbb{I} \ket{i}_R\bra{j} \mathbb{I} \ket{k}_R} \end{equation}
Sin  perder generalidad se asegura que se cumple ortonormalidad lo que convierte los productos  escalares en deltas que dejan solo una sumatoria
\begin{equation} {\bra{i} \ket{j} \textcolor{red}{\Rightarrow} Tr_R\ket{\Phi}_{RA} \bra{\Phi}_{RA}=\sum_i^d \sqrt{\rho}_A \ket{i}_A\bra{i} \sqrt{\rho}} \end{equation}
Dado que la base cumple la función de completitud y la suma de todos los proyectores es la identidad se puede decir que. 
\begin{equation} {Tr_R\ket{\Phi}_{RA} \bra{\Phi}_{RA} =\sqrt{\rho_A} \sqrt{\rho_A} =\rho_A} \end{equation}
\section{{Igualdad de definiciones de Fidelidad}}
Escribiendo las matrices densidad en sus respectivas descomposiciones espectrales, se observa que se también se puede escribir la {raiz de la matriz densidad}, considerando la raíz cuadrada de los autoestados:
\begin{equation}\begin{aligned} {\rho = \sum_i r_i \ket{i} \bra{i}\textcolor{red}{\Rightarrow} \sqrt{\rho} = \sum_i \sqrt{r_i} \ket{i} \bra{i} }\\ { \sigma= \sum_j s_j \ket{j} \bra{j}\textcolor{red}{\Rightarrow}\sqrt{\sigma} = \sum_j \sqrt{s_j} \ket{j} \bra{j}}\end{aligned} \end{equation}
Con estas definiciones se puede trabajar la definición de Fidelidad enseñada en clase:
\begin{equation}{F(\rho,\sigma)=Tr\sqrt{\sqrt{\rho}\sigma\sqrt{\rho}}=Tr\sum_{ijk}\sqrt{\sqrt{r_i}}\ket{i}\bra{i}\sqrt{s_j}\ket{j}\bra{j}\sqrt{\sqrt{r_k}}\ket{k}\bra{k}}\end{equation}
Separando los elementos de los que son números de los que son vectores:
\begin{equation}{F(\rho,\sigma)=Tr(\sum_{ijk}\sqrt{\sqrt{r_i}}\sqrt{s_j}\sqrt{\sqrt{r_k}}\ket{i}\bra{i}\ket{j}\bra{j}\ket{k}\bra{k})}\end{equation}
Multiplicando los elementos en raíz, y aprovechando que la traza es cíclica, queda
\begin{equation}{F(\rho,\sigma)=Tr(\sum_{ijk}\sqrt{\sqrt{r_i}s_j\sqrt{r_k}}\ket{k}\bra{k}\ket{i}\bra{i}\ket{j}\bra{j})}\end{equation}
La matriz ${\sqrt{\rho}}$ está escrita en su descomposición espectral (los términos, ${i}$ y ${k}$ de la sumatoria), por lo que se pueden cancelar convirtiendo sus productos internos en deltas.
\begin{equation}{F(\rho,\sigma)=Tr(\sum_{ij}\sqrt{\sqrt{r_i}s_j\sqrt{r_i}}\ket{i}\bra{i}\ket{j}\bra{j})=Tr(\sum_{ij}\sqrt{r_is_j}\ket{i}\bra{i}\ket{j}\bra{j}}\end{equation}
Siendo lo anterior equivalente a:
\begin{equation}{F(\rho,\sigma)=Tr(\sum_{ij}\sqrt{r_i}\sqrt{s_j}\ket{i}\bra{i}\ket{j}\bra{j}=tr\lvert\sqrt{\rho}\sqrt{\sigma}\rvert}\end{equation}
\section{{Fidelidad de estados producto}}
Escritos un conjunto de matrices densidad en sus descomposiciones espectrales, se escriben algunos productos tensoriales usando dichas descomposiciones:
\begin{equation}{\rho_1=\sum_i p_{1i}\ket{i}\bra{i}, \rho_2=\sum_j p_{2j}\ket{j}\bra{j}}\end{equation}
\begin{equation}{\textcolor{red}{\Rightarrow}\rho_1\otimes\rho_2=\sum_{ij}p_{1i}p_{2j}\ket{i,j}\bra{i,j}}\end{equation}
\begin{equation}{\sigma_1=\sum_k q_{1k}\ket{k}\bra{k}, \sigma_2=\sum_l q_{2l}\ket{l}\bra{l}}\end{equation}
\begin{equation}{\textcolor{red}{\Rightarrow}\sigma_1\otimes\sigma2=\sum_{kl}q_{1k}q_{2l}\ket{k,l}\bra{k,l}}\end{equation}
Para las matrices producto se calcula la fidelidad usando la fidelidad del ejercicio anterior:
\begin{equation}{F(\rho_1\otimes\rho_2,\sigma_1\otimes\sigma_2)=tr\lvert\sqrt{\rho_1\otimes\rho_2}\sqrt{\sigma_1\otimes\sigma_2}}\rvert\end{equation}
Que escrita espectralmente queda
\begin{equation}{=tr(\sum_{ijkl}\sqrt{p_{1i}p_{2j}}\sqrt{q_{1k}q_{2l}}\ket{i,j}\bra{i,j}\ket{k,l}\bra{k,l})}\end{equation}
Recordando que las trazas se pueden subdividir en sus respectivos subespacios se puede escribir lo anterior como
\begin{equation}{=tr_1(tr_2(\sum_{ijkl}\sqrt{p_{1i}p_{2j}}\sqrt{q_{1k}q_{2l}}\ket{i,j}\bra{i,j}\ket{k,l}\bra{k,l}))}\end{equation}
Aplicando la traza parcial 2 sobre lo que solo depende del subespacio 2
\begin{equation}{=Tr_1(\sum_{ik}\sqrt{p_{1i}q_{1k}}\ket{i}\bra{i}\ket{k}\bra{k}*Tr_2(\sum_{jl}\sqrt{p_{1j}q_{1l}}\ket{j}\bra{j}\ket{l}\bra{l}))}\end{equation}
Todo lo que está fuera de la traza en 2 está solo en el subespacio 1, así que se puede escribir lo último como:
\begin{equation}{=Tr_1(\sum_{ik}\sqrt{p_{1i}q_{1k}}\ket{i}\bra{i}\ket{k}\bra{k})*Tr_2(\sum_{jl}\sqrt{p_{1j}q_{1l}}\ket{j}\bra{j}\ket{l}\bra{l})}\end{equation}
Lo que, usando la definición de fidelidad, entrega al fin:
\begin{equation}{=Tr_1(\lvert\sqrt{\rho_1}\sqrt{\sigma_1}\rvert)*Tr_2(\lvert\sqrt{\rho_2}\sqrt{\sigma_2}\rvert)=F(\rho_1,\sigma_1)F(\rho_2,\sigma_2)}\end{equation}
\section{Estado Clásico-Cuántico no ortogonal en B}
Si se considera 2 estados no ortogonales
\begin{equation}\label{eq2.137} \ket{\psi_0}=cos\beta\ket{0}+sin\beta\ket{1}, \ket{\psi_1}=sin\beta\ket{0}+cos\beta\ket{1}\end{equation}
Cuyo producto escalar se define como la variable $\alpha$, que indicará qué tan \textit{ortogonal} o \textit{no ortogonal} es
\begin{equation}\label{eq2.138}\bra{\psi_0}\ket{\psi_1}=2cos\beta sin \beta=\alpha\end{equation}
Se puede escribir un estado bipartito clásico-cuántico que los incluye:
\begin{equation}\label{eq2.139}\rho_{AB}=\frac{1}{2}\sum_{i=0}^1 \ket{i}\bra{i}\otimes\ket{\psi_i}\bra{\psi_i}=\frac{1}{2}(\ket{0}\bra{0}\otimes\ket{\psi_0}\bra{\psi_0}+\ket{1}\bra{1}\otimes\ket{\psi_1}\bra{\psi_1})\end{equation}
\subsection{Información Accesible al medir en base lógica}
Trazando \ref{eq2.139} se obtiene la matriz que recibiría Bob.
\begin{equation}\label{eq2.140} \rho_B=\frac{1}{2}(\ket{\psi_0}\bra{\psi_0}+\ket{\psi_1}\bra{\psi_1}) =\begin{pmatrix} \frac{1}{2} & \frac{\alpha}{2} \\ \frac{\alpha}{2} & \frac{1}{2} \end{pmatrix}\end{equation}
En esta matriz Bob realiza una medida en la base lógica, para luego calcular la información accesible producto de dicha medición:
\begin{equation}\label{eq2.141}I_{acc}=S(\rho_A)-\frac{1}{2}S(\rho_{A,0})+S(\rho_{A,1}))\end{equation}
Usando las matrices proyectoras de \ref{eq2.141} y la matriz densidad que recibe Bob expresada en \ref{eq2.140}, que es una combinación lineal de los estados no ortogonales definidos en \ref{eq2.137}, se obtiene luego de medir para cada uno de los estados:
\begin{equation}\label{eq2.142}\rho_{A,0}=cos^2 \beta \ket{0}\bra{0}+sin^2\beta\ket{1}\bra{1}\end{equation}
\begin{equation}\label{eq2.143}\rho_{A,1}=sin^2 \beta\ket{0}\bra{0}+cos^2\beta\ket{1}\bra{1}\end{equation}
Por lo que los valores de entropía serán:
\begin{equation}\label{eq2.144} S(\rho_{A,0})=S(\rho_{A,1})=-cos^2\beta log(cos^2\beta)-sin^2\beta log(sin^2\beta)\end{equation}
Y, si se traza la matriz en el sistema de Bob:
\begin{equation}\label{eq2.145}\rho_A= \frac{1}{2}(\ket{0}\bra{0}+\ket{1}\bra{1})\end{equation}
Entonces, de acuerdo a la definición
de \ref{eq2.141}\begin{equation}\label{eq2.146}I_{acc}=1+cos^2\beta log(cos^2\beta)+sin^2\beta log(sin^2\beta)\end{equation}
Con lo que la información accesible será la mitad de la correspondiente a un estado maximalmente entrelazado (o a una combinación lineal entre algunos de ellos), considerando los estados de Bob luego de medir en la base lógica, como se sugiere en el enunciado.
\subsection{Límite de Holevo}
Según el apunte de clase, para un estado mixto que es la suma ponderada (con sus respectivas probabilidades) de distintas matrices densidad: 
\begin{equation}\label{eq2.147}\rho=\sum_x p_x\rho_x \textcolor{red}{\Rightarrow} S(\rho)-\sum_x p_x S(\rho_x) \leq H(p_x)\end{equation}
Siendo la izquierda de la desigualdad llamada Límite de Holevo, y la derecha igual a 1 dado que $H(\frac{1}{2})=1$. Nótese que el estado del ejercicio anterior cumple con lo requerido para cumplir esta desigualdad, que reemplazando queda:
\begin{equation}\label{eq2.148}\chi=S(\rho)-\frac{1}{2}(S(\ket{\phi_0}\bra{\phi_0})+S(\ket{\phi_1}\bra{\phi_1}))\leq 1\end{equation}
Como los estados de la suma ponderada son puros, dichos sumandos son 0. Por lo que el límite de Holevo es simplemente la entropía de Von Neumann de la matriz completa.
\begin{equation}\label{eq2.149}\chi=S(\rho)=-\frac{1+\alpha}{2}log(\frac{1+\alpha}{2})-\frac{1-\alpha}{2}log(\frac{1+\alpha}{2})=H(\frac{1+\alpha}{2})\end{equation}
Entonces, si $\beta\in[0,\frac{\pi}{4}]$, $\alpha\in[0,\frac{1}{2}]$ y por ende el límite de Holevo cumple la desigualdad de \ref{eq2.147}, dado es toma su valor máximo, que es 1, cuando los estados son ortogonales entre sí, y toma el valor de 0 cuando el ángulo $\beta$ toma el mayor valor posible.
\subsection{Comparación gráfica entre entropía, Límite de Holevo e Información Accesible}
\begin{figure}[ht][ht]
    \includegraphics[width=0.8\textwidth]{test1.png}
    \caption{Gráfico que muestra debidamente etiquetados la información accesible, la entropía de la fuente y el límite de Holevo para el estado Clásico Cuántico del problema indicado }
\end{figure}
\section{Distintos tipos de Discord para Estado de Werner}
El estado de Werner se define como:
\begin{equation}\label{eq2.150}\rho_{AB}^W=\frac{1-p}{4}\mathbb{I}\otimes\mathbb{I}+p\ket{\Phi^-}\bra{\Phi^-}\end{equation}
\subsection{Discord de Hellinger}
El estado de Werner se puede escribir como estado Bell diagonal
\begin{equation}\label{eq2.151}\rho_{AB}^W=\frac{1-p}{4}(\ket{\Psi^+}\bra{\Psi^+}+\ket{\Psi^-}\bra{\Psi^-}+\ket{\Phi^+}\bra{\Phi^+}+\ket{\Phi^-}\bra{\Phi^-})+p\ket{\Phi^-}\bra{\Phi^-}\end{equation}
Por lo que se puede tratar como los estados del apunte, considerando
\begin{equation}\label{eq2.152}\lambda_1=\lambda_2=\lambda_3=\frac{1-p}{4}, \lambda_4=\frac{1-p}{4}+p=\frac{1+3p}{4}\end{equation}
Y así se obtienen las constantes requeridas para obtener la representación del estado de Werner en matrices de Pauli:
\begin{equation}\label{eq2.153}\begin{aligned} h=\sqrt{\lambda_1}+\sqrt{\lambda_2}+\sqrt{\lambda_3}+\sqrt{\lambda_4}=3\frac{\sqrt{1-p}}{2}+\frac{\sqrt{1+3p}}{2}\\ d_1=\sqrt{\lambda_1}-\sqrt{\lambda_2}+\sqrt{\lambda_3}-\sqrt{\lambda_4}=\frac{\sqrt{1-p}}{2}-\frac{\sqrt{1+3p}}{2}\\ d_2=-\sqrt{\lambda_1}+\sqrt{\lambda_2}+\sqrt{\lambda_3}-\sqrt{\lambda_4}=\frac{\sqrt{1-p}}{2}-\frac{\sqrt{1+3p}}{2}\\ d_3=\sqrt{\lambda_1}+\sqrt{\lambda_2}-\sqrt{\lambda_3}-\sqrt{\lambda_4}=\frac{\sqrt{1-p}}{2}-\frac{\sqrt{1+3p}}{2}\\ \textcolor{red}{\Rightarrow} \sqrt{\rho_{AB}}= \frac{1}{4}(3\frac{\sqrt{1-p}}{2}+\frac{\sqrt{1+3p}}{2})\mathbb{I}\otimes\mathbb{I})+(\frac{\sqrt{1-p}}{2}-\frac{\sqrt{1+3p}}{2})(\sigma_x\otimes\sigma_x+\sigma_y\otimes\sigma_y+\sigma_z\otimes\sigma_z))\end{aligned}\end{equation}
Por lo que en la notación requerida para calcular Discord de Hellinger, se puede escribir el estado en la forma general del apunte usando:
\begin{equation}\label{eq2.154}t_0=\frac{3\sqrt{1-p}}{4}+\frac{\sqrt{1+3p}}{4}, T=(\frac{\sqrt{1-p}}{4}-\frac{\sqrt{1+3p}}{4})\mathbb{I}\end{equation}
Como la matriz T es Diagonal, el mayor autovalor de Z es $(\frac{\sqrt{1-p}}{4}-\frac{\sqrt{1+3p}}{4})^2$. Por lo que de acuerdo a fórmula, el Discord Geométrico de Hellinger vale:
\begin{equation}\label{eq2.155}D_{He}^G=2-2\sqrt{(\frac{3\sqrt{1-p}}{4}+\frac{\sqrt{1+3p}}{4})^2+(\frac{\sqrt{1-p}}{4}-\frac{\sqrt{1+3p}}{4})^2}\end{equation}
Nótese que cuando $p=0$, el discord es $0$.
\subsection{Comparación gráfica entre Discord de Dakic, de Luo y de Hellinger}
\begin{figure}[ht][ht]
    \includegraphics[width=0.85\textwidth]{test2.png}
    \caption{Comparación de los 3 tipos de Discord obtenidos, etiquetados debidamente. Se observa que para normalizarlas a todas las las definiciones de Discord se les tuvo que multiplicar por 2. Además de tomar valores estándar: 0 cuando $p=0$ y aproximadamente 1 cuando $p=1$.}
\end{figure}
Si el Discord de Dakic y Luo, de acuerdo a enunciado, para el mismo estado de Werner es
\begin{equation}\label{eq2.156}D_A^{(2)}=\frac{p^2}{2}, D_A^{(3)}=\frac{1}{4}(1+p-\sqrt{(1-p)(1+3p})\end{equation}
\section{Distintos tipos de Discord para Estado Clásico-Cuántico no ortogonal en B}
El estado escrito en \ref{eq2.139} se puede escribir matricialmente como: 
\begin{equation}\label{eq2.157}\rho_{AB}= \frac{1}{2} \begin{pmatrix} cos^2 \beta &\frac{\alpha}{2}& 0 & 0 \\  \frac{\alpha}{2}& sin^2 \beta & 0& 0 \\ 0&0 & sin^2 \beta & \frac{\alpha}{2}\\ 0&0 & \frac{\alpha}{2} & cos^2 \beta \end{pmatrix}\end{equation}
\subsection{Discord de Dakic}
Usando lo definido en el apunte, el estado se puede escribir en la descomposición de operadores de Schmidt, agregando que 
\begin{equation}\label{eq2.158}\begin{aligned}\mathbb{I}\otimes \sigma_x=\begin{pmatrix} 0&1&0&0 \\ 1&0&0&0 \\ 0&0&0&1 \\ 0&0&1&0\end{pmatrix}\textcolor{red}{\Rightarrow} \rho_{AB}=\frac{cos^2 \beta}{4}(\mathbb{I}\otimes\mathbb{I}+\sigma_z\otimes\sigma_z)+\frac{sin^2 \beta}{4}(\mathbb{I}\otimes\mathbb{I}-\sigma_z\otimes\sigma_z)+\\ \frac{\alpha}{4}(\mathbb{I}\otimes\sigma_x)=\frac{\mathbb{I}\otimes\mathbb{I}}{4}+\frac{\alpha}{4}(\mathbb{I}\otimes\sigma_x)+\frac{cos^2\beta-sin^2\beta}{4}(\sigma_z\otimes\sigma_z)\end{aligned}\end{equation}
Siguiendo la notación del apunte este se puede escribir usando 
\begin{equation}\label{eq2.159}\vec{x}=\vec{0}, \vec{y}=\begin{pmatrix} \alpha \\ 0\\ 0\end{pmatrix}, T=diag(0,0,cos^2\beta-sin^2\beta) \end{equation}
Y al ser T es una matriz diagonal y con un solo elemento, $k_{max}=(cos^2\beta-sin^2\beta)^2$, por lo que el discord de Dakic termina siendo
\begin{equation}\label{eq2.160}D_A^{(2)}(\rho)=\frac{(cos^2\beta-sin^2\beta)^2-(cos^2\beta-sin^2\beta)^2}{4}=0\end{equation}
\subsection{Discord de Luo}
Para hallar el Discord de Luo, se considera como base para los operadores en ambos subespacios la base de matrices de Pauli, las que usarán para descomponer la matriz raiz cuadrada. Pero antes, se obtiene la diagonalización de la matriz densidad (para calcular su raíz) \begin{equation}\label{eq2.161} \rho_{AB}=\frac{1}{2}(\ket{v_0}\bra{v_0}+\ket{v_1}\bra{v_1})\end{equation}
Donde los autovectores son:
\begin{equation}\label{eq2.162}\ket{v_0}=\ket{0}\ket{\phi_0}=cos\beta\ket{00}+sin\beta\ket{01},\ket{v_1}=\ket{1}\ket{\phi_1}=sin\beta\ket{10}+cos\beta\ket{11}\end{equation}
Pór lo que la raíz cuadrada del estado total es
\begin{equation}\label{eq2.163}\sqrt{\rho_{AB}}=\frac{1}{\sqrt{2}}((\ket{v_0}\bra{v_0}+\ket{v_1}\bra{v_1})\end{equation}
Lo que equivale a la definición de \ref{eq2.160}, solo que considerando la raíz de 2 en lugar de 2. Esto tiene sentido considerando que los vectores en el sistema completo si se pueden considerar como proyectores ortogonales. 
\begin{equation}\label{eq2.164}\sqrt{\rho_{AB}}= \frac{1}{\sqrt{2}} \begin{pmatrix} cos^2 \beta &\frac{\alpha}{2}& 0 & 0 \\  \frac{\alpha}{2}& sin^2 \beta & 0& 0 \\ 0&0 & sin^2 \beta & \frac{\alpha}{2}\\ 0&0 & \frac{\alpha}{2} & cos^2 \beta \end{pmatrix}\end{equation}
Por lo que, análogamente a lo visto en \ref{eq2.158} la descomposición en matrices de Pauli de la raíz de la matriz densidad es:
\begin{equation}\label{eq2.165}\sqrt{\rho_{AB}}=\frac{\mathbb{I}\otimes\mathbb{I}}{2\sqrt{2}}+\frac{\alpha(\mathbb{I}\otimes\sigma_x)}{2\sqrt{2}}+\frac{cos^2\beta-sin^2\beta}{2\sqrt{2}}(\sigma_z\otimes\sigma_z)\end{equation}
Considerando $\mathbb{I}=\sigma_0$, se puede escribir el estado en la descomposición necesaria para obtener Discord de Luo usando una matriz $\Gamma$ con los coeficientes necesarios para representar el estado en base de Pauli para ambos subespacios con base $\frac{\sigma_i\otimes\sigma_j}{2}$:
\begin{equation}\label{eq2.166}\Gamma=\begin{pmatrix} \frac{1}{\sqrt{2}}&\frac{\alpha}{\sqrt{2}}&0&0\\0&0&0&0\\0&0&0&0\\ 0&0&0&\frac{cos^2\beta-sin^2\beta}{\sqrt{2}}\end{pmatrix}\textcolor{red}{\Rightarrow} r=\begin{pmatrix}\frac{1}{\sqrt{2}}\\\frac{\alpha}{\sqrt{2}}\\0\\0\end{pmatrix}\end{equation}
A su vez, la matriz Z y el producto cuadrado con su traspuesta es:
\begin{equation}\label{eq2.167}Z=\begin{pmatrix}0&0&0&0\\0&0&0&0\\0&0&0&\frac{cos^2\beta-sin^2\beta}{\sqrt{2}}\end{pmatrix}\textcolor{red}{\Rightarrow} ZZ^T=\begin{pmatrix}0&0&0\\0&0&0\\0&0&\frac{1-\alpha^2}{2}\end{pmatrix}\end{equation}
En lo último se ha ocupado que \begin{equation}\label{eq2.168}(cos^2\beta-sin^2\beta)^2=(cos^2\beta+sin^2\beta)^2-4cos^2\beta sin^2\beta= 1-\alpha^2\end{equation}
Entonces, el mayor (por ser el único distinto de cero) autovalor de la matriz diagonal $ZZ^T$ es $\frac{1-\alpha^2}{2}$. Y el Discord de Luo será:
\begin{equation}\label{eq2.169} D_A^{(3)}=1-(\frac{1+
\alpha^2}{2}+\frac{1-\alpha^2}{2})=0\end{equation}
\section{Estimación de un estado desconocido de un qubit}
Para el estado desconocido 
\begin{equation}\label{eq2.170}\rho(\theta)=\frac{1}{2}(\mathbb{I}+\vec{\theta}\cdot\vec{\sigma})\end{equation}
Se busca un estimador de dicho estado para el cual se calculará la Información de Fisher y su eficiencia.
\subsection{Estimador para la variable desconocida}
Usando los 6 elementos pedidos por el enunciado para un POVM, que tienen como probabilidades:
\begin{equation}\label{eq2.171}\begin{aligned}p_1=\frac{1}{6}(1+\theta_1), p_2=\frac{1}{6}(1-\theta_1)\\ p_3=\frac{1}{6}(1+\theta_2), p_4=\frac{1}{6}(1-\theta_2)\\ p_5=\frac{1}{6}(1+\theta_3), p_6=\frac{1}{6}(1-\theta_3)\end{aligned}\end{equation}
Al medir $\frac{m}{2}$ veces cada operador, el número total de partículas es
\begin{equation}\label{eq2.172}n= 6*(\frac{m}{2})\sum_{i=0}^2 (p_{2i}+p_{2i+1})\end{equation}
Lo que genera la misma estimación mostrada en el apunte, solo que con un valor de $m=\frac{n}{3}$
Por lo que la varianza correspondiente es:
\begin{equation}\label{eq2.173}Var(2\frac{m_i}{M}-1)=\frac{1-\theta_i^2}{m}=\frac{3}{n}(1-\theta_i^2)\end{equation}
Y el estimador finalmente será:
\begin{equation}\label{eq2.174}\theta_n(x)=\begin{pmatrix} \frac{n_1-n_2}{n_1+n_2} & \frac{n_3-n_4}{n_3+n_4} & \frac{n_5-n_6}{n_5+n_6}\end{pmatrix}\end{equation}
\subsection{Información de Fisher del POVM}
Para un sistema con probabilidades (como el mostrado en el apunte) se cumple que la información de Fisher es:
\begin{equation}\label{eq2.175}[I(\theta)]_{i,j}=\sum_\alpha\frac{1}{q_\alpha}\frac{\partial q_\alpha}{\partial \theta_i}\frac{\partial q_\alpha}{\partial \theta_j}\end{equation}
El sistema de este problema, hecho con las probabilidades $\{p_1,p_2,p_3,p_4,p_5,p_6\}$ tiene como elementos de matriz para la información de Fisher:
\begin{equation}\label{eq2.176}\begin{aligned} 
I(\theta)_{11}=\frac{6}{1+\theta_1}\frac{1}{6}\frac{1}{6}+\frac{6}{1-\theta_1}\frac{1}{6}\frac{1}{6}=\frac{1}{3(1-\theta_1^2)} \\ I(\theta)_{22}=\frac{6}{1+\theta_2}\frac{1}{6}\frac{1}{6}+\frac{6}{1-\theta_2}\frac{1}{6}\frac{1}{6}=\frac{1}{3(1-\theta_2^2)} \\ I(\theta)_{33}=\frac{6}{1+\theta_3}\frac{1}{6}\frac{1}{6}+\frac{6}{1-\theta_3}\frac{1}{6}\frac{1}{6}=\frac{1}{3(1-\theta_3^2)}\\ [I(\theta)]_{12}=[I(\theta)]_{21}=[I(\theta)]_{23}=[I(\theta)]_{32}=[I(\theta)]_{13}=[I(\theta)]_{31}=0 \end{aligned}\end{equation}
Por lo que finalmente la matriz con informaciones de Fisher es diagonal, y para m evaluaciones vale:
\begin{equation}\label{eq2.177}I_1(\theta)=\begin{pmatrix}\frac{1}{3(1-\theta_1^2)}&0&0\\0&\frac{1}{3(1-\theta_2^2)}&0\\0&0&\frac{1}{3(1-\theta_3^2)}\end{pmatrix}\textcolor{red}{\Rightarrow} I_n(\theta)=\begin{pmatrix}\frac{n}{3(1-\theta_1^2)}&0&0\\0&\frac{n}{3(1-\theta_2^2)}&0\\0&0&\frac{n}{3(1-\theta_3^2)}\end{pmatrix} \end{equation}
\subsection{Eficiencia del Estimador}
Un estimador es eficiente si se cumple la desigualdad Cramer-Rao:
\begin{equation} \label{eq2.178}Var(\theta)\geq\frac{1}{I_n(\theta)}\end{equation}
Reemplazando \ref{eq2.177} y \ref{eq2.173} en \ref{eq2.178} se obtiene:
\begin{equation}\label{eq2.179}\frac{3(1-\theta_i^2}{n}\geq\frac{1}{\frac{n}{3(1-\theta_i}}=\frac{3(1-\theta_i^2}{n}\end{equation}
Por lo que se cumple la desigualdad, y por lo tanto, el estimador es eficiente.
\chapter{Métodos Numéricos}
\section{{Evolución temporal con Ecuación de Schrodinger}}
\subsection{{Forma del operador evolución temporal}}
\subsection{{Exponencial temporal como producto infinito}}
\subsection{{Operador evolución para varios intervalos}}
\subsection{{Operador evolución inverso}}
\subsection{{Descomposición para Hamiltoniano suma}}
\subsection{{Otra descomposición para Hamiltoniano suma}}
\subsection{{Evolución con Hamiltoniano transformado}}
\begin{equation}\label{eq1}{A=}\end{equation}
\section{{Ecuación Maestra Fenomenológica}}
\subsection{{Obtención de operadores de Linblad}}
Tal y como se presenta en el enunciado, la ecuación maestra para un reservorio comprimido es
\begin{equation}\label{eq2}\begin{aligned} {\dot{\rho}=\frac{\gamma}{2}((N+1)(2\sigma^-\rho\sigma^+-\sigma^+\sigma^-\rho-\rho\sigma^+\sigma^-)}\\ {+(N)(2\sigma^+\rho\sigma^--\sigma^-\sigma^+\rho-\rho\sigma^-\sigma^+)} \\ {-(Me^{i\theta})(2\sigma^+\rho\sigma^+-\sigma^+\sigma^+\rho-\rho\sigma^+\sigma^+)}\\
{-(Me^{-i\theta})(2\sigma^-\rho\sigma^--\sigma^-\sigma^-\rho-\rho\sigma^-\sigma^-))}\end{aligned}\end{equation}
Considerados las definiciones en el mismo enunciado
\begin{equation}\label{eq2.a} {\sigma^+=\begin{pmatrix} 0&1\\ 1&0\end{pmatrix}, \sigma^-=\begin{pmatrix}0&0\\1&0\end{pmatrix},  M=\sqrt{N}\sqrt{N+1}}\end{equation}
La ecuación \ref{eq2} se puede reordenar
\begin{equation}\label{eq2.b}\begin{aligned} {\dot{\rho}=\frac{\gamma}{2}(2(\sqrt{N+1}^2(\sigma^-\rho\sigma^+)+\sqrt{N}^2(\sigma^+\rho\sigma^-)-\sqrt{N}\sqrt{N+1}(e^{i\theta}(\sigma^+\rho\sigma^+)+e^{-i\theta}(\sigma^-\rho\sigma^-)))}\\ {-(\sqrt{N+1}^2(\sigma^+\sigma^-\rho)+\sqrt{N}^2(\sigma^-\sigma^+\rho)-\sqrt{N}\sqrt{N+1}(e^{i\theta}(\sigma^+\sigma^+\rho)+e^{-i\theta}(\sigma^-\sigma^-\rho)))} \\ {-(\sqrt{N+1}^2(\rho\sigma^+\sigma^-)+\sqrt{N}^2(\rho\sigma^-\sigma^+)-\sqrt{N}\sqrt{N+1}(e^{i\theta}(\rho\sigma^+\sigma^+)+e^{-i\theta}(\rho\sigma^-\sigma^-))))}\\ \end{aligned}\end{equation}
Si se define de forma conveniente:
\begin{equation}{s_1=\sqrt{N+1}\sigma^-, s_2=\sqrt{N}e^{i\theta}\sigma^+}\end{equation}
Lo anterior se puede escribir como:
\begin{equation}\begin{aligned}{\dot{\rho}=\frac{\gamma}{2}(2(s_1\rho s_1^\dag+s_2\rho s_2^\dag-(s_2 \rho s_1^\dag +s_1\rho s_2^\dag))} \\{ -(s_1^\dag s_1\rho+ s_2^\dag s_2 \rho -(s_1^\dag s_2\rho + s_2^\dag s_1\rho))}\\  {-(\rho s_1^\dag s_1+\rho s_2^\dag s_2  -(\rho s_1^\dag s_2 +\rho s_2^\dag s_1) )}\end{aligned}\end{equation}
Cada uno de los términos en los paréntesis, incluye elementos que se pueden reescribir como resultado del producto de 2 sumas:
\begin{equation}{\dot{\rho}=\frac{\gamma}{2}(2(s_1-s_2)\rho(s_1^\dag-s_2^\dag)-(s_1^\dag-s_2^\dag)(s_1-s_2)\rho-\rho(s_1^\dag-s_2^\dag)(s_1-s_2))}\end{equation}
Por lo tanto, si se escribe el operador de Lindblad:
\begin{equation}{S=s_1-s_2=\sqrt{N+1}\sigma^--\sqrt{N}e^{i\theta}\sigma^+}\end{equation}
La ecuación maestra original se puede escribir compactamente en la forma:
\begin{equation}{\dot{\rho}=\frac{\gamma}{2}(2S\rho S^\dag-S^\dag S\rho-\rho S^\dag S)}\end{equation}
\subsection{{Programación Ecuación Maestra para estados de Bell}}

De acuerdo a lo visto en \cite{Omar} basado en \cite{Method}, para matrices densidad con la forma
\begin{equation}\label{eq3}{\rho=\frac{1}{4}\begin{pmatrix}1+c_3&0&0&c1-c2
\\0&1-c_3&c_1+c_2&0\\0&c_1+c_2&1-c_3&0\\c_1-c_2&0&0&1+c_3\end{pmatrix}}\end{equation}
(donde ${c_1}$, ${c_2}$ y ${c_3}$ son números reales) existe una fórmula explícita para la Discordia Cuántica
\begin{equation} {D=\sum_i\lambda_ilog(4*\lambda_i)-\frac{1}{2}((1+c)log(1+c)+(1-c)log(1-c))}\end{equation}
donde $\lambda_i$ son los autovalores de la matriz. Para obtener tanto la discordia cuántica como la concurrencia para los estados de Bell, se usó Python y Qutip con el siguiente algoritmo:\\ 
\begin{itemize}
\item\textcolor{ForestGreen}{\texttt{Resolver ecuación maestra para 1000 puntos desde 0 a 5}}
\item\textcolor{ForestGreen}{\texttt{Extraer las matrices resultado para cada punto}}
\item\textcolor{ForestGreen}{\texttt{Calcular los 3 valores posibles de c}}
\item\textcolor{ForestGreen}{\texttt{Minimizar para obtener c}}
\item\textcolor{ForestGreen}{\texttt{Calcular discordia cuántica usando el C}}
\item\textcolor{ForestGreen}{\texttt{Calcular concurrencia con la definición de Qutip}}
\item\textcolor{ForestGreen}{\texttt{Hacer arreglos con discordia y concurrencia para cada punto}}
\end{itemize}
Los resultados obtenidos se encuentran al final del documento y en \textcolor{ForestGreen}{\url{https://github.com/Dacastillo/master}}. A continuación, algunos comentarios sobre los resultados:
\begin{itemize}
    \item Se observa que, en el caso de los estados de Bell, la discordia cuántica decae, pero luego se va recuperando, sobre todo si el reservorio es comprimido. El reservorio vacío lo hace decaer de manera directa, el térmico puede recuperar entrelazamiento pero de manera más bien parcial, y el reservorio comprimido es capaz de llevarlo de vuelta al estado maximalmente entrelazado.
    \item Si los reservorios están separados entre sí, el efecto de decaimiento es más rápido. Acoplar los reservorios ralentiza la caída tanto del entrelazamiento como de la discordia. Se observa de manera directa que el reservorio comprimido acoplado empieza a aumentar la concurrencia, lo que es bastante contraintuitivo. 
    \item Cambiar el estado de Bell a medir cambia muy poco en los resultados finales. Para la discordia sin acoplamiento se observa una evolución ligeramente distinta, pero siguiendo la misma tendencia. Lo que es esperable en estados que comparten sus características en cuanto entrelazamiento y discordia.
    \item En esta sección se pudo definir las variables a minimizar para la discordia de manera relativamente simple. No se observan posibles errores en el resultado, ni observando el avance de los gráficos ni comparándolos con lo realizado en \cite{Method2}
\end{itemize}
\subsection{{Programación Ecuación Maestra para estados lógicos}}
En los estados lógicos no se cumple la forma presentada anteriormente, pero si se cumple esta forma:
\begin{equation}{\rho=\begin{pmatrix} \rho_{11} & 0 & 0 & \rho_{14} \\ 0 & \rho_{22} & \rho_{23}
 & 0 \\ 0 & \rho_{32} & \rho_{33} & 0 \\ \rho_{41} & 0 & 0 & \rho_{44}\end{pmatrix}}\end{equation}
 Estos estados son conocidos como {Estados X} porque la disposición de los elementos de matriz distintos de cero sugiere una X.  De acuerdo a \cite{Ali}, para estos estados, el Quantum discord se obtiene para uno de los valores definidos de la entropía condicional en los que se minimiza. Esto está debidamente referenciado tanto en \cite{Ali} como en \cite{Results}.
El código también se programó usando Qutip y Python usando el siguiente algortimo:
\begin{itemize}
\item\textcolor{ForestGreen}{\texttt{Resolver ecuación maestra para 1000 puntos desde 0 a 5}}
\item\textcolor{ForestGreen}{\texttt{Extraer las matrices resultado para cada punto}}
\item\textcolor{ForestGreen}{\texttt{Definir entropía condicional con las 4 opciones posibles}}
\item\textcolor{ForestGreen}{\texttt{Elegir la opción que de entropía menor}}
\item\textcolor{ForestGreen}{\texttt{Calcular discordia cuántica usando el condicional definido}}
\item\textcolor{ForestGreen}{\texttt{Calcular concurrencia con la definición de Qutip}}
\item\textcolor{ForestGreen}{\texttt{Hacer arreglos con discordia y concurrencia para cada punto}}
\end{itemize}
Los resultados obtenidos se encuentran al final del documento y en \textcolor{ForestGreen}{\url{https://github.com/Dacastillo/master}}. 
 A continuación, algunos comentarios sobre los resultados:
\begin{itemize}
    \item Se pudo observar, de manera directa lo señalado en el enunciado: que la dinámica linbladiana puede generar entrelazamiento. A pesar de empezar con estados de la base lógica (que en principio no tienen ni discordia ni entrelazamiento), su interacción con el entorno hace que si la tengan. Se observa algo similar a lo visto para los estados de Bell, considerando en todo caso que no se parte desde un máximo, sino desde cero.
    \item Al igual que en el problema anterior, se observa que el reservorio comprimido aumenta tanto la cantidad de entrelazamiento y discordia producida, como su estabilidad en el tiempo. En menor medida, esto lo logra el reservorio térmico por sobre el vacío. 
    \item Existe un caso particular: El estado ${\ket{00}}$. Este estado, a diferencia de los otros 2, ni siquiera tiene interacciones. Lo que ocurre es que tanto con los reservorios vacío como térmico la discordia y el entrelazamiento tienden a aumentar, pero luego decaen con la misma rapidez, mientras el reservorio comprimido puede hacer que la discordia y la concurrencia incluso suban con el tiempo.
    \item Como se puede notar, el algoritmo para optimizar entropías condicionales para estados lógicos es algo más difícil que para estados de Bell. Aun con esto, se logró desarrollar sistemas que coinciden con lo referenciado en \cite{Results}. Sin embargo, es posible que el algoritmo requiera de mejoras, en las que seguiré trabajando más allá de la tarea, porque me interesa bastante poder generar sistemas de simulación de sistemas cuánticos abiertos.
    
\end{itemize}
\begin{thebibliography}{XXX0000}
  \bibitem{Omar} Jimenez, 0. Apunte Información Cuántica 1
    \bibitem{Method} Luo S, Quantum discord for two-qubit systems. Phys. Rev. A 77, 042303 (2008)
  \bibitem{Method2} Gallego M., Orszag M. Death and revival of the quantum discord and the
measurement-induced disturbance. J. Opt. Soc. Am. B Vol. 29, No. 7 (2012)
\bibitem{Ali} Ali M., Rau A.R.P. , Alber G. Quantum Discord for two-qubit X states. Phys. Rev. A 81, 042105 (2010)
  \bibitem{Results} Gallego M., Coto R., Orszag M. Generation of quantum correlations for
two qubits through a common reservoir. Phys. Scr. T147 (2012) 014012 (4pp)
\end{thebibliography}

\section{{Ejercicios Suplementarios}}
\subsection{{Elementos de Matriz de Ecuación Maestra}}
Para la ecuación de Linblad:
\begin{equation}\label{eq4}{\dot{\rho}=-i[H,\rho]+\frac{\gamma}{2}(2a\rho a ^\dag-a^\dag a\rho-\rho a^\dag a-\rho a^\dag a)}\end{equation}
\subsection{{Resultado obtenido de considerar origen de Ecuación Maestra}}
\subsection{{Problema considerando estado gato}}
Ahora bien, considerando el estado 
\begin{equation}{\ket{\alpha}+\ket{-\alpha}}\end{equation}
\subsection{{Solución considerando Hamiltoniano}}
Ahora, considerando el Hamiltoniano.
\begin{equation}{H=E(a+a^\dag)}\end{equation}

\section{{Modelo de Jaynes-Cumming con pérdidas}}
\subsection{{Teoría Previa}}
El modelo de Jaynes Cumming tiene como Hamiltoniano Libre (considerando ${\hslash=1}$:
\begin{equation}{H_{JC}=\omega_a \sigma_+\sigma_-+\omega_c a^\dag a +g(a^\dag\sigma^-+a\sigma^+)}\end{equation} 
Para todo el desarrollo que viene $ {\omega_a=1}$,$ {\omega_c=1}$, $ {g=\frac{1}{1000}}$, por lo que el Hamiltoniano queda: 
\begin{equation}{H_{JC}=\sigma_+\sigma_-+a^\dag a +\frac{1}{1000}(a^\dag\sigma^-+a\sigma^+)}\end{equation} 
Considerando las siguientes definiciones matriciales:
\begin{equation}{\sigma^+=\ket{0}_A\bra{1}\otimes\mathbb{I}=\begin{pmatrix} 0&0&1&0 \\ 0&0&0&1 \\ 0&0&0&0 \\ 0&0&0&0 \end{pmatrix}}\end{equation}
\begin{equation}{\sigma^-=\ket{1}_A\bra{0}\otimes\mathbb{I}=\begin{pmatrix} 0&0&0&0 \\ 0&0&0&0 \\ 1&0&0&0 \\ 0&1&0&0 \end{pmatrix}}\end{equation}
\begin{equation}{a^\dag=\mathbb{I}\otimes\ket{0}_F\bra{1}=\begin{pmatrix} 0&1&0&0 \\ 0&0&0&0 \\ 0&0&0&1 \\ 0&0&0&0 \end{pmatrix}}\end{equation}
\begin{equation}{a=\mathbb{I}\otimes\ket{1}_F\bra{0}=\begin{pmatrix} 0&0&0&0 \\ 1&0&0&0 \\ 0&0&0&0 \\ 0&0&1&0 \end{pmatrix}}\end{equation}
Haciendo los productos, y sumando, el Hamiltoniano Jaynes-Cumming para un sistema de átomo de 2 niveles con campo de 1 modo es:
\begin{equation}{H_{JC}=\begin{pmatrix} 2.000 & 0.000 & 0.000 & 0.000 \\ 0.000 & 1.000 & 0.001 & 0.000 \\ 0.000 & 0.001 & 1.000 & 0.000 \\ 0.000 & 0.000 & 0.000  & 0.000\end{pmatrix}}\end{equation}
Los operadores de disipación, de acuerdo a la teoría, tienen la forma: 
\begin{equation}{S_1= \sqrt{2\Gamma}\sigma_-=\begin{pmatrix} 0&0&0&0& \\ 0&0&0&0 \\ \sqrt{2\Gamma}&0&0&0 \\ 0&\sqrt{2\Gamma} & 0&0 \end{pmatrix}, S_2=\sqrt{2\kappa}a=\begin{pmatrix} 0&0&0&0 \\ \sqrt{2\kappa}&0&0&0 \\ 0&0&0&0 \\ 0&0&\sqrt{2\kappa}&0\end{pmatrix} }\end{equation}
Si ${\kappa=0,1}$ y ${\Gamma=0,05}$, reemplazando queda:
\begin{equation}{S_1=\begin{pmatrix}0&0&0&0\\0&0&0&0\\ \sqrt{0,1}&0&0&0\\ 0&\sqrt{0,1}&0&0 \end{pmatrix},S_2=\begin{pmatrix}0&0&0&0 \\ \sqrt{0.2}&0&0&0 \\ 0&0&0&0 \\ 0&0&\sqrt{0.2}&0\end{pmatrix}}\end{equation}
Los cálculos realizados, claramente, son para el caso particular en que se permite un máximo de 1 fotón en la cavidad. Para casos donde se permite un valor mayor, se hace un procedimiento análogo en el que se construyen simplemente operadores escalera más grande. Para el trabajo realizado se probó con un máximo de 1, 2 y 3 excitaciones. Matrices de dicha forma son las que se insertarán en los códigos. Todos los códigos fueron hechos en Python usando las librerías math, matplotlib (para graficar) y numpy, cumpliendo el siguiente algoritmo:
\begin{itemize}
\item\textcolor{ForestGreen}{\texttt{Definir operadores, Hamiltoniano Jaynes-Cumming y disipadores.}}
\item\textcolor{ForestGreen}{\texttt{Resolver ecuación maestra para 500 puntos y tiempo de 0 a 5.}}
\item\textcolor{ForestGreen}{\texttt{Definir 2 números aleatorios 500 veces.}}
\item\textcolor{ForestGreen}{\texttt{Usarlos para evaluar un si hay salto cuántico en el intervalo de 0 a 5.}}
\item\textcolor{ForestGreen}{\texttt{Repetir varias veces (cantidad variable de 100 a 600.}}
\item\textcolor{ForestGreen}{\texttt{Evaluar valores de expectación para g, e y número de partículas en gravedad promedio.}}
\item\textcolor{ForestGreen}{\texttt{Graficar los resultados para comparar ambos métodos.}}
\end{itemize}
Los resultados obtenidos se encuentran al final del documento y en \textcolor{ForestGreen}{\url{https://github.com/Dacastillo/master}}, en las carpetas llamadas 100, 200, 300, etc. (hasta 600). Cada número indica la cantidad de pasos de quantum jumps que se corrió.
\subsection{{Modelado para Temperatura 0}}
Algunos comentarios sobre los resultados obtenidos:
\begin{itemize}
    \item  Se observa en el número de partículas promedio en la cavidad el decaímiento esperado.
    \item 
    \item 
\end{itemize}
\subsection{{Modelo para Temperatura 2}}
Algunos comentarios sobre los resultados obtenidos:
\begin{itemize}
    \item Se observa en el número de partículas promedio en la cavidad el decaímiento esperado.
    \item
    \item 
\end{itemize}
\subsection{{Modelo para Temperatura 10}}
Algunos comentarios sobre los resultados obtenidos:
\begin{itemize}
    \item  Se observa en el número de partículas promedio en la cavidad el decaímiento esperado.
    \item 
    \item 
\end{itemize}

\appendix
\chapter{Imágenes resultado del programa de la sección 3.2}
\begin{figure}[ht]
\centering
\includegraphics[width=0.8\textwidth]{bell1.png}
\caption{Resultados para el estado de Bell ${\frac{1}{\sqrt{2}}(\ket{00}+\ket{11})}$ para reservorio vacío (linea verde), térmico (línea amarilla) y comprimido (línea azul)}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=0.8\textwidth]{bell3.png}
\caption{Resultados para el estado de Bell ${\frac{1}{\sqrt{2}}(\ket{01}+\ket{10})}$ para reservorio vacío (linea azul), térmico (línea amarilla) y comprimido (línea verde)}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=0.8\textwidth]{logi00.png}
\caption{Resultado para el estado lógico ${\ket{00}}$ para reservorio vacío (línea azul a la izquierda, línea verde a la derecha), térmico (línea amarilla a la izquierda, línea azul ala derecha) y comprimido (línea verde a la izquierda, línea morada a la derecha)}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=0.8\textwidth]{logi01.png}
\caption{Resultado para el estado lógico ${\ket{01}}$ para reservorio vacío (línea azul a la izquierda, línea verde a la derecha), térmico (línea amarilla a la izquierda, línea azul ala derecha) y comprimido (línea verde a la izquierda, línea morada a la derecha)}
\end{figure}

\chapter{Imágenes resultado del programa de la sección 3.4}

\begin{figure}[ht]
    \includegraphics[width=0.85\textwidth]{test0_T0_pob0.png}
    \caption{Resultados de número promedio de excitaciones para el sistema Jaynes-Cumming con temperatura 0  y  1 excitación máxima en la cavidad. Evaluado para varios numeros de pasos.}
\end{figure}

\begin{figure}[ht]
    \includegraphics[width=0.85\textwidth]{test0_T0_pobe.png}
    \caption{Resultados de probabilidad de estado excitado para el sistema Jaynes-Cumming con temperatura 0 y 1 excitación máxima en la cavidad. Evaluado para varios números de pasos.}
\end{figure}

\begin{figure}[ht]
    \includegraphics[width=0.85\textwidth]{test1_T0_pob0.png}
    \caption{Resultados de número promedio de excitaciones para el sistema Jaynes-Cumming con temperatura 0 y  2 excitaciones máximas en la cavidad. Evaluado para varios números de pasos.}
\end{figure}

\begin{figure}[ht]
    \includegraphics[width=0.85\textwidth]{test1_T0_pobe.png}
    \caption{Resultados de probabilidad de estado excitado para el sistema Jaynes-Cumming con temperatura 0 y 2 excitaciones máximas en la cavidad. Evaluado para varios números de pasos.}
\end{figure}

\begin{figure}[ht]
    \includegraphics[width=0.85\textwidth]{test2_T0_pob0.png}
    \caption{Resultados de número promedio de excitaciones para el sistema Jaynes-Cumming con temperatura 0 y 3 excitaciones máximas en la  Evaluado para varios números de pasos.}
\end{figure}

\begin{figure}[ht]
    \includegraphics[width=0.85\textwidth]{test2_T0_pobe.png}
    \caption{Resultados de probabilidad de estado excitado para el sistema Jaynes-Cumming con temperatura 0 y  3 excitaciones máximas en la cavidad. Evaluado para varios números de pasos }
\end{figure}

\begin{figure}[ht]
    \includegraphics[width=0.85\textwidth]{test0_T2_pob0.png}
    \caption{Resultados de número promedio de excitaciones para el sistema Jaynes-Cumming con temperatura 2 y 1 excitación máxima en la cavidad. Evaluado para varios numeros de pasos.}
\end{figure}

\begin{figure}[ht]
    \includegraphics[width=0.85\textwidth]{test0_T2_pobe.png}
    \caption{Resultados de probabilidad de estado excitado para el sistema Jaynes-Cumming con temperatura 2 y 1 excitación máxima en la cavidad. Evaluado para varios números de pasos.}
\end{figure}

\begin{figure}[ht]
    \includegraphics[width=0.85\textwidth]{test1_T2_pob0.png}
    \caption{Resultados de número promedio de excitaciones para el sistema Jaynes-Cumming con temperatura 2 y 2 excitaciones máximas en la cavidad. Evaluado para varios números de pasos.}
\end{figure}

\begin{figure}[ht]
    \includegraphics[width=0.85\textwidth]{test1_T2_pobe.png}
    \caption{Resultados de probabilidad de estado excitado para el sistema Jaynes-Cumming con temperatura 2 y 2 excitaciones máximas en la cavidad. Evaluado para varios números de pasos.}
\end{figure}

\begin{figure}[ht]
    \includegraphics[width=0.85\textwidth]{test2_T2_pob0.png}
    \caption{Resultados de número promedio de excitaciones para el sistema Jaynes-Cumming con temperatura 2 y 3 excitaciones máximas en la  Evaluado para varios números de pasos.}
\end{figure}

\begin{figure}[ht]
    \includegraphics[width=0.85\textwidth]{test2_T2_pobe.png}
    \caption{Resultados de probabilidad de estado excitado para el sistema Jaynes-Cumming con temperatura 2 y 3 excitaciones máximas en la cavidad. Evaluado para varios números de pasos }
\end{figure}

\begin{figure}[ht]
    \includegraphics[width=0.85\textwidth]{test0_T10_pob0.png}
    \caption{Resultados de número promedio de excitaciones para el sistema Jaynes-Cumming con temperatura 10 y  1 excitación máxima en la cavidad. Evaluado para varios numeros de pasos.}
\end{figure}

\begin{figure}[ht]
    \includegraphics[width=0.85\textwidth]{test0_T10_pobe.png}
    \caption{Resultados de probabilidad de estado excitado para el sistema Jaynes-Cumming con temperatura 10 y 1 excitación máxima en la cavidad. Evaluado para varios números de pasos.}
\end{figure}

\begin{figure}[ht]
    \includegraphics[width=0.85\textwidth]{test1_T10_pob0.png}
    \caption{Resultados de número promedio de excitaciones para el sistema Jaynes-Cumming con temperatura 10 y 2 excitaciones máximas en la cavidad. Evaluado para varios números de pasos.}
\end{figure}

\begin{figure}[ht]
    \includegraphics[width=0.85\textwidth]{test1_T10_pobe.png}
    \caption{Resultados de probabilidad de estado excitado para el sistema Jaynes-Cumming con temperatura 10 y  2 excitaciones máximas en la cavidad. Evaluado para varios números de pasos.}
\end{figure}

\begin{figure}[ht]
    \includegraphics[width=0.85\textwidth]{test2_T10_pob0.png}
    \caption{Resultados de número promedio de excitaciones para el sistema Jaynes-Cumming con temperatura 10 y 3 excitaciones máximas en la  Evaluado para varios números de pasos.}
\end{figure}

\begin{figure}[ht]
    \includegraphics[width=0.85\textwidth]{test2_T10_pobe.png}
    \caption{Resultados de probabilidad de estado excitado para el sistema Jaynes-Cumming con temperatura 10 y 3 excitaciones máximas en la cavidad. Evaluado para varios números de pasos }
\end{figure}
\end{document}   
